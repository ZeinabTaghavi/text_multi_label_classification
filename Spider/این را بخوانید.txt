سلام
۱ ‫-‬ فایل main‪.‬py برای اجرای تمام پروژه به صورت یکجاست و انجام مرحله به مرحله‌ی آن را می‌توانید در  ipython‪(‬guide‪)‬ ببینید.
۲ ‫-‬ داده‌ی جمع آوری شده از آنچه crawler ها جمع کرده‌اند‪,‬ آنچه به عنوان stopwords در نظر گرفته می‌شود و نیز داده‌های پیش پردازش شده در دایرکتوری Data هستند.
۳ ‫-‬ برای افزودن سایت جهت crawl کردن‪,‬ کافی است که Url آن و نام انتخابی را در دیکشتری موجود در make‪_‬spider افزوده و سپس تغییرات مورد نظر را اعمال کنید.(اطلاعات مورد نیاز جهت استفاده از user agent از قبل افزوده شده است) 