{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ParsBERT_pytorch_weighted_4fc.ipynb","provenance":[{"file_id":"1cercX3AbW78n80cYruyOWdTPSJvoHNCR","timestamp":1610980850385},{"file_id":"1jc285Yvl1awWsp_4oI6zqI97-A-GZr1-","timestamp":1610976334897},{"file_id":"1hFxE4P1sSAJbdt1ZRq2XFKlNu8MEnqdH","timestamp":1610702037313},{"file_id":"17Pz04rEo4Ru5mffLjXtdLPXDYz-O5RFP","timestamp":1610640908569},{"file_id":"1k6QLcPAJKT5H2yTy61U-YCB267inolmb","timestamp":1610512876213},{"file_id":"1AIm-KimERCJutlpyjiZ2IAwM-cCrVsWM","timestamp":1610440221899},{"file_id":"17mqUcShahUjZjQxywgKQSGU1jV-CZW8o","timestamp":1610336820188},{"file_id":"1FgtzYXY0CXNyE_2FU4IEqJTQzmVZNvDh","timestamp":1610105938882}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8d314ae5f2144f6e804abaee4f9eb10a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e748971edf0047b4b63832a7f53c484e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2540867df34473aae60da13bcc6b818","IPY_MODEL_5d65ff2af99c4b5b9bbfcfcfc23efa7e"]}},"e748971edf0047b4b63832a7f53c484e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2540867df34473aae60da13bcc6b818":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7559c53fa63d48e99315b44494e78523","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1441,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1441,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_527d06e1c660454f8b00df3ed7824f7e"}},"5d65ff2af99c4b5b9bbfcfcfc23efa7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa56b0a445e541d29801b0a29e9ea806","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.44k/1.44k [00:00&lt;00:00, 5.34kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c98a25ea80174a57943c75d2b99d6ca0"}},"7559c53fa63d48e99315b44494e78523":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"527d06e1c660454f8b00df3ed7824f7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa56b0a445e541d29801b0a29e9ea806":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c98a25ea80174a57943c75d2b99d6ca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7dff655f33f4758b89e9500988c0a37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ed1fd9ee618c4ded8f7f25d15a8ccc9c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_04ac665566264bdc9bd65b75f8a5ec52","IPY_MODEL_bedaf64ea2c64bad97f9b9cb6cd782af"]}},"ed1fd9ee618c4ded8f7f25d15a8ccc9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04ac665566264bdc9bd65b75f8a5ec52":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e9ca40f0494c4d6d9997cb69b5a1a04e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1198122,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1198122,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6087d4f9cc444e8f8717a6246617f985"}},"bedaf64ea2c64bad97f9b9cb6cd782af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ca7a4082acf44a4b8e6d82ef4a7e1d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20M/1.20M [00:00&lt;00:00, 1.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_caeebe376c6c40c59c998aca19dea03f"}},"e9ca40f0494c4d6d9997cb69b5a1a04e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6087d4f9cc444e8f8717a6246617f985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ca7a4082acf44a4b8e6d82ef4a7e1d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"caeebe376c6c40c59c998aca19dea03f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5625c69213e458691ccfed2097f8ec9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cc71e78aec6441a6810b7be6ccf04a83","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_63df0950f41e4b94b0e3930fa72c086e","IPY_MODEL_70887e0cbad14c13b95b12cfab23ab8f"]}},"cc71e78aec6441a6810b7be6ccf04a83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63df0950f41e4b94b0e3930fa72c086e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c03426209cb54eefbe45585ffb9cc1cc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e35de3d7a244a4392a3ab51e51fa7f6"}},"70887e0cbad14c13b95b12cfab23ab8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_460b60a8050e4407bdcf1d8926fdf2e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 317B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03a156de915b4eed97e1f774f159e1a6"}},"c03426209cb54eefbe45585ffb9cc1cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2e35de3d7a244a4392a3ab51e51fa7f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"460b60a8050e4407bdcf1d8926fdf2e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"03a156de915b4eed97e1f774f159e1a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73a4f8c2ee0245a3bf0369d38afc308c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_609652379cad45aab9e3e3ba09438f14","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_758cda52684b4253a8e6b015ff808806","IPY_MODEL_b9b9be3b94ad485aa3dddf2f12cfce0d"]}},"609652379cad45aab9e3e3ba09438f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"758cda52684b4253a8e6b015ff808806":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d83bd47a520447af8aa711b0bbb9312d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":62,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":62,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_470eaf053a1d4e3aba3390cb653860a7"}},"b9b9be3b94ad485aa3dddf2f12cfce0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_15fafccaf5584463acfbdafeab6e20f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62.0/62.0 [00:00&lt;00:00, 540B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_57b7cc364afc46988c03f04537c6b97d"}},"d83bd47a520447af8aa711b0bbb9312d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"470eaf053a1d4e3aba3390cb653860a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15fafccaf5584463acfbdafeab6e20f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"57b7cc364afc46988c03f04537c6b97d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b90ff975490f4463afa5f77901527458":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8ad056afd79743d48569c2bc3f3a35f7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9e39154ae0f54882b102d2b40ba4625e","IPY_MODEL_0817590e6b48497b9b67b7313b24d3d5"]}},"8ad056afd79743d48569c2bc3f3a35f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e39154ae0f54882b102d2b40ba4625e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a00ae9c9e6343838bd121a0d7128ce5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":651477729,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":651477729,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_41190197f76c4fc2bd0fabc981917c91"}},"0817590e6b48497b9b67b7313b24d3d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7dd3b3bc78754319a1c9c640100ac194","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 651M/651M [00:26&lt;00:00, 24.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cea4a486dd94d5c95907cbbf56baea2"}},"6a00ae9c9e6343838bd121a0d7128ce5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"41190197f76c4fc2bd0fabc981917c91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7dd3b3bc78754319a1c9c640100ac194":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2cea4a486dd94d5c95907cbbf56baea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"j1LTPn7IjqTz"},"source":["Source:\r\n","\r\n","huggingface: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","\r\n","Tutorial:https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"]},{"cell_type":"code","metadata":{"id":"OmPFvCbSqyZF"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRxC0Pz1qzKc"},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/sharif/FineTuning/ipython(guide)')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCRkKc3NcgkX","executionInfo":{"status":"ok","timestamp":1610981060451,"user_tz":-210,"elapsed":8294,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"95ef4b0e-a79a-492d-f770-baf61e320e00"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n","\r\u001b[K     |▏                               | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204kB 6.4MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 6.4MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 276kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 327kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 389kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 440kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 460kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 522kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 583kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 614kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 645kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 675kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 706kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 737kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 788kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 798kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 819kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 849kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 860kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 880kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 890kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 911kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 921kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 952kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 972kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 983kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 6.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 37.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 46.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f8f58b54d6e6315614747ecabcf62985991aa4515be8abe382949eddab562000\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODc44DglgNjZ","executionInfo":{"status":"ok","timestamp":1610981064304,"user_tz":-210,"elapsed":9589,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"51fe7e79-fc42-47bf-9cc2-504c33618d5d"},"source":["!pip3 install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 5.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 4.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJY_L2p9a0t0","executionInfo":{"status":"ok","timestamp":1610966650637,"user_tz":-210,"elapsed":11809,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"3bbecdbd-d7d2-4402-82cf-0c27495e0360"},"source":["!git clone https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","GIT_LFS_SKIP_SMUDGE=1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'bert-fa-base-uncased-clf-persiannews' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pT2x-6E-nlv7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610981068146,"user_tz":-210,"elapsed":12080,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"53848e8f-5c43-4ba9-d01e-f04cc2cffc20"},"source":["!pip install -q clean-text[gpl]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 71kB 4.3MB/s \n","\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n","\u001b[K     |████████████████████████████████| 245kB 20.1MB/s \n","\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"owqH8POKn50-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610981078089,"user_tz":-210,"elapsed":21228,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"5a367272-75db-4dc7-af52-66e8c59cadab"},"source":["!pip install -q hazm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 317kB 5.2MB/s \n","\u001b[K     |████████████████████████████████| 235kB 16.1MB/s \n","\u001b[K     |████████████████████████████████| 1.4MB 17.5MB/s \n","\u001b[?25h  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eg3Up037nThu"},"source":["import torch\r\n","import numpy\r\n","import pandas\r\n","import re\r\n","from sklearn.preprocessing import MultiLabelBinarizer\r\n","from sklearn.model_selection import train_test_split\r\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,TFAutoModel,AutoModel\r\n","from transformers import BertConfig, BertTokenizer\r\n","from transformers import TFBertModel, TFBertForSequenceClassification\r\n","from transformers import glue_convert_examples_to_features, InputExample\r\n","from sklearn.metrics import classification_report\r\n","\r\n","import hazm\r\n","from cleantext import clean\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ur9wv1ytrZu"},"source":["# specify GPU\r\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xax4bHubzpMp"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"yYGi5SCjODKl"},"source":["# data = pandas.read_csv('../Data/ParsBert.csv')\r\n","# data = data.dropna().reset_index(drop=True)\r\n","# X = data[\"body\"].values.tolist()\r\n","# y = pandas.read_csv('../Data/ParsBert.csv')\r\n","# labels = []\r\n","# tag=[]\r\n","# for item in y['tag']:\r\n","#   labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","#   tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","# labels = list(set(labels))\r\n","# mlb = MultiLabelBinarizer()\r\n","# Y=mlb.fit_transform(tag)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3fhVolUWMNG"},"source":["# #All data befor pre-processing...\r\n","data = pandas.read_csv('/content/drive/MyDrive/sharif/Spider/Data/unpreprocessed_dataset.csv')\r\n","# print(data['body'])\r\n","# print(data.shape)\r\n","X = data[\"body\"].values.tolist()\r\n","# y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","y=data[\"tag\"].values.tolist()\r\n","labels = []\r\n","tag=[]\r\n","for item in y:\r\n","  labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","  tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","labels = list(set(labels))\r\n","mlb = MultiLabelBinarizer()\r\n","Y=mlb.fit_transform(tag)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ML1OHDiXn32"},"source":["# for i in range(10):\r\n","#   print(X[i])\r\n","#   print(Y[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTlTvEDSpCqf"},"source":["def cleanhtml(raw_html):\r\n","    cleanr = re.compile('<.*?>')\r\n","    cleantext = re.sub(cleanr, '', raw_html)\r\n","    return cleantext\r\n","\r\n","\r\n","def cleaning(text):\r\n","    text = text.strip()\r\n","    \r\n","    # regular cleaning\r\n","    text = clean(text,\r\n","        fix_unicode=True,\r\n","        to_ascii=False,\r\n","        lower=True,\r\n","        no_line_breaks=True,\r\n","        no_urls=True,\r\n","        no_emails=True,\r\n","        no_phone_numbers=True,\r\n","        no_numbers=False,\r\n","        no_digits=False,\r\n","        no_currency_symbols=True,\r\n","        no_punct=False,\r\n","        replace_with_url=\"\",\r\n","        replace_with_email=\"\",\r\n","        replace_with_phone_number=\"\",\r\n","        replace_with_number=\"\",\r\n","        replace_with_digit=\"0\",\r\n","        replace_with_currency_symbol=\"\",\r\n","    )\r\n","\r\n","    # cleaning htmls\r\n","    text = cleanhtml(text)\r\n","    \r\n","    # normalizing\r\n","    normalizer = hazm.Normalizer()\r\n","    text = normalizer.normalize(text)\r\n","    \r\n","    # removing wierd patterns\r\n","    wierd_pattern = re.compile(\"[\"\r\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n","        u\"\\U00002702-\\U000027B0\"\r\n","        u\"\\U000024C2-\\U0001F251\"\r\n","        u\"\\U0001f926-\\U0001f937\"\r\n","        u'\\U00010000-\\U0010ffff'\r\n","        u\"\\u200d\"\r\n","        u\"\\u2640-\\u2642\"\r\n","        u\"\\u2600-\\u2B55\"\r\n","        u\"\\u23cf\"\r\n","        u\"\\u23e9\"\r\n","        u\"\\u231a\"\r\n","        u\"\\u3030\"\r\n","        u\"\\ufe0f\"\r\n","        u\"\\u2069\"\r\n","        u\"\\u2066\"\r\n","        # u\"\\u200c\"\r\n","        u\"\\u2068\"\r\n","        u\"\\u2067\"\r\n","        \"]+\", flags=re.UNICODE)\r\n","    \r\n","    text = wierd_pattern.sub(r'', text)\r\n","    \r\n","    # removing extra spaces, hashtags\r\n","    text = re.sub(\"#\", \"\", text)\r\n","    text = re.sub(\"\\s+\", \" \", text)\r\n","    \r\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dQ0R7VepF6z"},"source":["# cleaning taxt\r\n","clean_x=[]\r\n","for t in X:\r\n","  clean_x.append(cleaning(t))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCYV45j-q7va","executionInfo":{"status":"ok","timestamp":1610892454747,"user_tz":-210,"elapsed":1256,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"0f5a3a95-7c71-4f66-e5ec-886d1e8c4309"},"source":["for i in range(10):\r\n","  print(clean_x[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["اگر جزو آن دسته دولوپرهایی هستید که پس از چندین و چند سال کدنویسی و کسب تجربه کماکان از عملکرد خود راضی نیستید، توصیه می‌‌کنیم به ارزیابی دقیق‌تر کار خود بپردازید. به عبارت دیگر، ببینید که آیا حرفه‌‌ای عمل می‌‌کنید؟ آیا از نسبت میان میزان proficiency و competency خود اطلاع دارید؟ آیا می‌‌دانید چه راه‌کارهایی را برای افزایش بهره‌‌وری خود می‌بایست در پیش بگیرید؟ واقعیت امر آن است که در اغلب موارد، دو مفهوم proficiency و competency مشابه یکدیگر در نظر گرفته می‌شوند؛ اما در حقیقت این‌طور نیست! چنانچه این دو واژه واقعا هم‌معنا بودند، هر کسی که در هر کاری سررشته داشت می‌بایست در کار خود بهترین می‌بود؛ به‌ علاوه اینکه شکی نیست که در قرن بیست و یکم، هر روز بیش از روز قبل نیاز به کسب تخصص احساس می‌شود. با‌توجه به میزان و شتاب گسترش اطلاعات از یک سو و همچنین رقابت شدید متخصصان برای ربودن گوی سبقت از رقبای خود از سوی دیگر، قطعا کسب proficiency برای فعالان حوزهٔ کسب‌وکار یک ضرورت است. به‌ طور کلی، آنچه در ادامه قصد داریم مورد بررسی قرار دهیم این است که به چه شکل می‌توان با تلفیق proficiency و competency، از یک برنامه‌نویس معمولی به یک برنامه‌نویس تمام‌عیار، حرفه‌ای و تراز اول مبدل شویم. برنامه‌نویس شدن و ورود به بازار کار برای هر کسی و در هر سنی امکان‌‌پذیر است اما در عین حال، تبدیل شدن به یک برنامه‌نویس حرفه‌ای کاری زمان‌بر و به‌ نسبت دشوار است. پیش از این در مقاله‌ای تحت عنوان چگونه هرچه زودتر برچسب دولوپر «تازه‌کار» را از روی خود برداریم؟ به بیان نکاتی پرداختیم که می‌توانند به برنامه‌نویسان مبتدی کمک کنند تا هرچه سریع‌تر از دوران گذار خود عبور کنند؛ اما در این مقاله قصد داریم در تکمیل نکات طرح شده در مقالهٔ فوق، ببینیم که به چه شکل با سرمایه‌گذاری روی proficiency، می‌توان به یک دولوپر ارشد مبدل شد. آشنایی با تفاوت میان proficiency (تخصص) و competency (مهارت) بررسی این مسئله کمی چالش‌برانگیز به‌ نظر می‌رسد چرا که غالبا این دو واژه هم‌معنی تلقی می‌شوند؛ اما تفاوت ظریفی که میان این دو مفهوم وجود دارد، از اهمیتی فوق‌العاده برخوردار است. به طور کلی: - competency یعنی داشتن تجربه و دانش کافی برای انجام یک تسک (کار) - proficiency به معنی اشراف به چرایی انجام تسکی در قالب روشی مشخص است (و اینکه چگونه آن تسک در مجموعهٔ مربوطه جای می‌گیرد.) به عبارت دیگر، یک دولوپر به اصطلاح proficient (متخصص) می‌‌تواند competent (داری مهارت) هم باشد؛ اما عکس این گزاره درست نیست! کتابی تحت عنوان dreyfus model of skill acquisition این موضوع را با همۀ جوانب و جزئیاتش بررسی کرده است (گرچه عنوان کتاب تاحدی آکادمیک به‌ نظر می‌رسد، اما نکاتی که در این کتاب مطرح شده‌اند بسیار کاربردی و قابل‌فهم هستند.) در ابتدا بیایید تعریفی کاربردی از competency (مهارت) ارائه دهیم بدین صورت که «می‌دونم کارم رو چگونه انجام بدم». این تعریف تا حد زیادی ساده شده است اما برای دستیابی به مقصود ما در اینجا مناسب است. لازم به ذکر است که مهم نیست شما در چه زمینه‌ای فعالیت می‌کنید، بلکه آنچه اهمیت دارد توانایی انجام آن کار است. به طور مثال، اگر شما یک برنامه‌نویس هستید، احتمالا با نحوهٔ کار با مسائل زیر آشنایی خواهید داشت: - نحوۀ کار با سیستم‌های ورژن کنترلی همچون git- نحوۀ به‌کارگیری یک معماری برنامه‌نویسی- نحوۀ راه‌اندازی یک سروری که پایدار باشد- نحوۀ کوئری زدن به دیتابیس- نحوۀ پیاده‌سازی یک لی‌‌اوت ریسپانسیو (واکنش‌گرا) - نحوۀ کار با api و غیرهچیزی که در اینجا نیاز به دقت دارد این است که گرچه داشتن توانایی نحوۀ انجام درست کارها در حد نیاز یک باید است و از اهمیت بالایی برای یک دولوپر برخوردار است، اما کافی نیست! واقعیت امر آن است که با در نظر گرفتن تنگناهایی که یک برنامه‌نویس در حد متوسط در آن‌ها گرفتار می‌شود، ممکن است تصور شود تفاوت میان یک شخص مبتدی و یک شخص حرفه‌ای در میزان اطلاعات آن‌ها است. اما این فقط نیمی از ماجرا است که از قضا، نیمۀ بی‌اهمیت‌تر آن است و اینجا دقیقا همان جایی است که proficiency (تخصص) وارد بحث می‌شود! اساسا proficiency (تخصص) عبارت است از «چرایی» انجام کارها در قالب روشی مشخص. در واقع، تعریف تخصص گویای تفاوت میان فهم هر یک از اجزای مسأله به‌تنهایی، با فهم چگونگی قرارگیری هر جزء در کل تشکیل دهندۀ آن است (به نظر می‌رسد که این جمله کمی گنگ باشد! در همین راستا، در ادامه سعی می‌کنیم بیشتر این مسئله را رمزگشایی کنیم.) برای درک بهتر این موضوع، یک مثال از دنیای واقعی می‌زنیم. یک برنامه‌نویس مبتدی می‌داند که دیزاین پترن چیست اما یک برنامه‌نویس ماهر می‌داند که چه موقع و در چه شرایطی از کدام دیزاین پترن‌ در معماری اپلیکیشن خود استفاده کند (برای درک اهمیت به‌کارگیری دیزاین پترن‌ها، به دورهٔ آشنایی با الگوهای طراحی مراجعه نمایید.) در حقیقت، یک برنامه‌نویس متخصص مطمئنا می‌تواند دیزاین پترن memento را درک کرده و آن را پیاده‌سازی نماید. این شخص احتمالا قادر خواهد بود مواردی را که این الگوی طراحی در آن کاربردی است را تشخیص دهد و این در حالی است که بدون وجود اطلاعات تکمیلی در مورد ماهیت موضوع، دولوپری که فقط از مهارت کدنویسی برخوردار است، این دیزاین پترن را به‌ صورت نادرست به‌ کار خواهد گرفت! همچنین یک برنامه‌نویس حرفه‌ای قادر به تشخیص شرایطی است که دیزاین پترن memento جواب نمی‌دهد! به‌ عنوان مثال، زمانی که نرم‌افزار در حال کپی کردن حجم بالایی از اطلاعات یا انجام تعداد قابل‌توجهی تسک می‌باشد که در این شرایط، برنامه‌نویس نسبت به گزینه‌های جایگزین آگاهی دارد تا چنانچه الگوی مورد استفادۀ او مناسب نبود، از آن‌ها استفاده کند. همچنین این شخص به‌ خوبی می‌داند که دیزاین پترن memento در پشت پرده چگونه کار می‌کند، لذا در صورتی که نیاز به ارائهٔ یک راه‌کار شخصی‌سازی (customized) وجود داشته باشد، به‌ سادگی قادر خواهد بود با الهام گرفتن از چیزهایی که بلد است از یک سو، و همچنین اعمال یکسری تغییرات در دیزاین پترن مربوطه، راه‌کاری عملی‌تر ارائه دهد. برای درک بهتر اهمیت این موضوع، خوب است بدانیم که یک برنامه‌نویس حرفه‌ای می‌تواند زمان مناسب به‌کارگیری الگوهای طراحی را تشخیص دهد. به‌ عنوان مثال، زمانی که یک برنامه‌نویس حرفه‌ای صرفا روی یک قابلیت جدید نرم‌افزار یا یک ایدهٔ جدید کار می‌کند، اصلا نیازی به استفاده از دیزاین پترن‌ها ندارد بلکه صرفا باید کدی بنویسد که نتیجهٔ مد نظر وی را به قول معروف return کند. یا زمانی که یک برنامه‌نویس حرفه‌ای قرار است تا چیزی را برای یک برنامه‌نویس مبتدی توضیح دهد، به‌ جای نام بردن از انواع الگوها و حواله دادن شخص مبتدی به مطالعۀ کتاب «الگوهای طراحی گنگ آف فور»، به این مبحث می‌پردازد که کدها اساسا چه کارکردی دارند و این‌ را به ساده‌ترین شکل ممکن توضیح خواهد داد (برای آشنایی بیشتر با این کتاب، به آموزش آشنایی با مفهومی تحت عنوان دیزاین پترن در برنامه‌نویسی شییٔ‌گرایی مراجعه نمایید.) چگونه به یک دولوپر متخصص مبدل شویم؟ تسلط به الگوها، اصول، اصطلاحات، لایبرری‌ها و ویژگی‌های خاص هر زبان برنامه‌نویسی، همگی ابزارهای لازم برای تبدیل شدن از یک برنامه‌نویس اصطلاحا «دست به کد» به یک برنامه‌نویس متخصص و تمام‌عیار هستند اما در عین حال، یک برنامه‌نویس واقعا حرفه‌ای، می‌داند که هریک از این ابزارها را چگونه در جای مناسب خود به کار گیرد. واقعیت امر آن است که بسیاری از افراد تمایل دارند به‌ جای کسب تخصص، در یک حوزهٔ خاص اطلاعاتی کلی به‌ دست آورند چرا که نسبت به کسب تخصص، به مراتب بی‌دردسرتر و راحت‌تر است اما اگر می‌خواهید تمرکز خود را بر روی کسب تخصص قرار دهید و برای شروع این کار به کمک نیاز دارید، در ادامه ایده‌هایی کاربردی ارائه خواهیم داد. - به دلیل انجام یک کار پی ببرید: برای خود توضیح دهید که به چه دلیل می‌خواهید تسکی را در قالب شیوه‌ای مشخص انجام دهید. برای این کار، به بهترین شیوه‌هایی که نوعا مورد استفاده قرار می‌گیرند و یا به دستورالعمل‌های عمومی اکتفا نکنید؛ بلکه مزایا و معایبی اپروچ‌ها (رویکردها) را به‌ صورت جداگانه و دقیق بررسی کنید. - به جزئیات توجه کنید: سعی کنید موارد فوق را در زمینه‌ها و موارد مختلف به‌ کار گرفته و ببینید هر کدام در چه موقعیتی بهتر جواب می‌دهند و در کدام موارد به‌کار نمی‌آیند. مواردی را که با شکست مواجه شدند، برای شناخت فرصت‌های جدید به‌کار بگیرید تا بتوانید ابزارهای جدیدی را پیدا کنید که می‌توانند به تقویت مهارت‌های شما کمک کنند. - ببینید سایر دولوپرها چرا و چگونه دست به ساختارشکنی زده‌اند: در واقع، به قانون‌شکنی‌هایی که مقدمۀ پیروزی شده‌اند، دقت کنید. گاهی برخی از قوانین خود را بشکنید و ببینید کدام‌یک به شما آسیب می‌رساند، کدام‌یک کمک‌تان می‌کند و کدام یک هیچ تغییری ایجاد نمی‌کند! - در سورس‌های معتبر کندوکاو کنید: به‌ جای اینکه به خواندن آموزش‌های مبتدیانه اکتفا کنید، به دنبال آموزش‌های تخصصی بروید. این کار قطعا سخت‌تر است اما این فرصت را به شما می‌دهد تا هم اصول و هم حواشی یک تکنیک را درک کنید. همچنین این امکان را در اختیار شما قرار می‌دهد تا ایده‌های جدید خود را که به واسطۀ آگاهی از اصول اولیه کسب کرده‌اید، جمع‌آوری و سازماندهی کنید. - خود را درگیر پروژه‌های عملی کنید: خود را عمیقا در پروژه‌ای که با آن چندان آشنا نیستید درگیر کنید و تلاش کنید تا راه درستی برای انجام آن پیدا کنید. برای این کار، به روتین‌های قدیمی خود، عادت‌ها و سولوشن‌های قدیمی و تاریخ‌گذشته اکتفا نکنید. - از دیگران در مورد دلیل انجام کارهایشان سؤال کنید: تا می‌توانید از دیگر دولوپرها سؤال کنید اما دلایل جزمی و متعصبانه را اصلا نپذیرید! از آن‌ها بخواهید با مثال توضیح دهند و خود در رابطه با زمینۀ کاری آن‌ها تحقیق کنید؛ بدین ترتیب، می‌توانید وضعیت مورد بحث را در شرایطی مشابه شرایط ایشان تصور کنید. انجام این کار به شکل خارق‌العاده‌ای ارزشمند و مؤثر است چرا که به شما کمک می‌کند نقاط ضعف و قوت ایده‌های دیگران را در عادات معمول‌شان ببینید. - در مهارت‌های خود عمیق شوید: تعداد کمی از مهارت‌های خاص خود را که در آن‌ها نسبتا خوب عمل می‌کنید انتخاب کرده و تلاش کنید دانش خود را در آن مهارت‌ها با وسواس تمام تا حد ممکن ارتقاء دهید تا جایی که به نقطه‌ای برسید که احساس کنید در آن زمینۀ خیلی خاص، از تمام اشخاص دیگری که در اطراف خود می‌شناسید، بهتر هستید. کلام آخرمطالبی که در مورد کسب تخصص در هر حوزه‌ای بیان می‌شوند، بسیار تأثیرگذار هستند. شما به سطح فوق‌العاده‌ای از دانش تخصصی نیاز دارید تا بتوانید در خارج از محدودۀ فقط دانستن چیزی بایستید. در عین حال، شکی نداشته باشید که یک درک ناقص از چگونگی به‌کارگیری بهترین چیز در بهترین زمان، شما را از هدفتان دور می‌کند! در نهایت، خلاصهٔ بحث ما در این مقاله این است که هنگامی که شما بر روی کسب تخصص تمرکز می‌کنید، شانسی عالی برای پیدا نمودن راه درست برای تسلط بر کار خود خواهید داشت. حال نوبت به نظرات شما می‌رسد. آیا پس از ورود به حوزهٔ توسعهٔ نرم‌افزار صرفا به competency توجه کرده‌اید یا اینکه تمام تمرکز خود را روی proficiency یا بهتر بگوییم «تخصص» معطوف نموده‌اید؟ نظرات، دیدگاه‌ها و تجربیات خود را با دیگر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","دولوپرها همواره به ابزارهای مختلفی نیاز دارند تا فرایند کدنویسی را برای خود تسهیل نمایید و نیاز به توضیح نیست که ابزارهای توسعهٔ نرم‌افزار زندگی فعالان این حوزه را به مراتب آسان‌تر می‌کنند چرا که باعث می‌شوند ایشان کارایی و بهره‌وری بیشتری داشته و به جای وقت گذاشتن روی مسائل حاشیه‌ای، فقط بر روی کدنویسی تمرکز کند. شاید به این فکر کنید که وقتی ابزارهای قدیمی هنوز هم کارمان را راه می‌اندازند، پس چرا باید دنبال ابزارهای جدید باشیم که در پاسخ به چنین سؤالی می‌توان گفت تکنولوژی باعث ایجاد تغییر می‌شود و تأثیری که بر نحوهٔ شکل‌دهی فرایندهای کاری‌مان دارا است، باعث می‌شود تا آپدیت بودن در دنیای تکنولوژی به یک ضرورت تبدیل شود که در همین راستا در ادامه به بررسی برخی ابزارهایی خواهیم پرداخت که می‌توانند به برنامه‌نویسان در سرعت بخشیدن پروسهٔ توسعهٔ نرم‌افزار کمک کنند. liveeduliveedu می‌تواند پاتوق مناسبی برای دولوپرها و مهندسین نرم‌افزار باشد به طوری که یک پلتفرم یادگیری مبتنی بر پروژه است که در آن افرادی که دانش و مهارت متوسطی دارند می‌توانند مهارت‌هایشان را در زمینهٔ توسعهٔ بازی، طراحی، علم داده‌ها، برنامه‌نویسی، واقعیت افزوده، واقعیت مجازی و هوش مصنوعی بهبود بخشیده و تقویت کنند. standupstandup یک ابزار کاربردی برای مشاهدهٔ پیشرفت کار تیم‌های نرم‌افزاری است به طوری که به‌ آسانی با ابزارهای دیگری مانند trello، bitbucket، github و غیره ادغام می‌گردد. دربارهٔ نحوهٔ کار این ابزار هم باید گفت که standup کار خودش را با اتوماتیک کردن گزارش‌های مهندسی شروع می‌کند که کاملا بر پایهٔ commit history انجام شده توسط اعضای تیم می‌باشد که این مسئله هم به نوبهٔ خود اشتراک‌گذاری اطلاعات میان اعضای تیم را تسهیل کرده و همچنین باعث شکل‌گیری رقابتی مثبت بین اعضای تیم می‌شود. cellcell در واقع یک فریمورک که استراتژی‌های جدیدی برای چگونگی نحوهٔ کدنویسی به زبان html در اختیار دولوپرها می‌گذارد که به عنوان نمونه سورس‌کد نوشته شده با این ابزار داریم: osqueryosquery که توسط فیسبوک توسعه داده شده، لیست سخت‌افزارهای مورد استفاده را در معرض دید کاربر قرار می‌دهد. شاید در نگاه اول این ابزار خیلی جالب به‌ نظر نرسد اما کاربردهای بسیاری دارد به طوری که مثلا با استفاده از آن می‌توان لیست سخت‌افزارهایی که توسط usb متصل شده‌اند را مشاهده کرد. osquery می‌تواند بدون استفاده از فانکشن‌های سطح پایین یا بدون استفاده از هیچ‌گونه api، با سیستم‌عامل ارتباط برقرار کند و در واقع می‌تواند برای دولوپرهایی که می‌خواهند اپلیکیشن خود را از نقض‌های امنیتی محافظت کنند یا می‌خواهند عملکردشان را بر روی سیستم‌های مختلف نظارت کنند، بسیار کارآمد باشد. react native firebasereact native firebase به دولوپرها کمک می‌کند تا بتوانند به‌ راحتی از react native و firebase استفاده کنند به طوری که با این ابزار اپن‌سورس می‌توانید با استفاده از javascript bridge، به‌ راحتی هم روی اندروید و هم روی آی‌او‌اس به یکسری sdk نیتیو فایربیس دسترسی داشته باشید. wrapwrap یک ابزار ساده است که هر کسی با استفاده از آن می‌تواند ترمینال خود را به اشتراک بگذارد. در واقع، اشتراک‌گذاری ترمینال به‌ راحتی تایپ کردن دستور سادهٔ wrap open قابل انجام است که این مسئله می‌تواند به دولوپرها یا ارائه‌دهنده‌های هاستینگ کمک کند تا ترمینال‌های خود را به‌ سادگی با یکدیگر به اشتراک گذاشته و به صورت ریموت به آموزش، دیباگینگ و … بپردازند (در واقع، ویژگی کلیدی wrap امنیت و سهولت انتقال آن است.) draftdraft یک ابزار جدید است که به‌ سادگی به دولوپرها کمک می‌کند تا اپلیکیشن‌هایی را بر پایهٔ kubernetes ایجاد کنند که بدون استفاده از ابزارهایی از این دست، تست اپلیکیشن‌ها قبل از فرستادن آن‌ها به مرحلهٔ ورژن کنترل بسیار سخت و وقت‌گیر است اما با استفاده از draft دولوپرها می‌توانند اپلیکیشن‌ها را به سادگی و قبل از دیپلوی تست کنند. docsifydocsify ابزاری برای مستندسازی است اما در عین حال با تمام ابزارهای مشابه دیگر فرق می‌کند چرا که با‌ استفاده از docsify می‌توانید با به‌کارگیری سینتکس مارک‌داون، پروتوتایپ خود را تولید کرده و بدین طریق است که می‌توانید با تغییر کد در مارک‌داون شاهد آپدیت فوری تغییرات خود باشید و همین موضوع است که docsify را به انتخاب مناسب‌تری در برابر ابزارهای دیگر تبدیل کرده‌ است (برای کسب اطلاعات بیشتر، می‌توانید به مقالهٔ docsify: سرویسی اپن‌سورس برای ایجاد داکیومنتیشن با استفاده از فایل‌های markdown مراجعه نمایید.) javalinjavalin لایبرری سبکی است که یکسری api ساده‌ برای زبان‌های جاوا و کوتلین ارائه می‌دهد که در آن به خوبی اصول طراحی restful api در نظر گرفته شده است (برای کسب اطلاعات بیشتر، می‌توانید به مقالهٔ javalin: لایبرری تحت وب برای زبان‌های جاوا و کاتلین مراجعه نمایید.) به‌ طور مثال، ‌ نمونه‌ای از برنامهٔ hello world به زبان کوتلین که با استفاده از javalin نوشته شده را می‌توانید در ادامه مشاهده کنید: bootsnapbootsnap ابزاری است که سعی کرده تا با بهینه کردن متدهای زبان روبی و بهبود عملکرد کلی آن، سرعت اپلیکیشن‌های نوشته شده با این زبان را بیشتر کند و این در حالی است که bootsnap را می‌توانید به‌ راحتی در قالب یک gem به اپلیکیشن خود اضافه کنید. سخن پایانیابزارهای کدنویسی بخشی از مهمات یک دولوپر هستند؛ درست مانند هر حرفهٔ دیگری، در برنامه‌نویسی هم استفاده از ابزارها برای بهبود و افزایش بهره‌وری ضروری است اما شما از چه ابزارهایی در فرآیند توسعهٔ نرم‌افزار استفاده می‌کنید؟ تجربیات خود را با دیگر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","چنانچه افرادی را می‌شناسید که جزو توسعه‌دهندگان و یا فعالان موفق حوزهٔ نرم‌افزار هستند، می‌توانید با ارسال نام و نام‌خانوادگی، حوزهٔ فعالیت، محل کار و آدرس لینکداین ایشان به ایمیل ما را در تولید پادکست‌های مرتبط با حوزه‌های فناوری به خصوص برنامه‌نویسی و توسعهٔ اپلیکیشن یاری رسانید. لازم به یادآوری است که معرفی افراد مد نظر شما الزاما به معنی دعوت عمل آوردن از ایشان نیست و پس از بررسی رزومهٔ کاری‌شان و در صورت صلاحدید با ایشان وارد مذاکره خواهیم شد و در غیر این صورت امیدواریم که پوزش ما را پذیرا باشید. با تشکرارادتمندتیم سکان آکادمیسپهر گنجی توسعه دهنده‌ی اندروید، آی او اس و ویندوز فون می‌باشد که عمده فعالیت وی در زمینه آموزش برنامه نویسی است و ویدیو‌های آموزشی سپهر را می‌توان در سایت‌های اسفندونه و کوئیک لرن دنبال نمود. سپهر اندروید را به عنوان تخصص اصلی دنبال می‌کند و تجربه توسعه اپلیکیشن‌های اندرویدی با زبان‌های برنامه نویسی جاوا و بیسیک را دارا است. در این مصاحبه، بحث اصلی حول اندروید، روش‌های کدنویسی اپ‌های اندرویدی و سایر مباحث مربوطه خواهد چرخید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","microsoft research open data یک ریپازیتوری مبتنی بر داده است که دیتاست‌هایی که محققان مایکروسافت همراه با تحقیقاتشان ایجاد و منتشر می‌کنند را در دسترس عموم قرار می‌دهد. شما می‌توانید در میان صدها دیتاست موجود جست‌وجو کرده و در صورت لزوم آن‌ها را دانلود کنید یا مستقیما آن‌ها را به ماشین‌های مجازی مبتنی بر azure (سرویس کلود مایکروسافت) یا ماشین‌های مجازی داده‌کاوی کپی کنید. این کمپانی سعی کرده است که تا حد امکان از اصول داده‌ای به اصطلاح fair پیروی کند و هم‌چنان به استفاده از بالاترین استانداردهای به اشتراک‌گذاری داده‌ها ادامه می‌دهد (fair مخفف واژگان findable: قابل یافتن، accessible: قابل دستیابی، interoperable: قابل اجرا و reusable: قابل استفادهٔ مجدد است.) تصمیم‌گیران این کمپانی به خوبی در جریان این موضوع هستند که در حال حاضر صدها ریپازیتوری دادهٔ مختلفی که توسط محققان مورد استفاده قرار می‌گیرند موجود است و انتظار دارند تا با اتخاذ این رویکرد، تلاش‌های آن‌ها در این حوزه تقویت گردد. اگر علاقمند به آشنایی با دیگر پلتفرم‌های فعال در این حوزه هستید، می‌توانید به مقالات زیر نیز مراجعه نمایید: kaggle: کامیونیتی مدیریت پروژه‌های مرتبط با data science dtazar: ابزاری برای جستجو و اشتراک‌گذاری دیتای تحقیقاتیبه طور کلی، مجموعه‌ٔ داده‌های موجود در microsoft research open data توسط حوزه‌ٔ اصلی‌شان طبقه‌بندی می‌شود که این حوزه‌ها عبارتند از علوم کامپیوتری، فیزیک، علوم اجتماعی، ریاضیات و … که همچنین می‌توانید با استفاده از این ریپازیتوری لینک‌ پروژه‌های تحقیقاتی یا نشریاتی که از یک مجموعه‌ داده‌ٔ خاص استفاده می‌کنند را نیز بیابید. هدف مایکروسافت از ایجاد این ریپازیتوری چه بوده است؟ هدف این شرکت از ایجاد microsoft research open data، مهیا کردن بستری ساده برای محققان بوده تا بتوانند دیتاست‌ها، تکنولوژی‌های تحقیقاتی و سایر ابزارها را با یکدیگر به اشترک‌ گذارند. این سایت طراحی شده است تا دسترسی به مجموعه‌ داده‌ها و همکاری بین پژوهشگران را با استفاده از منابع مبتنی بر کلود تسهیل کند و امکان بازتولید تحقیقات را فراهم سازد (مایکروسافت قصد دارد هم‌چنان به تکمیل و بهبود این ریپازیتوری ادامه داده و ویژگی‌های این ریپازیتوری را با توجه به بازخوردهای کاربرانش افزایش دهد.) در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","امروزه کامپیوترها بخش مهم و بزرگی از زندگی ما را تشکیل داده‌اند به‌ طوری‌ که در انجام بسیاری از مشکلات روزمره به داد ما رسیده‌اند؛ همچنین بسیاری از مسائل پیچیده مثل آمار گرفتن در ابعاد وسیع و یا محاسبات پیچیده را برای ما به فرایندی سریع و ساده مبدل کرده‌اند. خیلی از مردم از تاریخچه کامپیوترها آگاهی ندارند اما این مسئله می‌تواند برای برنامه‌نویسان کامپیوتر حائز اهمیت باشد چرا که حاوی مطالب ارزشمند و مفیدی است که ممکن است آیندهٔ شغلی ایشان را در این زمینه تحت تأثیر قرار دهد! در ادامه، حقایقی را برای شما گردآوری کرده‌ایم که اگر برنامه‌نویس هستید حتما باید در موردشان اطلاع داشته باشید. اولین کامپیوتر دنیا با بخار کار می‌کرد! چارلز بابیج، کسی به عنوان پدر برنامه‌نویسی شناخته می‌شود، اولین ماشین با قابلیت برنامه‌ریزی را اختراع کرد. چارلز نام دستگاه جدیدش را analytical engine (موتور تحلیلی) گذاشت؛ آن دستگاه که با ۶ موتور بخار کار می‌کرد، به‌ وسیله‌ٔ کارت‌های پانچ شده اصطلاحا برنامه‌نویسی شده بود و آن‌قدر بزرگ و عظیم بود که می‌توانست فضای یک خانه را به‌ طور کامل آشغال کند. موتور تحلیلی از سه بخش اساسی تشکیل شده بود که عبارتند از: - mill (آسیاب): که نقشی همچون cpu امروزی را بازی می‌کرد. - store (منبع ذخیره): که طبیعتا معادل است با مموری، هارددیسک و یا سیستم‌هایی که امکان ذخیره‌ٔ هر چیزی را به شما می‌دهند. - reader (خوانندهٔ اطلاعات): که به عنوان ورودی دیتا شناخته می‌شد. این روند، چگونگی خروجی هر نوع اطلاعاتی در موتور تحلیلی را به ما نشان می‌دهد. اما جالب است بدانید که چارلز نتوانست موتور تحلیلی را کامل کند، که احتمالا علتش به خاطر درگیری میان چارلز و مهندس ارشد پروژه -جوزف کلمنت- بوده است. اولین ویروس کامپیوتری برای تخریب کردن طراحی نشده بود! فرد کوهن، خالق ویروس‌های کامپیوتری با تکنیک دفاعی است. کوهن کسی بود که اولین ویروس کامپیوتری را به شکلی نوین طراحی کرد؛ در واقع، فرد با زبان c برنامه‌ای نوشت که درون یک برنامهٔ پرکاربرد قرار داشت و می‌توانست بدون اجازه‌ٔ کاربر سیستم را آلوده کند و به اسناد و فایل‌های قربانی دسترسی پیدا کند و کوهن نام ‌آن برنامه را virus گذاشت! قرار بود ویروس اختیار یک کامپیوتر را به‌ دست بگیرد، تکثیر شود و به‌ وسیله‌ٔ چیزی مثل فلاپی‌دیسک از یک کامپیوتر به بقیه سیستم‌ها سرایت کند. کوهن می‌خواست بفهمد که آیا ساخت چنین برنامه‌ای که مانند ویروس تکثیر و کلون شود امکان‌پذیر است یا خیر و او هرگز قصد تخریب و آسیب رساندن به امنیت کامپیوترها را نداشت. فرد کوهن بعدها ویروسی مفید ساخت که به کاربران کامپیوترها کمک می‌کرد تا فایل‌های اجرایی آلوده در سیستم‌هایی که آلوده نشده بودند را پیدا کنند (یعنی قبل از این که سیستم آلوده شود، آن‌ها را متوجه می‌کرد!) اولین برنامه‌نویس کامپیوتر یک زن بود! چه باور کنید و چه باور نکنید، باید بدانید که اولین برنامه‌نویس واقعا یک زن بود. نام او ada lovelace بود که ریاضی‌دانی بود که در سال ۱۸۴۳ در بریتانیا متولد شد (برای آشنایی بیشتر با وی، به مقالهٔ آیا می‌دانستید که اولین زبان برنامه‌نویسی دنیا چه‌ نام دارد؟ مراجعه نمایید). او مقالهٔ یک مهندس ایتالیایی به‌ نام luigi menabrea که در مورد موتورهای تحلیلی بود را ترجمه و تفسیر کرد. ada وقتی که در حال ترجمه بود، یادداشت‌هایی را از طرف خود به متن اصلی اضافه کرد و این‌ کار او باعث شد سرعت رشد برنامه‌نویسی کامپیوتر به شدت افزایش پیدا کند. علت این که از ada به عنوان اولین برنامه‌نویس کامپیوتر یاد می‌شود این است که او برای اولین بار از الگوریتم در ماشین‌های محاسباتی آن زمان استفاده کرد که کمک می‌کرد محاسبهٔ اعداد برنولی سریع‌تر انجام شود که در تاریخ از آن الگوریتم به عنوان اولین برنامهٔ نوشته شده برای یک کامپیوتر واقعی یاد می‌شود و اولین الگوریتمی بود که برای یک کامپیوتر واقعی نوشته شده بود. اولین بازی کامپیوتری، هرگز پول‌ساز نبود! امروزه یکی از بخش‌های موفق و پول‌ساز صنعت برنامه‌نویسی، ساخت گیم است اما جالب است بدانید اولین بازی کامپیوتری دیجیتال از نظر درآمدزایی شکستی عظیم خورد! در سال ۱۹۶۲، یک برنامه‌نویس کامپیوتر از دانشگاه mit به نام استیو راسل به همراه تیمش، نزدیک به ۲۰۰ ساعت کار کردند تا اولین نسخه از بازی کامپیوتری spacewar را خلق کردند. spacewar در‌ واقع یک بازی دونفره بود که هر نفر باید سعی می‌کرد سفینهٔ فضایی کوچک خود را کنترل کند. روند بازی به این شکل بود که شما باید از برخورد به نقطه‌های سفید که حکم ستاره‌ها را داشتند اجتناب می‌کردید و سفینهٔ فضایی دشمن‌تان را نابود می‌کردید (البته اگر دشمن‌تان به شما فرصت می‌‌داد) و در نهایت بازیکنی مغلوب می‌شد که به ستاره‌ها برخورد می‌کرد. ساخت spacewar یک موفقیت بزرگ در mit بود، اما متأسفانه راسل و تیمش هرگز نتوانستند از ساخت این بازی به درآمدزایی برسند. واقعیت امر آن است که بدون حرکت بزرگ راسل و تیمش، این روزها چیزی به‌ نام صنعت بزرگ و پولساز گیمینگ نداشتیم! fortran اولین زبان برنامه‌نویسی سطح بالا در تاریخ برنامه‌نویسی است! اولین زبان برنامه‌نویسی سطح بالا در تاریخ fortran است که نسبت به زبان‌های دیگر آن‌ روزها بسیار به انگلیسی محاوره‌ای نزدیک‌تر بود. fortran در سال ۱۹۵۴ در کمپانی ibm قدم به دنیای برنامه‌نویسی گذاشت و دریچه‌ای نو برای خلق زبان‌های سطح بالای جدیدتر باز کرد. در آن‌‌ سال، جان باکوس که تنها ۳۰ سال داشت با همکاری ۲۵ نفر از دانشمندان ibm دست به خلق این زبان برنامه‌نویسی سطح بالا (high-level) زدند و لازم است بدانید fortran مخفف formula translation به معنای «ترجمهٔ فرمول» است. اولین باگ کامپیوتری، واقعا یک باگ (حشره) بود! این روزها تا اسم باگ را می‌شنویم قطعا خطاهای فنی در نرم‌افزارها را تصور می‌کنیم. عبارت bug را اولین بار توماس ادیسون در سال ۱۸۷۸ به کار برد. سا‌ل‌ها پس از ‌آن، یک افسر زن به نام grace hopper که در نیروی دریایی آمریکا خدمت می‌کرد، وقتی که در حال بر روی کامپیوتر mark ll بود در کتابش به نام log در مورد یک bug (حشره) موجود در کامپیوتر نوشت (برای آشنایی بیشتر با وی، به مقالهٔ آشنایى با دریابان گریس مارى هوپر، کسی که برای اولین بار اصطلاح bug را باب کرد! مراجعه نمایید). برخلاف اصطلاحی که ما این روزها برای باگ‌های کامپیوتری به‌کار می‌بریم (که در‌ واقع خطاها هستند)، اما باگی که grace hopper کشف کرد یک خطا نبود بلکه واقعا یک باگ (حشره) بود. در‌ واقع، آن حشره یک پروانه بود که در relay (تقویت‌کننده) کامپیوتر گیر افتاده بود و به دلیل اختلال در مدارها الکترونیکی، باعث جلوگیری از عملکرد صحیح کامپیوتر می‌شد. همان‌طور که در تصویر بالا می‌بینید، او نوشته است که «اولین اشکال که نوعی از یک حشره است پیدا شد» و نهایتا سیستم حشره‌زدایی یا اصطلاحا debug شد! یک برنامه نویس باید بداند که زبان‌های برنامه‌نویسی چگونه خلق شدند و چه فلسفه‌ای برای آن‌ها وجود دارد، یا در مورد افرادی که در چنین نوآوری‌های مهمی سهم بزرگی داشته‌اند باید اطلاعات داشته باشد. برای این‌ که یک برنامه‌نویس موفق باشید، صرافا در مورد زبان‌های مختلف برنامه‌نویسی نظر دادن اصلا کافی نیست بلکه بهتر است در مورد تاریخچهٔ برنامه‌نویسی هم مطالعه کنید آن‌ وقت شاید بتوانید در مورد آیندهٔ زبان‌های برنامه‌نویسی حدس و گمان‌هایی تقریبا درست بزنید. حال نوبت به نظرات شما می‌رسد. آیا حقائق جالب دیگری در مورد صنعت برنامه‌نویسی می‌شناسید که ارزش دانستن داشته باشند؟ نظرات خود را با ما و سایر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","google alerts این امکان را برای کاربران فراهم کرده است تا موضوعات مورد علاقهٔ خود را مشخص کنند و هر زمان که موتور جستجوی گوگل نتایج جدیدی در مورد آن موضوع پیدا کرد، محتوای آن را از طریق یک ایمیل به اطلاع ایشان برساند. برای مثال، شما می‌توانید از طریق این سرویس از جدیدترین نسخهٔ اپلیکیشن‌های مورد علاقهٔ خود، آخرین سرخط خبرها و یا پست‌هایی که کاربران اینترنتی در مورد افراد مشهور می‌گذارند آگاه شوید. دنبال کردن شرکت‌های مورد علاقهٔ خوداگر یک شرکت تکنولوژی که مورد نظر شما است در شهر محل سکونت شما دفتر جدیدی باز کند یا برای انجام پروژه‌ای قصد استخدام افرادی با توانایی‌ها و مهارت‌های شما را داشته باشد، احتمال زیادی وجود دارد -به ویژه اگر شرکت مد نظر شما از شرکت‌های برجسته و شناخته شده باشد- تا اخبار مربوط به آن در google alerts نمایش داده شود. چنین اطلاعاتی می‌توانند اطلاعات بهتری در مورد آنکه کجا، چه‌وقت و چگونه برای ارائهٔ درخواست یک شغل به شرکت مورد نظر اقدام کنید، به شما بدهند. دنبال کردن تکنولوژیتکنولوژی‌ها به سرعت دگرگون می‌شوند و لازم است که شما نیز همگام با آن‌ها تغییر کنید و به‌روز شوید. سرویس google alerts (هشدارهای گوگل) روشی عالی برای دنبال کردن آخرین تکنولوژی‌های قابل‌استفاده در حیطهٔ تخصصی شما است. شما هر چه‌قدر زودتر از تازه‌های تکنولوژی آگاه شوید، زودتر می‌توانید آن‌ها را بیاموزید و به عنوان یک نقطهٔ قوت نسبت به سایرین آن را در رزومهٔ خود وارد کنید تا توجه کارفرمایان را به خود جلب کنید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","فیسبوک برای آنکه فرآیند ساخت اپلیکیشن در جاوا اسکریپت را ساده‌تر کند، ابزار اپن‌سورس create react app را به بازار عرضه کرد. این ابزار اپن‌سورس به دولوپرها کمک می‌کند تا دیگر با پیکربندی محیط جاوااسکریپت و کامپوننت‌های دیگر آن کلنجار نروند! بنابراین اگر شما از آن دسته افرادی هستید که به طور پیوسته با اپلیکیشن‌های مبتنی بر جاوااسکریپت سر و کار دارید، احتمالا این ابزار را کاربردی خواهید یافت. امروزه لایبرری react. js فیسبوک به یکی از محبوب‌ترین ابزارها برای کدنویسی اپلیکیشن‌های نیتیو (native) برای پلتفرم‌های مختلف از جمله اندروید و آی‌او‌اس تبدیل شده است. البته این لایبرری جاوااسکریپتی پیچیدگی‌های مخصوص به خود را هم دارا است و به همین دلیل است که فیسبوک ابزار اپن‌سورسی به نام create react app را به بازار عرضه کرد. create react app در حقیقت در هکاتونی (مسابقهٔ کدنویسی) در فیسبوک به وجود آمد و توسعه‌دهندگان می‌توانند از آن برای شروع توسعه و گسترش پروژه‌های جدید جاوااسکریپت خود استفاده کنند. این ابزار در واقع ترکیبی از سه ابزار webpack (ابزار کمکی برای بیلد اپلیکیشن)، babel (کامپایلر جاوااسکریپت) و eslint (ابزار لینتینگ) می‌باشد. با استفاده از این ابزار، توسعه‌دهندگان دیگر در مورد مرتب‌سازی فایل‌ها و تنظیمات اولیه نگرانی نخواهند داشت چرا که فقط لازم است با یک وابستگی سر و کار داشته باشند. فیسبوک در مورد این ابزار در بلاگ رسمی خود نوشت: ما به‌روزرسانی bbel، eslint و webpack را بر عهده گرفتیم تا نسخه‌های سازگاری از آنها داشته باشیم تا شما بتوانید یک وابستگی را بدون نیاز به دریافت تمام آنها، به‌روزرسانی کنید. همچنین به خاطر داشته باشید که create react app به هیچ وجه شما را محدود نمی‌کند، بلکه به شما اجازه می‌دهد تا از ابزارهای مختلف دیگر react و اکوسیستم js استفاده کنید. همچنین از آنجا که این ابزار اپن‌سورس است، به شما اجازه خواهد داد که پس از به پایان رسیدن پیکربندی اپلیکیشن، هر نوع شخصی‌سازی را بتوانید بر روی آن اعمال کنید. اگر دوست دارید اپلیکیشن‌های خود را در جاوا اسکریپت بنویسید، create react app فرصتی عالی برای این منظور را در اختیار شما قرار می‌دهد. این ابزار همان‌طور که گفته شد، با حذف نیاز به پیکربندی و خلاصه کردن وابستگی‌ها، می‌تواند هم برای توسعه‌دهندگان تازه‌کار و هم حرفه‌ای‌ها، بسیار کارآمد باشد. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","geek واژه‌ای است که در طول زمانی تعاریف مختلفی برایش در نظر گرفته شده و در عصرهای مختلف، صفتی بوده که قشر خاصی را هدف قرار داده است (این واژه ریشه در زبان هلندی دارد به معنی دیوانه و این در حالی است که معادل آلمانی این واژه نیز geck به معنی ابله است)؛ به عبارت دیگر، در گذشته این اصطلاح نوعی توهین محسوب می‌شد اما امروزه نه تنها توهین نیست، بلکه صفتی کاملا محبوب است به طوری که کمتر دولوپری را می‌توان یافت که دوست نداشته باشد وی را گیک خطاب کنند! واقعیت امر آن است که در گذشته گیک به کسی گفته می‌شد که مثلا در سیرک‌ها -به عنوان دلقلک- می‌توانست کارهای عجیب‌وغریب و خارق‌‌العاده‌ای انجام دهد اما به مرور زمان و پس از انقلاب دیجیتال مفهومش تغییر کرد و امروزه گیک به کسی گفته می‌شود که در کار با کامپیوترها و به طور کلی هر نوع فناوری استاد است (البته نیاز به توضیح است که گیک‌ها فقط در حوزهٔ کامپیوتر فعال نیستند؛ بلکه در دیگر علوم همچون ریاضیات، مهندسی، گیم و غیره نیز چنین افرادی یافت می‌شوند). همچنین تعریف geek از نگاه دیکشنری merriam-webster برابر است با: یک متخصص یا شیفتهٔ کامپیوتر، تکنولوژی و حوزه‌های وابسته گیک نامیده می‌شود. پیش از این در مقاله‌ای همچون آیا شما یک گیک هستید؟ برخی خصوصیات گیک‌ها، با تعریف امروزی‌اش، را برشمردیم که به طور خلاصه می‌توان گفت گیک کسی است که: - به خاطر پول کاری را انجام نمی‌دهد بلکه کاری می‌کند که از آن لذت ببرد. - معمولا روابط اجتماعی ضعیفی دارد! - به خاطر سهولت بیشتر، به هک کردن سخت‌افزارها و نرم‌افزارهای مختلف می‌پردازد (لازم به ذکر است که هک در اینجا به معنی کاستومایز کردن است). - علاقه به یادگیری چیزهای جدید دارد. - معمولا در مواجه با تکنولوژی‌های جدید یک early adopter است (آشنایی با مفهوم اصطلاح early adopter). آشنایی با سندرمی تحت عنوان خودگیک‌پنداری (یا خود‌خفن‌پنداری و یا خودآس‌پنداری) مسلما زمانی که واژهٔ گیک بار معنایی منفی داشته، هیچ‌کس تمایلی نداشته تا وی را گیک بنامند؛ اما همان‌طور که گفته شد، پس از انقلاب دیجیتال بار معنای گیک دستخوش تغییر ۱۸۰ درجه‌ای شد به طوری که از مفهومی کاملا negative، به قول دولوپرها به مفهومی اصطلاحا ++positive مبدل شد (قابل‌توجه کسانی که دولوپر نیستند، ++ در علوم کامپیوتر به معنی یک واحد بیشتر است). با فرض کردن این نکته که از اینجای بحث تا پایان منظورمان از گیک کسی است که در حوزهٔ تکنولوژی به خصوص کدنویسی فعالیت می‌کند، در پاسخ به این سؤال که ویژگی‌های یک خودگیک‌پندار چیست، می‌توان به موارد زیر اشاره کرد: - احتمالا تازه‌کار است. - رزومهٔ این افراد حداقل ۴ صفحه است پر از نام زبان‌ها، لایبرری‌ها، فریمورک‌ها و تکنولوژی‌های جدید. - طرفدار اپن‌سورس است نه به خاطر رعایت کپی‌رایت بلکه به این دلیل که امروزه همایش‌های اپن‌سورسی زیاد برگزار می‌شود و دوست دارد در این رویدادها حرفی برای گفتن داشته باشد. - یک بار توانسته با نرم‌افزارهای رایج در بازار، یوزر/پس وای‌فای همسایهٔ خود را برباید. - عادت دارد اکثر زبا‌ن‌های برنامه‌نویسی را مزه‌چش کند، اما در هیچ‌کدام عمیق نیست! - ریش/موی بلند، پیپ، عینک با شیشهٔ رنگی، توییت‌های رکیک، تی‌شرت مشکی و سلام ندادن به جمعی که واردش می‌شود جزو علاقمندی‌های او است. - در مورد پرفورمنس زبان‌های برنامه‌نویسی مختلف اظهار نظرهای تعصبی می‌کند. - یکسری اسطوره در دنیای فناوری دارد که کورکورانه آن‌ها را دنبال می‌کند. - در حالی که در شرکتی مشغول به کدزنی است، برای کار در جاهای دیگر اقدام می‌کند و بی‌مقدمه می‌گوید که از یک هفته بعد دیگر نمی‌تواند بیاید! - و در شبکه‌های اجتماعی همواره کامنتی برای گذاشتن دارد حتی اگر موضوع قابل بحث نباشد. به نظر می‌رسد کسی که وارد حوزهٔ توسعهٔ نرم‌افزار شده باشد، خواه‌ناخواه به درجه‌ای از این سندرم دچار شده باشد و این موضوع اصلا اشکالی هم ندارد چرا که ماهیت صنعت توسعهٔ نرم‌افزار چنین چیزی را ایجاب می‌کند، اما دولوپری برندهٔ این بازی است که هرچه زودتر مسیر junior تا senior را بپیماید زیرا تجربه نشان داده هرچه دانش، توان تحلیلی و تجربهٔ فرد بالاتر می‌رود، علائم این سندرم کم‌رنگ و کم‌رنگ‌تر می‌شود تا جایی که اگر افراد واقعا حرفه‌ای را از دور/نزدیک دیده‌ باشید، آنچه که به عنوان علائم یک خودگیک‌پندار در بالا عنوان کردیم اصلا -یا خیلی کم- در ایشان دیده می‌شود. حال برای درک بهتر این موضوع، در ادامه برخی از ویژگی‌های شخصیتی خودگیک‌پندارها که آسیب‌های جبران‌ناپذیری به اطرافیان وارد می‌کند را شرح و بسط می‌دهیم: بی‌تعهدیپیش از این گفتیم ممکن است که یک خودگیک‌پندار در حالی که در شرکتی مشغول به کدزنی است، برای کار در دیگر شرکت‌ها اقدام کرده و بی‌مقدمه بگوید که از یک هفته بعد دیگر نمی‌توانم بیایم و این در حالی است که هنوز ۸ ماه از مدت قراردادش باقی مانده است! تجربهٔ نگارنده در حوزهٔ hr حاکی از آن است که این خصیصه صدمات جبران‌ناپذیری به شرکتی که یک خودگیک‌پندار در آن مشغول به کار است و همچنین پروژه‌ای که روی آن کار می‌کند و دیگر اعضای تیم می‌زند؛ اما در عین حال، چنانچه این خصیصه به عادتی برای یک دولوپر تبدیل شود، در دراز مدت صدماتی جدی به برند شخصی خود دولوپر وارد خواهد آمد. دنبال کردن سبک زندگی گیک‌های واقعیبا تعریف مدرن geek، شاید استیو جابز را بتوان یک گیک به معنای واقعی کلمه دانست. در حقیقت، وی کسی بود که دنیا را از زاویهٔ دید متفاوتی می‌دید، آنچه در بازار وجود داشت وی را ارضاء نمی‌‌کرد و شور و شوق عجیبی در کاری که بدان مشغول بود داشت. مسلما هر فردی سبک صحبت، پوشش و تعامل خاص خود را دارا است و استیو جابز هم از این قاعده مستثنی نبود؛ اما داستان از جایی شروع می‌شود که خودگیک‌پندارها بیش از آنکه از مدل فکری، آینده‌نگری، تیم‌سازی و مهارت‌های مدیریت/رهبری گیکی همچون استیو جابز الهام بگیرند، ظاهر وی را تقلید می‌کنند (به واژگان الهام و تقلید خوب توجه کنید و اگر علاقمند به مطالعهٔ بیشتر پیرامون این موضوع هستید، می‌توانید به مقالهٔ چرا اکثر افراد به دنبال یک قهرمان، لیدر و اسطوره می‌گردند تا وی را در زندگی شخصی خود دنبال کنند؟ مراجعه نمایید). به عبارت دیگر، همچون استیو جابز فقید بددهنی می‌کنند، شلوار لی کهنه بدون بستن کمربند می‌پوشند، پیراهن مشکی بر تن می‌کنند و برخی هم که شنیده‌اند وی گاهی‌اوقات قوانین رانندگی را رعایت نمی‌کرده، بی‌اعتنا به قوانین راهنمایی‌ و رانندگی می‌شوند تا بلکه همچون استیو جابز، در نگاه سایرین یک گیک به نظر برسند (رها کردن دوست‌دختر به همراه یک جنین در شکمش هم که جای خود بماند!) رزومهٔ غیرواقعییک خودگیک‌پندار چون شنیده است که ایلون ماسک رزومه‌ای تک صفحه‌ای دارد، شروع به تقلید از وی می‌کند و یا همچون بسیاری از دیگر هم‌قطارانش، رزومه‌ای چند صفحه‌ای دارا است که از زبان basic گرفته تا زبان‌های مدرن امروزی همچون r یا go را شامل می‌شود. علاوه بر این، از نام بردن هیچ لایبرری یا فریمورکی هم در بخش مهارت‌ها فروگذار نکرده اما داستان رزومهٔ یک خودگیک‌پندار هرگز به اینجا ختم نمی‌شود. اگر رزومه انگلیسی باشد (که در بیشتر مواقع همین‌طور است)، غلط‌های فاحش تایپو و گرامری توجه کسی که خرده زبانی بلد باشد را به خود جلب می‌کند (برای آشنایی بیشتر با اصول نگارش یک رزومهٔ حرفه‌ای، به مقالهٔ چگونه یک روزمهٔ خوب به عنوان برنامه‌نویس یا توسعه‌دهنده بنویسیم؟ مراجعه نمایید). همچنین در بخش interests (علائق) هم چیزهایی همچون موارد زیر را به کرات می‌بینیم: - علاقمند به سورس‌کد خوانی - علاقمند به مشارکت در پروژه‌های اپن‌سورس- ابداع الگوریتم‌های هش اختصاصی- یادگیری زبان‌های برنامه‌نویسی جدید- مطالعه در مورد متافیزیک و چیزهایی از این دست- قهوه- بیلیارد- سریال گیم آو ترونز- کتاب‌های علمی-تخیلیاما این در حالی است که اگر به تایم‌لاین اکانت گیت‌هاب وی نگاه کنیم (البته چنانچه اصلا اکانتی داشته باشد)، هیچ نقطهٔ سبزی مشاهده نخواهد شد! کلام آخروقتی در سنین نوجوانی مثل‌هایی همچون «هرچه درختی پربارتر، افتاده‌تر» می‌شنویم، شاید آن‌طور که باید و شاید مفهوم چنین ضرب‌المثل‌هایی را متوجه نشویم اما وقتی که وارد جامعه و بازار کار می‌شویم، با افرادی از اقشار مختلف آشنا می‌شویم و پس از مدتی به معنای واقعی کلمه الگوریتمی در ذهنمان پدیدار می‌شود بدین صورت که «هرچه دولوپری باسوادتر، کمتر خودگیک‌پندارتر» و تجربهٔ گپ و گفتگوی نگارنده با دولوپرهای موفق داخل و خارج در رادیو فول‌استک، مهرتأییدی بر این ادعا است. مسلما این تاپیک بحثی داغ را در پی خواهد داشت و احتمال دارد که به مذاق برخی خوانندگان خوش نیاید؛ اما بیان دیدگاه‌ها و نقطه‌ نظرات -خواه موافق و خواه مخالف- می‌تواند به هرچه اثربخش‌تر شدن چنین محتواهایی کمک کند. در همین راستا، می‌توانید دیدگاه‌ها، نظرات و تجربیات خود را با دیگر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","اگر به دنبال یک شغل جدید در زمینه‌ی فناوری اطلاعات می‌گردید، این مقاله به شما کمک می‌کند که با ۱۰ شغل برتر در این زمینه آشنا شوید که در سال ۲۰۱۶ نسبت به سایر مشاغل در جایگاه به مراتب بهتری قرار دارند. این فهرست، مشاغل را برحسب حقوق و دستمزد و تعداد فرصت‌های شغلی موجود درجه بندی کرده است. با سکان آکادمی همراه باشید. اگر از شغل فعلی خودتان خسته شده‌اید و تصمیم دارید که تغییر شغل دهید به این نکته توجه کنید که در دنیای فناوری، برخی مشاغل نسبت به سایر شغل‌ها برتری دارند. پس، باید در زمان درست، اطلاعات مورد نیاز را جمع آوری کنید و تصمیم درست را بگیرید. سایت کاریابی glassdoor فهرستی از بهترین مشاغل را ارائه کرده است. البته مهندسی عمران، مهندسی مکانیک و وکالت در این فهرست جایی ندارند. فهرست ارائه شده، ۲۵ شغل برتر برای سال ۲۰۱۶ را معرفی می‌کند که ما در این مقاله ۱۰ شغل در حوزه‌ی فناوری را انتخاب کرده‌ایم. این فهرست شامل متوسط حقوق، تعداد فرصت‌های شغلی و امتیازی از ۱ تا ۵ است. البته توجه داشته باشیم که معیارهای انتخابی، بازار کار غرب هستند و مسلما فضای کاری در ایران با آنچه در کشورهای جهان اول دیده می‌شود کاملا متفاوت است. دانشمند داده این شغل که اصطلاحا data scientist نامیده می‌شود، به عنوان جذاب‌ترین شغل قرن بیست و یکم نیز انتخاب شده است. دانشمند داده نیاز به مهارت‌های وسیعی در زمینه‌های گوناگون از جمله ریاضیات، آمار، مدل سازی پیش بینی و مهارت‌های استراتژی کسب و کار دارد. این شغل، نیازمند جمع آوری و آنالیز داده‌های عظیم یا همان big data برای نمایش فرصت‌های مخفی کسب و کار است. - تعداد فرصت‌های شغلی: ۱,۷۳۶- متوسط پایه حقوق: ۱۱۶,۸۴۰ دلار- رتبه‌ی فرصت‌های شغلی: ۴٫۱معمار راهکاربه نظر می‌رسد که این فرصت شغلی خیلی در کشورمان ایران شناخته شده نباشد و بسیاری از شرکت‌ها وظایف چنین فردی را به کسانی که در سایر حوزه‌ها فعالیت می‌کنند تفویض کنند. به طور کلی، معماری راهکار موقعیتی است که کارشناس این حوزه می‌بایست معماری یک سیستم نرم افزاری را تعریف و تشریح کند به گونه‌ای که دربرگیرنده‌ی نمای کلی سیستم باشد. - تعداد فرصت‌های شغلی: ۲,۹۰۶- متوسط پایه حقوق: ۱۱۹,۵۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۵توسعه دهنده‌ی موبایلیک توسعه دهنده‌ی موبایل، الزامات نرم افزار را به کدهای قابل برنامه ریزی و برنامه‌ها تبدیل می‌کند. معمولا یک توسعه دهنده‌ی موبایل در یک زمینه‌ی خاص دارای مهارت است، مانند توسعه‌ی تلفن همراه، نرم افزار‌های گرافیکی و نرم افزار کسب و کار. - تعداد فرصت‌های شغلی: ۲,۲۵۱- متوسط پایه حقوق: ۹۰,۰۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۸مدیر تولیدوظیفه‌ی یک مدیر تولید، یافتن پلی ارتباطی میان فناوری، کسب و کار و تجربه‌ی کاربری به منظور تولید محصولی ارزشمند و عملی است. این فرد، بر پروژه‌ها و نحوه‌ی پیشرفت آن‌ها نظارت می‌کند. - تعداد فرصت‌های شغلی: ۶,۶۰۷- متوسط پایه حقوق: ۱۰۶,۶۸۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۳مهندس نرم افزاریک مهندس نرم افزار، مسئول چرخه‌ی حیات نسخه‌ی جدید یا اصلاح شده‌ی یک نرم افزار است. وی همچنین بر روی طراحی، کاربرد، پشتیبانی و ارزیابی نرم افزار کار می‌کند. تعداد فرصت‌های شغلی: ۴۹,۲۷۰متوسط پایه حقوق: ۹۵,۰۰۰ دلاررتبه‌ی فرصت‌های شغلی: ۳٫۳مدیر تحلیل و ارزیابییک مدیر تحلیل و ارزیابی، نقشی کلیدی در طراحی استراتژی آینده‌ی یک شرکت دارد. این فرد برای رسیدن به نتیجه‌ای جامع و مطلوب از اطلاعات به دست آمده از سوابق و عملکرد گذشته‌ی شرکت بهره می‌برد. - تعداد فرصت‌های شغلی: ۹۸۲- متوسط پایه حقوق: ۱۰۵,۰۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۷مدیر توسعه‌ی نرم افزاراین فرد با پروژه‌های توسعه‌ی نرم افزار سر و کار دارد و بر جنبه‌های گوناگون کار مانند مراحل مختلف توسعه، مشتریان، مدیریت و فروش نظارت می‌کند. - تعداد فرصت‌های شغلی: ۱,۱۹۹- متوسط پایه حقوق: ۱۳۵,۰۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۴مدیر تضمین کیفیتیک مدیر تضمین کیفیت بر اجرای ضوابط مربوطه، روش‌های تولید و استاندارهای کیفیت یک محصول نظارت می‌کند. - تعداد فرصت‌های شغلی: ۳,۷۴۹- متوسط پایه حقوق: ۸۵,۰۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۴طراح تجربه‌ی کاربرییک طراح تجربه‌ی کاربری، مسئول ظاهر نهایی و نتیجه‌ی یک محصول است. این فرد برای این که اطمینان حاصل کند که محصول مورد نظر بهترین تجربه‌ی کاربری را فراهم می‌کند، روش‌های مختلفی را برای رسیدن به راهکار نهایی امتحان می‌کند. - تعداد فرصت‌های شغلی: ۸۶۳- متوسط پایه حقوق: ۹۱,۸۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۶معمار نرم افزاریک معمار نرم افزار، متخصصی است که کاربردهای یک نرم افزار را طراحی کرده و تعیین می‌کند که نرم افزار چه طور در یک پروژه یا شرکت، ایفای نقش خواهد کرد. - تعداد فرصت‌های شغلی: ۶۵۳- متوسط پایه حقوق: ۱۳۰,۰۰۰ دلار- رتبه‌ی فرصت‌های شغلی: ۳٫۴آیا فهرست بهترین مشاغل حوزه‌ی فناوری در سال ۲۰۱۶ می‌تواند به شما در انتخاب شغل آینده تان کمک کند؟ نظرات خود را با سایر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n","با وجود اینکه گوگل از زبان‌های java و kotlin برای توسعهٔ اپ‌های اندرویدی پشتیبانی می‌کنند، اما همواره دولوپرها این امکان را داشته و خواهند داشت تا با استفاده از دیگر زبان‌های برنامه‌نویسی نیز به توسعهٔ اپ بپردازند که از آن جمله می‌توان به c و javascript اشاره کرد اما یکی دیگر از این زبان‌ها ++c است که کاربردهای خاص خود را دارا است. در عین حال، این پرسش مطرح می‌شود که مزیت‌های استفاده از ++c برای توسعهٔ اپ‌های اندرویدی چیست؟ قبل از اینکه در سال ۲۰۱۷ زبان کاتلین توسط گوگل ساپورت شود، زبان جاوا در حوزهٔ توسعهٔ اپ‌های اندرویدی یکه‌تازی می‌کرد اما در زمینه‌هایی مانند بازی‌سازی، شبیه‌سازی فیزیکی و پردازش سیگنال زبانی همچون سی‌پلاس‌پلاس می‌تواند سریع‌تر و کارآمدتر باشد (البته تمام این کارها را با استفاده از فریمورک java native interface یا به اختصار jni می‌توانید انجام دهید.) همچنین تعداد زیادی از سورس‌کد بازی‌ها من‌جمله موتورهای بازی اپن‌سورس وجود دارند که برای شروع می‌توانید فهرست موجود بازی‌های اپن‌سورس در ویکیپدیا را بررسی کنید. البته همهٔ بازی‌ها با ++c نوشته نشده‌اند اما ابزارهای موجود، فرصت ادغام و ترکیب کدها را برای دولوپرها فراهم می‌کنند. به طور کلی می‌توان گفت که جاوا زبان خوبی است اما در عین حال برای توسعهٔ اپلیکیشن‌های اندرویدی زبانی همچون سی‌پلاس‌پلاس برتری‌هایی نسبت به جاوا دارا است که یکی از آن‌ها مدیریت بهتر حافظه است. سی‌پلاس‌پلاس به نوعی فرزند زبان سی محسوب می‌شود و نیاز به توضیح هم نیست که سی زبانی است که در زمینهٔ پرفرمونس شهرهٔ خاص و عام است که برای آشنایی بیشتر با مزیت‌های این زبان، می‌توانید به مقالهٔ آیا می‌دانید اگر زبان برنامه‌نویسی c نبود، چه بلایی سر دنیای نرم‌‌افزار می‌آمد! مراجعه نمایید. اگر دولوپر اندروید هستید، آیا تا به حال با استفاده از ndk و زبان‌های ++c یا c اقدام به توسعهٔ اپ‌های اندرویدی کرده‌اید؟ اگر این زمینه تجربه‌ای دارید، نظرات، دیدگاه‌ها و تجربیات خود را با سایر کاربران سکان آکادمی به اشتراک بگذارید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. در صورتی‌ که قبلا ثبت‌نام نموده‌اید، با کلیک روی دکمهٔ ورود وارد ناحیهٔ کاربری خود شوید و در غیر این‌ صورت ثبت‌نام نمایید. امکان بازگردانی پس از حذف وجود ندارد! میزبانی شده روی سرورهای کلود پارس‌پک\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TJf6T40glV5g"},"source":["# limit_number = 750\r\n","# data = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv',index_col=0)\r\n","# data = data.dropna().reset_index(drop=True)\r\n","# X = data[\"body\"].values.tolist()\r\n","# y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","# labels = []\r\n","# tag=[]\r\n","# for item in y['tag']:\r\n","#   labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","#   tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","# labels = list(set(labels))\r\n","# mlb = MultiLabelBinarizer()\r\n","# Y=mlb.fit_transform(tag)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"A59_7YPmnZgA","executionInfo":{"status":"ok","timestamp":1610981499489,"user_tz":-210,"elapsed":3375,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"ed7798a5-93fe-4f40-f855-98c16e5eaf56"},"source":["seq_len = [len(i.split()) for i in clean_x]\n","pandas.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f7feca0d160>"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"1JEmlyFMm6Tf","executionInfo":{"status":"ok","timestamp":1610703026242,"user_tz":-210,"elapsed":5380,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"14d106f7-eb76-4106-b875-501a7918b841"},"source":["seq_len = [len([j for j in i.split() if len(j)>2]) for i in X]\n","pandas.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fbcc2abb710>"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"PH3jCKaZsEWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610981509320,"user_tz":-210,"elapsed":954,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"3c833ddb-9862-4beb-c542-0b7c8872d982"},"source":["X_train, X_test, y_train, y_test = train_test_split(clean_x,Y , test_size=0.2)\r\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n","print('train: ', len(X_train) , '\\ntest: ', len(X_test) , '\\nval: ', len(X_val) ,\"\\ny_tain:\",len(y_train) )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train:  21399 \n","test:  7134 \n","val:  7134 \n","y_tain: 21399\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vei6iu9atmyd","colab":{"base_uri":"https://localhost:8080/","height":218,"referenced_widgets":["8d314ae5f2144f6e804abaee4f9eb10a","e748971edf0047b4b63832a7f53c484e","a2540867df34473aae60da13bcc6b818","5d65ff2af99c4b5b9bbfcfcfc23efa7e","7559c53fa63d48e99315b44494e78523","527d06e1c660454f8b00df3ed7824f7e","aa56b0a445e541d29801b0a29e9ea806","c98a25ea80174a57943c75d2b99d6ca0","b7dff655f33f4758b89e9500988c0a37","ed1fd9ee618c4ded8f7f25d15a8ccc9c","04ac665566264bdc9bd65b75f8a5ec52","bedaf64ea2c64bad97f9b9cb6cd782af","e9ca40f0494c4d6d9997cb69b5a1a04e","6087d4f9cc444e8f8717a6246617f985","3ca7a4082acf44a4b8e6d82ef4a7e1d0","caeebe376c6c40c59c998aca19dea03f","b5625c69213e458691ccfed2097f8ec9","cc71e78aec6441a6810b7be6ccf04a83","63df0950f41e4b94b0e3930fa72c086e","70887e0cbad14c13b95b12cfab23ab8f","c03426209cb54eefbe45585ffb9cc1cc","2e35de3d7a244a4392a3ab51e51fa7f6","460b60a8050e4407bdcf1d8926fdf2e4","03a156de915b4eed97e1f774f159e1a6","73a4f8c2ee0245a3bf0369d38afc308c","609652379cad45aab9e3e3ba09438f14","758cda52684b4253a8e6b015ff808806","b9b9be3b94ad485aa3dddf2f12cfce0d","d83bd47a520447af8aa711b0bbb9312d","470eaf053a1d4e3aba3390cb653860a7","15fafccaf5584463acfbdafeab6e20f9","57b7cc364afc46988c03f04537c6b97d"]},"executionInfo":{"status":"ok","timestamp":1610981513353,"user_tz":-210,"elapsed":2927,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"a3a13810-5a5b-4114-ef04-2b9f36b8e90c"},"source":["##we would load the tokenizer\r\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d314ae5f2144f6e804abaee4f9eb10a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1441.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7dff655f33f4758b89e9500988c0a37","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1198122.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5625c69213e458691ccfed2097f8ec9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73a4f8c2ee0245a3bf0369d38afc308c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7wdU0zejDNq","executionInfo":{"status":"ok","timestamp":1610800065599,"user_tz":-210,"elapsed":1335,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"e37590c5-0282-4f4c-ddd1-ef9b8828c088"},"source":["#example\r\n","text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\r\n","tokenized=tokenizer.tokenize(X_train[0])\r\n","input_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n","print(tokenized)\r\n","print(input_ids)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['عصری', 'زندگی', 'می', '\\\\', 'xa', '##d', '##کنیم', 'کره', 'زمین', 'دهک', '##د', 'جهانی', 'فراهم', 'توانایی', 'هر', '\\\\', 'xa', '##d', '##جایی', 'کره', 'خاکی', 'عرضه', 'توانایی', 'بهره', '\\\\', 'xa', '##d', '##برداری', 'ارتباطی', 'منطقه', 'جغرافیایی', 'اصطلاحا', 'کار', 'می', '\\\\', 'xa', '##d', '##کنند', 'فریلنسر', 'ازادکار', 'می', '\\\\', 'xa', '##d', '##شود', 'دنبال', 'ازادی', 'زندگی', 'حرفه', '\\\\', 'xa', '##d', 'داشتهباشید', 'بتوانید', 'متناسب', 'سلایق', 'کار', 'درامد', 'کسب', 'فریلنس', 'ازادکار', '##ی', 'تجربه', 'ای', 'ناب', 'خواهدبود', 'فریلنسر', 'شاخصه', '\\\\', 'xa', '##d', 'ها', 'تقویت', 'عده', 'ی', 'کثیری', 'فریلنسر', 'می', '\\\\', 'xa', '##d', 'نامند', 'مشکل', 'اساسی', 'انتصاب', '\\\\', 'xa', '##d', '##هم', 'توانایی', 'برد', 'جذب', 'پروژه', '\\\\', 'xa', '##d', 'ها', 'اساس', 'مهارت', 'هایشان', 'ازادکار', 'ازاد', 'کار', 'موضوعی', 'قصد', 'مقاله', 'صحبت', 'ازادکار', 'کار', 'در', '\\\\', 'xa', '##d', '##ا', '\\\\', 'xa', '##d', '##مد', 'کسب', 'مقاله', 'نیازمندی', '\\\\', 'xa', '##d', 'ها', 'فریلنسر', '\\\\', 'xa', '##d', '##هایی', 'صحبت', 'می', '\\\\', 'xa', '##d', 'قصد', 'بازار', '\\\\', 'xa', '##d', 'ها', 'المللی', 'کار', '\\\\', 'xa', '##d', '##عنوان', 'فریلنسر', 'المللی', 'مقرون', '\\\\', 'xa', '##d', '##صرفه', 'دفتر', 'دايمی', 'فریلنس', 'المللی', 'موضوع', 'هزینه', '\\\\', 'xa', '##d', 'ها', 'برایتان', 'ایجاد', 'می', '\\\\', 'xa', '##d', 'چندانی', 'می', '\\\\', 'xa', '##d', 'کارتان', 'خانه', 'محیط', 'کار', 'فریلنسر', '\\\\', 'xa', '##d', 'ها', 'بهشت', 'می', '\\\\', 'xa', '##d', 'فضای', 'کار', 'اشتراکی', 'محیط', 'کاری', 'سازمان', '\\\\', 'xa', '##d', 'ها', 'ثانویه', 'تدارک', '\\\\', 'xa', '##d', '##صورت', 'رایگان', 'هزینه', 'حداقلی', 'اختیار', 'مشاغلی', 'قرار', 'می', '\\\\', 'xa', '##d', '##گیرد', 'موضوع', 'فعالیت', 'به', '\\\\', 'xa', '##d', 'فضا', 'محیط', 'زیبایی', 'برگزاری', 'جلسات', 'کاری', 'موضوع', '\\\\', 'xa', '##d', '##واقع', 'بازار', 'کار', 'مدرسه', 'حساب', 'می', '\\\\', 'xa', '##d', 'فضای', 'کار', 'اشتراکی', 'نشسته', '\\\\', 'xa', '##d', 'مشغول', 'کار', 'برنامه', 'پروژه', '\\\\', 'xa', '##d', '##ای', 'احتمال', 'پروژه', 'جدیدی', 'معرفی', 'محیط', 'کار', 'دریافت', 'معرفی', 'می', '\\\\', 'xa', '##d', 'شبیه', 'rss', 'feed', 'دقت', 'بالا', '\\\\', 'xa', '##d', '##تر', 'پیشنهاد', 'می', '\\\\', 'xa', '##d', 'رزومه', '\\\\', 'xa', '##d', 'ساز', '\\\\', 'xa', '##d', 'ها', 'عمومی', 'وبسایت', 'ها', 'انتشار', 'رزومه', 'ی', 'فراهم', 'می', '\\\\', 'xa', '##d', '##کنند', 'جامعه', 'مجازی', 'لینکدین', 'لینکدین', 'جامعه', 'مجازی', 'اساس', 'زندگی', 'کاری', 'حرفه', '\\\\', 'xa', '##d', '##ای', 'طراحی', 'ارتباط', 'ها', 'اساس', 'مهارت', 'اتفاق', 'می', '\\\\', 'xa', '##d', '##افت', '##د', 'کار', '\\\\', 'xa', '##d', '##هایی', 'می', '\\\\', 'xa', '##d', '##دهند', 'صحبت', 'جامعه', 'ی', 'مجازی', 'بهشت', 'کارفرمایان', 'می', '\\\\', 'xa', '##d', 'راحتی', 'توانمندی', 'نیازشان', 'پروفایل', 'لینکدین', '\\\\', 'xa', '##d', '##خوبی', 'تکمیل', 'کرده', '##باشید', 'دستیابی', 'خواهید', '##بود', 'نمونه', 'نگاهی', 'پروفایل', '##در', 'لینکدین', 'بی', '\\\\', 'xa', '##d', '##ندا', '##زید', 'جان', 'کارم', '##ک', 'برنامه', '\\\\', 'xa', '##d', 'نویس', '\\\\', 'xa', '##d', 'ها', 'دنیاست', 'رزومه', 'انلاین', 'ازادکار', 'پیشنهاد', 'ایجاد', 'رزومه', 'گوگل', 'داکس', 'می', '\\\\', 'xa', '##d', '\\\\', 'xa', '##d', '##راحتی', 'رزومه', 'ایجاد', 'حساب', 'گوگل', 'ذخیره', 'می', '\\\\', 'xa', '##d', 'می', '\\\\', 'xa', '##d', '##توانی', '##د', 'هر', '\\\\', 'xa', '##d', '##جایی', 'هر', '\\\\', 'xa', '##d', '##کسی', 'اشتراک', '\\\\', 'xa', '##d', 'گوگل', 'داکس', 'رایگان', 'نامحدود', 'برایتان', 'ایمیل', 'تبلیغاتی', 'ارسال', 'نمی', '\\\\', 'xa', '##d', '##کند', 'جستجوی', 'عبارت', 'رزومه', 'انلاین', 'می', '\\\\', 'xa', '##d', '##توانی', '##د', 'ابزار', '\\\\', 'xa', '##d', 'ها', 'ایجاد', 'رزومه', 'انلاین', 'ازادکار', 'می', '\\\\', 'xa', '##d', 'اشکال', 'مختلفی', 'راه', '\\\\', 'xa', '##d', 'ارتباط', 'ایجاد', 'تلگرام', 'فیسبوک', 'راحت', '\\\\', 'xa', '##d', 'تر', 'دوست', 'داشتهباشید', 'نشانی', 'اینستاگرام', 'ارتباطی', 'منتشر', 'اشکالی', 'هرطور', 'دوست', 'می', '\\\\', 'xa', '##d', '##توانی', '##د', 'معرفی', 'قرار', 'وبسایت', 'ماه', 'اینستاگرام', '\\\\', 'xa', '##d', '##تان', 'ارتباطی', 'حساب', 'اینستاگرام', '\\\\', 'xa', '##d', '##تان', 'خواندن', 'بخش', 'ها', 'بعدی', 'مقاله', 'بلاگ', 'کوین', 'دنبال', 'مقاله', 'فریلنسر', 'المللی', 'بخوانید', 'پایانی', 'مقاله', 'فریلنسر', 'المللی', 'بخوانید', 'blog', 'cc', '##ap', '##coin', 'com']\n","[32235, 3351, 2793, 1022, 52017, 2059, 8224, 4958, 3164, 9829, 2013, 3553, 4319, 4992, 2937, 1022, 52017, 2059, 6818, 4958, 11479, 4060, 4992, 3893, 1022, 52017, 2059, 4989, 6443, 3438, 6986, 14324, 2867, 2793, 1022, 52017, 2059, 4784, 79317, 58405, 2793, 1022, 52017, 2059, 20199, 3777, 4880, 3351, 10153, 1022, 52017, 2059, 70122, 8757, 7523, 23763, 2867, 4014, 3927, 96874, 58405, 2003, 4246, 2938, 5107, 39230, 79317, 30918, 1022, 52017, 2059, 5929, 5070, 10314, 1442, 20507, 79317, 2793, 1022, 52017, 2059, 50038, 3404, 4664, 12265, 1022, 52017, 2059, 2993, 4992, 4073, 4727, 4484, 1022, 52017, 2059, 5929, 3561, 5896, 11789, 58405, 3912, 2867, 7934, 4790, 6012, 4386, 58405, 2867, 2786, 1022, 52017, 2059, 2006, 1022, 52017, 2059, 3231, 3927, 6012, 16700, 1022, 52017, 2059, 5929, 79317, 1022, 52017, 2059, 3047, 4386, 2793, 1022, 52017, 2059, 4790, 3179, 1022, 52017, 2059, 5929, 5424, 2867, 1022, 52017, 2059, 25553, 79317, 5424, 13033, 1022, 52017, 2059, 31772, 5236, 7660, 96874, 5424, 3444, 4001, 1022, 52017, 2059, 5929, 7129, 3280, 2793, 1022, 52017, 2059, 7883, 2793, 1022, 52017, 2059, 13340, 3764, 3952, 2867, 79317, 1022, 52017, 2059, 5929, 8848, 2793, 1022, 52017, 2059, 4049, 2867, 17864, 3952, 3972, 3270, 1022, 52017, 2059, 5929, 12042, 10369, 1022, 52017, 2059, 5548, 6552, 4001, 20370, 4209, 27929, 2959, 2793, 1022, 52017, 2059, 3524, 3444, 3426, 2789, 1022, 52017, 2059, 5517, 3952, 5714, 5003, 7295, 3972, 3444, 1022, 52017, 2059, 83397, 3179, 2867, 4752, 4206, 2793, 1022, 52017, 2059, 4049, 2867, 17864, 9535, 1022, 52017, 2059, 4908, 2867, 3329, 4484, 1022, 52017, 2059, 2784, 3884, 4484, 5019, 3852, 3952, 2867, 3552, 3852, 2793, 1022, 52017, 2059, 5101, 85585, 60482, 5884, 3683, 1022, 52017, 2059, 2838, 4360, 2793, 1022, 52017, 2059, 22056, 1022, 52017, 2059, 3091, 1022, 52017, 2059, 5929, 3885, 8223, 5929, 3986, 22056, 1442, 4319, 2793, 1022, 52017, 2059, 4784, 3780, 5427, 32573, 32573, 3780, 5427, 3561, 3351, 3972, 10153, 1022, 52017, 2059, 2784, 3841, 3490, 5929, 3561, 5896, 3929, 2793, 1022, 52017, 2059, 4270, 2013, 2867, 1022, 52017, 2059, 3047, 2793, 1022, 52017, 2059, 44988, 4386, 3780, 1442, 5427, 8848, 14094, 2793, 1022, 52017, 2059, 5842, 9787, 36480, 18005, 32573, 1022, 52017, 2059, 81391, 5770, 3027, 57637, 7111, 5655, 3290, 4723, 7223, 18005, 3055, 32573, 2883, 1022, 52017, 2059, 49139, 10928, 3607, 17793, 2004, 3329, 1022, 52017, 2059, 4373, 1022, 52017, 2059, 5929, 13229, 22056, 5442, 58405, 4360, 3280, 22056, 5934, 62010, 2793, 1022, 52017, 2059, 1022, 52017, 2059, 12746, 22056, 3280, 4206, 5934, 5605, 2793, 1022, 52017, 2059, 2793, 1022, 52017, 2059, 44300, 2013, 2937, 1022, 52017, 2059, 6818, 2937, 1022, 52017, 2059, 5405, 6468, 1022, 52017, 2059, 5934, 62010, 6552, 13887, 7129, 7366, 8523, 4855, 3821, 1022, 52017, 2059, 11241, 8457, 4226, 22056, 5442, 2793, 1022, 52017, 2059, 44300, 2013, 4766, 1022, 52017, 2059, 5929, 3280, 22056, 5442, 58405, 2793, 1022, 52017, 2059, 7666, 5042, 3148, 1022, 52017, 2059, 3490, 3280, 7563, 10968, 6309, 1022, 52017, 2059, 3088, 4219, 70122, 7150, 9172, 6443, 3692, 16043, 53160, 4219, 2793, 1022, 52017, 2059, 44300, 2013, 3852, 2959, 8223, 3125, 9172, 1022, 52017, 2059, 3384, 6443, 4206, 9172, 1022, 52017, 2059, 3384, 6388, 3108, 5929, 4630, 6012, 37108, 13202, 3777, 6012, 79317, 5424, 5546, 6591, 6012, 79317, 5424, 5546, 80971, 25987, 7051, 63682, 8596]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az4rwU0l5ECn"},"source":["# encode text\r\n","sent_id = tokenizer.batch_encode_plus(X_train[:10], padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNRU-SH65ZEE","executionInfo":{"status":"ok","timestamp":1610510956416,"user_tz":-210,"elapsed":1393,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"05ce4767-dd4d-42bb-88e1-569ecfe0e8d1"},"source":["sent_id"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 5071, 4790, 32701, 3911, 19910, 74701, 3329, 52810, 2030, 4613, 57968, 7341, 4957, 3916, 3911, 4613, 9607, 39230, 4378, 28724, 36539, 5929, 15619, 20936, 41215, 2002, 5102, 57637, 6012, 74701, 5565, 2867, 5929, 26040, 6501, 4484, 64198, 5438, 13334, 23344, 58164, 6012, 6820, 58164, 4766, 6667, 33103, 35204, 6921, 37745, 23460, 4366, 5929, 2959, 45309, 8146, 4856, 4613, 2938, 5782, 4394, 2959, 45098, 60379, 2061, 45309, 8146, 29511, 5929, 52446, 11231, 2867, 6921, 8094, 52810, 64831, 35138, 33103, 62321, 4366, 5929, 2959, 27409, 45309, 8146, 23460, 4613, 35667, 33103, 62321, 6921, 37745, 4366, 5929, 33336, 2959, 27409, 45309, 8146, 52810, 64831, 35138, 4484, 18009, 92228, 74701, 37458, 48687, 52446, 22157, 45498, 29420, 4090, 23709, 2050, 30750, 7796, 6945, 11999, 32701, 3528, 10672, 35428, 17389, 5655, 3030, 5071, 3329, 52810, 2030, 4613, 44594, 18230, 4790, 52248, 7120, 28678, 10741, 3531, 3916, 45309, 62071, 88012, 5929, 74701, 3462, 5289, 7222, 5071, 96817, 88012, 67519, 70473, 11332, 54934, 5690, 3328, 6945, 3329, 10884, 39230, 45309, 8146, 5754, 4459, 88012, 5929, 52810, 64831, 35138, 17687, 3796, 5929, 98322, 33103, 88012, 2032, 6921, 8094, 98322, 17358, 4366, 5929, 2959, 45309, 8146, 88012, 54176, 2032, 8094, 45309, 8146, 5444, 22367, 10672, 43300, 28724, 88012, 43815, 47224, 11721, 54934, 4366, 5929, 2959, 42528, 45309, 8146, 52810, 5642, 52248, 7120, 28678, 10741, 11231, 12139, 4296, 28724, 2959, 54729, 23116, 17687, 9706, 44594, 71030, 3778, 52810, 32590, 29048, 5717, 7152, 52810, 2030, 18230, 13459, 4366, 5929, 6921, 52810, 32590, 29048, 33103, 88012, 2959, 42528, 45309, 8146, 88012, 54176, 2032, 8094, 28807, 19921, 30304, 11231, 5731, 3629, 10672, 4856, 32701, 4836, 27409, 71030, 3778, 4613, 49225, 89243, 11802, 7843, 10672, 18009, 28807, 19921, 30304, 15766, 10672, 5503, 49225, 3629, 10672, 88012, 38793, 3318, 3528, 5754, 6749, 71030, 3778, 52810, 60791, 2040, 13459, 4366, 5929, 71030, 3778, 2959, 42528, 45309, 8146, 71030, 3778, 52810, 2030, 3251, 5690, 4070, 4856, 71030, 3778, 52810, 2030, 10884, 8146, 52810, 2030, 12182, 13453, 14142, 4568, 7843, 18009, 96566, 5731, 3629, 67682, 2959, 35814, 18009, 28807, 19921, 30304, 10884, 45309, 8146, 52248, 7120, 28678, 10741, 11231, 18430, 3629, 10672, 7162, 10672, 12139, 10672, 18009, 98346, 96109, 18489, 10884, 45309, 8146, 52810, 2030, 28724, 8543, 10672, 28724, 52248, 7120, 28678, 10741, 7162, 10207, 4459, 88292, 88012, 5929, 6921, 52810, 64831, 35138, 10884, 32717, 45309, 17979, 23116, 74701, 5842, 88292, 88012, 5929, 19089, 4629, 5842, 50317, 28552, 5796, 71442, 4], [2, 5071, 8456, 23639, 5929, 25625, 36108, 32088, 30765, 13870, 86836, 24091, 4209, 4957, 2959, 42528, 4713, 48212, 4089, 3475, 28552, 4957, 19791, 5891, 39298, 88184, 86836, 24091, 25625, 6552, 4175, 58164, 3521, 10602, 25625, 3780, 3329, 12035, 8387, 3841, 3475, 5224, 7178, 11231, 25625, 58164, 23639, 5929, 11231, 5232, 5929, 3191, 58164, 16958, 2795, 79566, 2011, 15615, 39298, 5891, 7765, 24578, 35088, 7223, 68981, 6839, 22338, 2816, 4158, 2805, 7579, 23639, 25625, 5224, 17687, 4241, 10672, 7164, 15407, 23639, 5929, 32701, 3852, 35088, 13443, 10130, 25554, 23639, 5484, 4366, 5929, 25625, 17802, 13113, 12702, 6945, 22367, 3083, 3404, 70724, 2003, 12139, 17802, 3642, 22367, 10672, 8152, 12702, 23639, 5596, 58164, 5939, 5929, 6777, 50980, 14878, 22394, 2061, 7796, 45498, 82343, 46718, 25625, 86836, 24091, 3528, 17420, 21503, 6310, 13443, 10130, 25554, 13113, 15307, 15611, 23639, 6820, 5754, 6917, 14745, 48212, 28817, 3229, 6921, 48212, 4274, 17420, 58164, 6921, 4366, 5929, 27989, 14599, 13113, 6945, 4366, 5929, 3528, 10672, 70724, 2003, 32701, 22367, 3083, 23639, 5929, 4942, 45309, 62071, 33192, 4179, 8340, 20208, 3419, 5929, 3329, 12035, 6921, 4366, 5929, 48212, 2959, 64856, 19327, 10672, 18781, 46158, 5350, 5929, 6492, 5929, 11231, 74701, 23157, 26604, 98637, 2033, 3419, 5929, 3329, 12035, 1876, 3064, 48212, 2867, 4360, 18781, 46158, 5350, 5929, 4089, 39298, 27989, 3191, 9567, 5929, 3852, 61145, 19994, 6312, 24003, 2805, 27989, 7106, 2867, 8417, 81884, 52810, 2030, 53895, 2032, 38793, 13334, 47489, 23169, 28552, 4957, 4241, 27989, 7098, 84506, 2805, 29121, 4366, 5929, 6537, 81884, 5929, 6575, 28552, 4957, 3952, 54570, 65740, 4816, 24843, 23639, 52810, 2030, 22821, 36649, 3647, 3929, 3821, 43481, 2008, 23639, 52810, 2030, 28552, 4957, 2959, 10514, 10884, 8076, 57941, 58164, 17687, 4482, 6917, 8094, 32458, 22821, 36649, 5806, 37400, 30387, 32538, 3903, 11231, 52810, 2030, 5929, 5754, 10207, 58164, 17687, 4482, 23639, 5929, 4942, 23639, 50145, 46614, 22367, 3083, 96291, 77626, 4735, 11231, 23639, 3469, 12702, 8417, 4766, 10884, 5954, 4957, 15305, 15305, 12101, 28552, 4957, 4062, 17521, 3475, 5839, 36189, 11231, 22930, 23180, 2867, 14114, 14829, 12139, 3640, 3910, 5929, 3640, 6173, 17687, 9258, 23116, 17476, 26029, 12453, 23639, 22930, 49722, 13654, 4209, 4957, 2959, 3083, 26029, 12453, 12702, 23639, 22930, 49722, 58164, 4905, 5929, 24954, 9258, 28302, 6173, 86836, 24091, 4482, 5071, 23639, 5929, 11904, 2938, 32701, 3852, 48476, 2816, 36711, 12139, 50715, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 65312, 8478, 1457, 1393, 1460, 1392, 65312, 8478, 1460, 1393, 1457, 1392, 1455, 2034, 65312, 8478, 1462, 1393, 1462, 1392, 43428, 41740, 8596, 40506, 2077, 18481, 61527, 2041, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4900, 41086, 3351, 4172, 3381, 11424, 3318, 4485, 4900, 3948, 3432, 15186, 4900, 3948, 4179, 7879, 8179, 72591, 2793, 3689, 4238, 3916, 7602, 2967, 5929, 7154, 72591, 4449, 4900, 3640, 76537, 6604, 7154, 5929, 6914, 5929, 3432, 5939, 5929, 4209, 3432, 8872, 9060, 2871, 3145, 3404, 3432, 5939, 5929, 3625, 3634, 5725, 78884, 2783, 5939, 5929, 7882, 4586, 5929, 5725, 2793, 3188, 4449, 3270, 72591, 15826, 35425, 47607, 10197, 17989, 94808, 83327, 42629, 2032, 3211, 5817, 3521, 5725, 4900, 5484, 6429, 5939, 5939, 5929, 2959, 2793, 9933, 5939, 3376, 32270, 23921, 47308, 6825, 3381, 3630, 2793, 6414, 4639, 7129, 5823, 16020, 3027, 3145, 3905, 4639, 9913, 6262, 67486, 4238, 3531, 6951, 3561, 13191, 5725, 8179, 23921, 68115, 42985, 2048, 3470, 37039, 41627, 12454, 63493, 4484, 4188, 3439, 2844, 5117, 2938, 3221, 2793, 10941, 3361, 78884, 2783, 13038, 42703, 4449, 78884, 2783, 5973, 2930, 5929, 8204, 3561, 3632, 4463, 5973, 3171, 19319, 3474, 3952, 2867, 3270, 5725, 4386, 9310, 3821, 74489, 10871, 3733, 2867, 3473, 4247, 18586, 24201, 5929, 5042, 3525, 24201, 10307, 24201, 5108, 4980, 8349, 5140, 37142, 3364, 4459, 4247, 18586, 4108, 2793, 6414, 5484, 6429, 3926, 10023, 18586, 5850, 37039, 41627, 3270, 5101, 11002, 5983, 8097, 7971, 43624, 72591, 3280, 5399, 4872, 6637, 6825, 12330, 3310, 5939, 5929, 3432, 2871, 3145, 6378, 3661, 2910, 3504, 4912, 6198, 6724, 4449, 4900, 18275, 3201, 9324, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 37620, 4884, 10755, 9258, 5929, 19911, 6596, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 53615, 5224, 15407, 3521, 3916, 9825, 3810, 4160, 2904, 3073, 3444, 4924, 3521, 6542, 22201, 29808, 22686, 4816, 62030, 43059, 6022, 7065, 5834, 5929, 3521, 53615, 3043, 7285, 25029, 2793, 5655, 6429, 16477, 53615, 31792, 3916, 4394, 5929, 3251, 5071, 3287, 6012, 2938, 6629, 4139, 8369, 53615, 3640, 16004, 3000, 2811, 9622, 3521, 31041, 6012, 3835, 3347, 5733, 3521, 3916, 7443, 53615, 9706, 53615, 3786, 5524, 8369, 16477, 6734, 4139, 16477, 53615, 2793, 3689, 3251, 3640, 2793, 17355, 2015, 4532, 9585, 4139, 16477, 53615, 5785, 6012, 6629, 4139, 8369, 53615, 3364, 3439, 3475, 5839, 53615, 3444, 4449, 6095, 2959, 3171, 3145, 5209, 5455, 4244, 3821, 3435, 4449, 5302, 4386, 4449, 5209, 4900, 8052, 4639, 4816, 53615, 53615, 3828, 2793, 74489, 4449, 53615, 6378, 63432, 4639, 3525, 3905, 53615, 3828, 7580, 4605, 5725, 5522, 4485, 53615, 3521, 5929, 3114, 9825, 6748, 7580, 5725, 8246, 22880, 4449, 4394, 53615, 2003, 4347, 7688, 53615, 3521, 4175, 3780, 5020, 3475, 3810, 3310, 8872, 5455, 7276, 53615, 3280, 5076, 4089, 4816, 4367, 3905, 8707, 4909, 2938, 3211, 4394, 7078, 5418, 2910, 2867, 5418, 3404, 7670, 3100, 4981, 3966, 4394, 5102, 57637, 12512, 4394, 5655, 3390, 5717, 12512, 3640, 4394, 7502, 3966, 12512, 6839, 3640, 4394, 6137, 4001, 4316, 5040, 3100, 12512, 4394, 4084, 13294, 36465, 3531, 3927, 2867, 4482, 5399, 7212, 3475, 11176, 53615, 6542, 7919, 4816, 4394, 5929, 53615, 2003, 7934, 4394, 4084, 6501, 4394, 4995, 6206, 3422, 5929, 23763, 2793, 70933, 4394, 5842, 5596, 5094, 23639, 92809, 4816, 12218, 8417, 2910, 2867, 4995, 21250, 39298, 79757, 2783, 5054, 62346, 13459, 3329, 17012, 3475, 11176, 53615, 4001, 3486, 15615, 5094, 5929, 23639, 5929, 53615, 2003, 3100, 4981, 3419, 5929, 46558, 30466, 23460, 46227, 48212, 3329, 4373, 53615, 6947, 4856, 5520, 4995, 7014, 2938, 4394, 3280, 6012, 3347, 5733, 3521, 3916, 3381, 16477, 3640, 2959, 54074, 3966, 53615, 5369, 5145, 2793, 8096, 66823, 3810, 3494, 8369, 16477, 8204, 4383, 53615, 7152, 4394, 5929, 4360, 4909, 3211, 3966, 53615, 6672, 3625, 3318, 4459, 3229, 2793, 31149, 4394, 3521, 53615, 5040, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6506, 4712, 4565, 5929, 4428, 7254, 5574, 7043, 6506, 10783, 5506, 5206, 7332, 7648, 3873, 3469, 6506, 4712, 3665, 5929, 2959, 6168, 13883, 3475, 3329, 5929, 7164, 6545, 4565, 3541, 5929, 5042, 5054, 6506, 4712, 3329, 6259, 4006, 15407, 3469, 9321, 8246, 2793, 5655, 3469, 6506, 4712, 7502, 12158, 6907, 5929, 6506, 13533, 7060, 4683, 7649, 5929, 4656, 5988, 4905, 5929, 7670, 3625, 29082, 5223, 3329, 5929, 4663, 6160, 5407, 3287, 4778, 3169, 4869, 3768, 9730, 38930, 88706, 2959, 13394, 4603, 6712, 4778, 6802, 7316, 2793, 31149, 10132, 5407, 3287, 4241, 3329, 23211, 53967, 8357, 4454, 4883, 73527, 46634, 81397, 49349, 2039, 3179, 3553, 6506, 4712, 2844, 5797, 7001, 1455, 1393, 1458, 3700, 8108, 5473, 2904, 3073, 2844, 30791, 1462, 1393, 1462, 3700, 5493, 8973, 4066, 3205, 3329, 6259, 4321, 7670, 7014, 11748, 12132, 46567, 8457, 7456, 7670, 4816, 4390, 7332, 2838, 13375, 2838, 3475, 11176, 3329, 5929, 6545, 7753, 20638, 2938, 6506, 4712, 9859, 3639, 3625, 3971, 3927, 3310, 4992, 5929, 4981, 3329, 5929, 5407, 3287, 3280, 3983, 7273, 5574, 3088, 9713, 10287, 9321, 4274, 5929, 6076, 3625, 17048, 5757, 3661, 7755, 25548, 7401, 4497, 4472, 3088, 4348, 6506, 3945, 3911, 3911, 6404, 7152, 3945, 3911, 3777, 6029, 4820, 2013, 35065, 2003, 6506, 4712, 3211, 3052, 5929, 23180, 6506, 13533, 3911, 3280, 3027, 3145, 2793, 31149, 14792, 3434, 3376, 6506, 13533, 16917, 16764, 34151, 58212, 3280, 3318, 4924, 3475, 11176, 6259, 4875, 5929, 3329, 5929, 16917, 2032, 40176, 45671, 23180, 4723, 3475, 11176, 5311, 2793, 9868, 4834, 10937, 4778, 6802, 3640, 37990, 5076, 4175, 22930, 3911, 2867, 5929, 4980, 4788, 4875, 5929, 22930, 6917, 6552, 5859, 2793, 3689, 11107, 5934, 5582, 4778, 11014, 74978, 23180, 5929, 5754, 2867, 4207, 22930, 3318, 11107, 16917, 4206, 16917, 40176, 58964, 4247, 23180, 22930, 45671, 4303, 4905, 5929, 3531, 9938, 5929, 4175, 4274, 5929, 5861, 6506, 13533, 3521, 4383, 5929, 4980, 5311, 2793, 9868, 3280, 3911, 10755, 3945, 5929, 5506, 3469, 2910, 3521, 4383, 5929, 5574, 16477, 3789, 5783, 9408, 6907, 5929, 7878, 11384, 3329, 6506, 13533, 15720, 3100, 3475, 6506, 4712, 4875, 5929, 6783, 3625, 5077, 8359, 9007, 7060, 4683, 4411, 3171, 5929, 5506, 5929, 4411, 2959, 5392, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 79317, 5929, 3329, 3972, 10953, 79317, 5896, 5929, 9227, 3928, 11809, 11742, 7440, 3329, 5102, 18421, 79317, 3047, 2793, 4564, 3927, 16069, 3598, 3329, 2938, 2844, 3972, 2793, 5655, 3927, 16069, 4554, 2844, 3944, 5781, 4788, 62735, 2844, 3378, 5071, 14313, 5929, 5020, 3329, 5929, 5020, 3927, 16069, 3351, 6258, 14313, 5929, 5100, 5323, 2904, 3939, 4370, 5929, 6095, 2959, 2793, 9933, 14313, 5676, 3329, 3927, 16069, 91722, 3620, 3088, 7440, 3329, 6325, 4977, 13809, 13216, 25643, 5754, 5140, 4613, 8963, 7764, 5000, 3351, 4224, 3972, 3280, 5653, 3640, 2844, 28140, 15233, 6428, 6428, 3385, 15407, 4014, 3972, 5891, 2793, 39316, 16417, 5929, 2844, 6734, 3821, 3168, 7690, 13248, 5158, 3944, 3640, 4788, 5100, 3927, 16069, 2844, 3944, 5100, 6650, 5757, 3789, 5783, 3940, 5100, 27285, 12179, 5100, 5597, 1044, 7291, 89216, 1038, 24839, 4205, 12376, 5114, 25136, 23687, 1026, 24839, 6471, 90653, 19324, 1043, 24219, 3465, 62197, 36456, 2040, 17395, 16799, 14320, 3280, 3792, 5659, 2910, 47684, 4713, 3789, 5783, 15723, 4196, 12266, 36132, 5100, 7478, 12266, 4238, 6429, 6839, 3473, 24810, 16650, 3736, 12822, 12822, 5781, 2844, 3944, 2793, 11543, 3125, 6180, 4790, 32573, 10153, 2938, 3777, 9719, 2793, 11543, 2867, 15490, 3280, 4825, 3625, 2793, 6150, 2844, 19987, 3789, 5783, 5100, 3521, 5100, 5653, 5929, 3850, 3280, 5358, 4630, 3521, 5929, 4386, 5690, 3328, 3287, 3280, 2844, 3972, 3469, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3500, 4060, 4259, 7358, 67913, 7834, 3821, 5398, 2013, 8707, 8900, 4207, 7358, 4816, 11080, 65925, 3186, 67913, 3625, 3573, 4801, 3286, 3521, 4383, 5224, 7358, 2930, 11080, 55355, 2006, 13222, 5342, 21308, 5680, 4139, 4884, 12675, 10357, 19209, 4825, 18571, 5929, 4219, 16047, 7358, 5094, 4207, 2793, 3689, 1876, 2949, 5484, 6429, 7753, 94425, 3841, 20411, 9497, 4207, 7358, 33962, 3251, 5369, 5690, 2881, 1876, 2949, 7358, 23073, 8757, 5524, 3573, 14273, 67913, 5574, 7043, 3205, 7358, 2959, 3171, 2822, 1876, 3035, 3852, 2793, 17355, 2015, 3500, 4060, 7358, 3205, 3091, 5929, 11457, 5929, 3205, 3625, 6542, 5929, 6909, 3525, 3205, 9508, 5929, 9255, 7010, 5200, 4157, 23073, 7358, 14335, 5929, 3151, 3145, 23120, 36537, 18903, 84081, 25837, 3205, 7358, 4735, 38611, 3205, 20424, 4207, 5929, 9111, 4816, 4360, 14686, 3318, 3205, 7358, 2044, 7510, 5574, 7043, 11827, 6937, 16945, 6921, 65745, 4497, 7843, 4390, 3205, 3500, 7214, 11384, 79249, 5596, 3521, 72642, 4321, 3205, 5770, 2793, 31149, 5524, 7358, 2044, 11817, 5891, 4246, 5891, 4246, 32717, 3521, 6676, 12944, 9545, 6201, 67948, 7214, 6029, 7843, 17914, 21396, 6075, 17914, 19209, 3229, 6917, 67509, 5785, 4497, 4482, 7358, 3205, 32717, 94425, 5311, 42375, 12675, 4565, 10357, 7358, 7510, 94425, 16945, 17914, 6921, 91942, 4497, 7843, 4321, 5596, 3500, 4426, 12675, 10357, 5655, 3290, 1876, 12675, 10357, 7358, 5596, 94425, 3499, 39230, 7843, 17914, 11384, 3528, 4305, 5929, 19209, 4209, 2959, 5596, 5596, 94425, 2793, 31149, 12675, 18571, 5929, 7358, 3205, 7173, 7164, 15407, 18571, 5929, 7358, 2793, 31149, 19209, 70122, 1876, 2965, 7510, 94425, 4497, 16945, 6921, 91942, 7843, 4280, 5596, 3500, 1876, 2965, 3559, 12958, 5596, 7843, 17914, 19209, 4482, 5655, 3328, 18430, 65819, 2032, 14233, 4241, 3318, 5596, 5596, 2793, 31149, 7494, 5929, 11827, 13854, 60976, 7173, 55159, 37643, 1461, 5028, 65517, 8596, 5071, 3211, 2959, 3296, 10061, 3598, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 14322, 23772, 2991, 5929, 9655, 29257, 3419, 5929, 3329, 12035, 13229, 11987, 22614, 5929, 6492, 5929, 6362, 3310, 6506, 3419, 2793, 17355, 2015, 3419, 3419, 11999, 5929, 3419, 15024, 3795, 3972, 6815, 11999, 7440, 4766, 5929, 8398, 17031, 3911, 2867, 3419, 5224, 15407, 29257, 3419, 3329, 12035, 3810, 3052, 5929, 4484, 5929, 3419, 5819, 3419, 3329, 17012, 3229, 9258, 45147, 3346, 3919, 7494, 5929, 6166, 6545, 20553, 2881, 3795, 3419, 3795, 4900, 9963, 7494, 5929, 14369, 11107, 3419, 4113, 48212, 6163, 3419, 5929, 3329, 12035, 6506, 14322, 23772, 5086, 3218, 1442, 11883, 6198, 4935, 4461, 1442, 3419, 6429, 4183, 1035, 64606, 2048, 53076, 3419, 6506, 4613, 11373, 2922, 9678, 5929, 3419, 5929, 3329, 12035, 3329, 12035, 2867, 10871, 5161, 10260, 16253, 3768, 5526, 3329, 4373, 14322, 23772, 3052, 3777, 7525, 3329, 4373, 3329, 4373, 14322, 23772, 21250, 5175, 5966, 10622, 5161, 11710, 5929, 3768, 5526, 3329, 17012, 7924, 5001, 74701, 11987, 22614, 14322, 23772, 10968, 3475, 45147, 86365, 14322, 23772, 3329, 12035, 9258, 3795, 3419, 21457, 3841, 9811, 5929, 11324, 11999, 8223, 30078, 8505, 6492, 14322, 23772, 32580, 18481, 23000, 3114, 3318, 3795, 3419, 4394, 10595, 7273, 8505, 3280, 4394, 22056, 10822, 23122, 28938, 61319, 91584, 66580, 9977, 11999, 7551, 11757, 5082, 7179, 3795, 3057, 10153, 2938, 13713, 14322, 23772, 14829, 12958, 3057, 3494, 3795, 62755, 14322, 23772, 3057, 48650, 7899, 33911, 2040, 3057, 5413, 4049, 17358, 3795, 48212, 62755, 46227, 2043, 13874, 8223, 12369, 17358, 4875, 5929, 7333, 5929, 9367, 4875, 53615, 28933, 2846, 4988, 20922, 8223, 13874, 3280, 9678, 5929, 5524, 6132, 3504, 4394, 13874, 2910, 5929, 3114, 3331, 13543, 52046, 22631, 30361, 59894, 1, 1044, 80971, 30784, 20845, 26018, 79061, 12907, 1, 1044, 80971, 68034, 21857, 23376, 54912, 93648, 54934, 1, 1044, 80971, 68034, 21857, 23376, 28654, 48774, 95380, 2025, 1, 1044, 80971, 68034, 21857, 23376, 4384, 69010, 3795, 11408, 6506, 3419, 3329, 12035, 14322, 23772, 4411, 3088, 6865, 3088, 8751, 3419, 3494, 24664, 4394, 5574, 7043, 5929, 3052, 5929, 3361, 3928, 3552, 14322, 23772, 4219, 6378, 3419, 3329, 12035, 30687, 3201, 3463, 6012, 3280, 7578, 6506, 14322, 23772, 4868, 6575, 4997, 7560, 14322, 23772, 7171, 5392, 43030, 57345, 18172, 21849, 3106, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3434, 5929, 6386, 5929, 4431, 9450, 7563, 3426, 3665, 5929, 5042, 4908, 44821, 5929, 6386, 5929, 17539, 3777, 5757, 6177, 3469, 13200, 2905, 5929, 6386, 2973, 3191, 3148, 5929, 3535, 13200, 2905, 7563, 3052, 8523, 4790, 3535, 13200, 2905, 6386, 7563, 3384, 3535, 13200, 2905, 6804, 6917, 5676, 5071, 4394, 53839, 22999, 4790, 3640, 13200, 2905, 6804, 7563, 8369, 14280, 3287, 3929, 21276, 7026, 2867, 7563, 19645, 4863, 6545, 3528, 9916, 2793, 3201, 6386, 3434, 17539, 14020, 5016, 3434, 5929, 6386, 5929, 17539, 6804, 4855, 36019, 4816, 5442, 7563, 7333, 2904, 3073, 6500, 6386, 3434, 17539, 25599, 3535, 13200, 2905, 45585, 5101, 4157, 13200, 2905, 6949, 6386, 5929, 4179, 32717, 4909, 70122, 3535, 13200, 2905, 6804, 7563, 11568, 7563, 7532, 4108, 3191, 13200, 2905, 5929, 6804, 6386, 17539, 8369, 3287, 8369, 13200, 2905, 5929, 4727, 3318, 3251, 8369, 3535, 13200, 2905, 6804, 5369, 5690, 3328, 44821, 6386, 3569, 4080, 5042, 7563, 3426, 6429, 13200, 43184, 2003, 6269, 4418, 13200, 2905, 5929, 5850, 3535, 4080, 10217, 3625, 5351, 8122, 3535, 13200, 2905, 5929, 30197, 19338, 6386, 5929, 10151, 13200, 2905, 5929, 19338, 3535, 3821, 3168, 3680, 14979, 3287, 9861, 3390, 13200, 2905, 5929, 4997, 5929, 6386, 5589, 6810, 5972, 9871, 13200, 2905, 5929, 6386, 32717, 3535, 13200, 2905, 7563, 6804, 6386, 5929, 5850, 5670, 3444, 6386, 17539, 4605, 3885, 3541, 5929, 6380, 3088, 3191, 13200, 2905, 5929, 6386, 17539, 3384, 9871, 13200, 2905, 6386, 5929, 3885, 6386, 4912, 3426, 13101, 4912, 4816, 7563, 6804, 3191, 13200, 2905, 11416, 3535, 13200, 2905, 8755, 7563, 20412, 3955, 6386, 5929, 6106, 6386, 5929, 6106, 5351, 3504, 18850, 2003, 4241, 6839, 7443, 4219, 4183, 6386, 17539, 3569, 4968, 4891, 3426, 13200, 43184, 2003, 5850, 4181, 17019, 36132, 5781, 10538, 6386, 4400, 2871, 3145, 5458, 5670, 4001, 3535, 13200, 2905, 6804, 3541, 5929, 5178, 18275, 5700, 6804, 6386, 2871, 3145, 44821, 4296, 4299, 6771, 6839, 13200, 2905, 5929, 6804, 4296, 16428, 3229, 15352, 5850, 4997, 5929, 6386, 3384, 3271, 96094, 6386, 12126, 5739, 3905, 2793, 31149, 6839, 13200, 2905, 5929, 6386, 3191, 4296, 6386, 3535, 13200, 2905, 7563, 3541, 5929, 5042, 4108, 3535, 13200, 2905, 6804, 3541, 5929, 3191, 13200, 2905, 6386, 5929, 17539, 6386, 3569, 4080, 7563, 3426, 5850, 13200, 43184, 2003, 6804, 6386, 5929, 17539, 9871, 4296, 6386, 4062, 3088, 16477, 3535, 13200, 2905, 6804, 4001, 3541, 5929, 3364, 4, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"uF3FFsPzc6zD"},"source":["sentence_maxlen=128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4m2Qc2IkrnEp"},"source":["##Tokenize training and validation sentences:\r\n","train_encodings = tokenizer.batch_encode_plus(X_train,\r\n","    max_length = sentence_maxlen,\r\n","    truncation=True,\r\n","    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\r\n","    return_token_type_ids=True,\r\n","    return_attention_mask=True,\r\n","    padding='max_length',\r\n","    return_tensors='pt',  # Return PyTorch tensors\r\n","    )\r\n","\r\n","val_encodings = tokenizer.batch_encode_plus(X_val,\r\n","    max_length = sentence_maxlen,\r\n","    truncation=True,\r\n","    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\r\n","    return_token_type_ids=True,\r\n","    return_attention_mask=True,\r\n","    padding='max_length',\r\n","    return_tensors='pt',  # Return PyTorch tensors\r\n","    )\r\n","\r\n","test_encodings=tokenizer.batch_encode_plus(X_test,\r\n","    max_length = sentence_maxlen,\r\n","    truncation=True,\r\n","    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\r\n","    return_token_type_ids=True,\r\n","    return_attention_mask=True,\r\n","    padding='max_length',\r\n","    return_tensors='pt',  # Return PyTorch tensors\r\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwjkXARbetX-","executionInfo":{"status":"ok","timestamp":1610510986454,"user_tz":-210,"elapsed":17675,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"7a0d4997-5314-453f-c7f0-48412a1b987b"},"source":["train_encodings[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"-iCp2PUEupYK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610981601470,"user_tz":-210,"elapsed":82143,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"6db799b6-d884-42c2-d299-2c050a5f091b"},"source":["import torch\r\n","import torch.nn as nn\r\n","\r\n","# for train set\r\n","train_seq = torch.tensor(train_encodings['input_ids'])\r\n","train_mask = torch.tensor(train_encodings['attention_mask'])\r\n","train_y = torch.FloatTensor(y_train)\r\n","\r\n","# for validation set\r\n","val_seq = torch.tensor(val_encodings['input_ids'])\r\n","val_mask = torch.tensor(val_encodings['attention_mask'])\r\n","val_y = torch.FloatTensor(y_val)\r\n","\r\n","# for test set\r\n","test_seq = torch.tensor(test_encodings['input_ids'])\r\n","test_mask = torch.tensor(test_encodings['attention_mask'])\r\n","test_y = torch.FloatTensor(y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  app.launch_new_instance()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0JkQbxVBmbM","executionInfo":{"status":"ok","timestamp":1610784357507,"user_tz":-210,"elapsed":1159,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"1fb47ba8-cf8a-49ef-cdee-33e992bae32e"},"source":["train_y[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":160}]},{"cell_type":"code","metadata":{"id":"T2xiV6Nb0ddZ"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","#define a batch size\r\n","batch_size = 32\r\n","\r\n","# wrap tensors\r\n","train_data = TensorDataset(train_seq, train_mask, train_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","train_sampler = RandomSampler(train_data)\r\n","\r\n","# dataLoader for train set\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","val_data = TensorDataset(val_seq, val_mask, val_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","val_sampler = SequentialSampler(val_data)\r\n","\r\n","# dataLoader for validation set\r\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","test_data = TensorDataset(test_seq, test_mask, test_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","test_sampler = SequentialSampler(test_data)\r\n","\r\n","# dataLoader for validation set\r\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwGHXIjGfmaN","executionInfo":{"status":"ok","timestamp":1610803387527,"user_tz":-210,"elapsed":4672,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"68cfd7ab-99fd-4bb8-f0ad-896ae6e24e52"},"source":["# example\r\n","\r\n","\r\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n","\r\n","# encode text\r\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\r\n","print(sent_id)\r\n","\r\n","seq = torch.tensor(sent_id['input_ids'])\r\n","mask = torch.tensor(sent_id['attention_mask'])\r\n","train_y = torch.tensor([0,1])\r\n","\r\n","transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")\r\n","cls_hs=transformer_model(seq,mask)\r\n","print(cls_hs)\r\n","print(cls_hs[0])\r\n","print(cls_hs[1])\r\n","print(cls_hs[1].shape)\r\n","print(cls_hs[0].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': [[2, 32071, 9574, 1026, 89390, 36260, 84378, 40908, 2041, 4, 0], [2, 13632, 25909, 70608, 1011, 40716, 2033, 1026, 89390, 36260, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>)\n","tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>)\n","torch.Size([2, 768])\n","torch.Size([2, 11, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByUEn_v4zknn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"n3AjEaHcEMfb","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["b90ff975490f4463afa5f77901527458","8ad056afd79743d48569c2bc3f3a35f7","9e39154ae0f54882b102d2b40ba4625e","0817590e6b48497b9b67b7313b24d3d5","6a00ae9c9e6343838bd121a0d7128ce5","41190197f76c4fc2bd0fabc981917c91","7dd3b3bc78754319a1c9c640100ac194","2cea4a486dd94d5c95907cbbf56baea2"]},"executionInfo":{"status":"ok","timestamp":1610981620229,"user_tz":-210,"elapsed":95537,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"dfc4ebf3-249a-40a9-e15a-e2af215b8350"},"source":["transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b90ff975490f4463afa5f77901527458","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=651477729.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RaAlYydhxPTd"},"source":["# freeze all the parameters\r\n","for param in transformer_model.parameters():\r\n","    param.requires_grad = False\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUa1R1WQONe6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610981631990,"user_tz":-210,"elapsed":891,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"2c5f806f-de9c-46ea-beb8-f190c6adc663"},"source":["classnum=len(labels)\r\n","classnum"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["80"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"oyE_ThEms5aZ"},"source":["class BERT_Arch(nn.Module):\r\n","\r\n","    def __init__(self, bert):\r\n","      \r\n","      super(BERT_Arch, self).__init__()\r\n","\r\n","      self.bert = bert \r\n","      \r\n","      # dropout layer\r\n","      self.dropout = nn.Dropout(0.1)\r\n","      \r\n","      # relu activation function\r\n","      self.relu =  nn.ReLU()\r\n","\r\n","      # dense layer 1\r\n","      self.fc1 = nn.Linear(768,512)\r\n","\r\n","      # dense layer 2\r\n","      self.fc2 = nn.Linear(512,256)\r\n","\r\n","      # dense layer 3\r\n","      self.fc3 = nn.Linear(256,128)\r\n","      \r\n","      \r\n","      # dense layer 4 (Output layer)\r\n","      self.fc4 = nn.Linear(128,classnum)\r\n","\r\n","      #sigmoid activation function\r\n","      self.sigmoid = nn.Sigmoid()\r\n","\r\n","    #define the forward pass\r\n","    def forward(self, sent_id, mask):\r\n","\r\n","      #pass the inputs to the model  \r\n","      cls_hs = self.bert(sent_id, attention_mask=mask)\r\n","      \r\n","      x = self.fc1(cls_hs[1])\r\n","      \r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      x = self.fc2(x)\r\n","      \r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      x = self.fc3(x)\r\n","      \r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      # output layer\r\n","      x = self.fc4(x)\r\n","      \r\n","      # apply sigmoid activation\r\n","      x = self.sigmoid(x)\r\n","\r\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDuHzo96z6z8"},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(transformer_model)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUNSLBYcLc9q"},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8kG515nRHVs","executionInfo":{"status":"ok","timestamp":1610981723138,"user_tz":-210,"elapsed":2340,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"04301970-9d99-418b-a442-a9bf6b325102"},"source":["##Calculating weights \r\n","class_counts=[0]*classnum\r\n","for cl in Y:\r\n","    #  print(cl)\r\n","     for i in range(classnum):\r\n","      #  print(i)\r\n","       if cl[i]==1:\r\n","          class_counts[i]+=1\r\n","print(class_counts)\r\n","\r\n","weight=[]\r\n","for indx in range(classnum):\r\n","  weight.append(((class_counts[indx]/len(Y))))\r\n","\r\n","print(weight)\r\n","\r\n","# class_weight= torch.tensor(weight,dtype=torch.float)\r\n","class_weight = torch.FloatTensor(weight)\r\n","class_weight = class_weight.to(device)\r\n","print(class_weight)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[580, 3730, 91, 1283, 88, 68, 80, 1408, 261, 222, 584, 1008, 34, 57, 84, 81, 1263, 518, 2839, 630, 311, 770, 721, 200, 735, 370, 1020, 949, 808, 1481, 783, 282, 481, 551, 1098, 509, 919, 1136, 69, 222, 160, 1176, 575, 405, 1040, 1167, 119, 174, 1339, 1481, 288, 155, 121, 286, 466, 763, 211, 282, 201, 3061, 939, 970, 307, 2391, 825, 690, 417, 1126, 144, 602, 3323, 2416, 559, 3518, 605, 200, 147, 959, 311, 615]\n","[0.01626153026607228, 0.1045784618835338, 0.002551378024504444, 0.03597162643339782, 0.0024672666610592426, 0.0019065242380912329, 0.0022429696918720384, 0.03947626657694788, 0.007317688619732526, 0.006224240894944907, 0.016373678750665883, 0.028261418117587686, 0.0009532621190456164, 0.0015981159054588275, 0.0023551181764656405, 0.002271006813020439, 0.035410884010429806, 0.01452322875487145, 0.07959738694030898, 0.017663386323492306, 0.00871954467715255, 0.02158858328426837, 0.02021476434799675, 0.005607424229680097, 0.020607284044074355, 0.010373734824908179, 0.028597863571368492, 0.02660722796983206, 0.02265399388790759, 0.041522976420781114, 0.02195306585919758, 0.007906468163848936, 0.013485855272380632, 0.015448453752768666, 0.03078475902094373, 0.014270894664535846, 0.025766114335380043, 0.031850169624582945, 0.0019345613592396333, 0.006224240894944907, 0.004485939383744077, 0.03297165447051897, 0.016121344660330276, 0.011355034065102196, 0.029158605994336503, 0.03271932038018336, 0.0033364174166596572, 0.004878459079821684, 0.03754170521770825, 0.041522976420781114, 0.008074690890739339, 0.004345753778002075, 0.0033924916589564585, 0.008018616648442538, 0.013065298455154625, 0.021392323436229568, 0.0059158325623125015, 0.007906468163848936, 0.005635461350828497, 0.08582162783525388, 0.026326856758348054, 0.02719600751394847, 0.008607396192558948, 0.06703675666582555, 0.023130624947430397, 0.019345613592396334, 0.011691479518883002, 0.03156979841309894, 0.0040373454453696694, 0.01687834693133709, 0.0931673535761348, 0.06773768469453556, 0.01567275072195587, 0.0986345922000729, 0.016962458294782292, 0.005607424229680097, 0.004121456808814871, 0.02688759918131606, 0.00871954467715255, 0.017242829506266297]\n","tensor([0.0163, 0.1046, 0.0026, 0.0360, 0.0025, 0.0019, 0.0022, 0.0395, 0.0073,\n","        0.0062, 0.0164, 0.0283, 0.0010, 0.0016, 0.0024, 0.0023, 0.0354, 0.0145,\n","        0.0796, 0.0177, 0.0087, 0.0216, 0.0202, 0.0056, 0.0206, 0.0104, 0.0286,\n","        0.0266, 0.0227, 0.0415, 0.0220, 0.0079, 0.0135, 0.0154, 0.0308, 0.0143,\n","        0.0258, 0.0319, 0.0019, 0.0062, 0.0045, 0.0330, 0.0161, 0.0114, 0.0292,\n","        0.0327, 0.0033, 0.0049, 0.0375, 0.0415, 0.0081, 0.0043, 0.0034, 0.0080,\n","        0.0131, 0.0214, 0.0059, 0.0079, 0.0056, 0.0858, 0.0263, 0.0272, 0.0086,\n","        0.0670, 0.0231, 0.0193, 0.0117, 0.0316, 0.0040, 0.0169, 0.0932, 0.0677,\n","        0.0157, 0.0986, 0.0170, 0.0056, 0.0041, 0.0269, 0.0087, 0.0172],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ArHmwhh7JrZh"},"source":["# loss_func =nn.MultiLabelMarginLoss()\r\n","# loss_func=nn.BCEWithLogitsLoss(weight=class_weight)\r\n","loss_func=nn.BCELoss(weight=class_weight)\r\n","# loss_func=nn.NLLLoss(weight=class_weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8LjQyDXs0bG"},"source":["# function to train the model\r\n","def train():\r\n","  \r\n","  model.train()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save model predictions\r\n","  total_preds=[]\r\n","  \r\n","  # iterate over batches\r\n","  for step,batch in enumerate(train_dataloader):\r\n","    \r\n","    # progress update after every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [r.to(device) for r in batch]\r\n"," \r\n","    sent_id, mask, labels = batch\r\n","   \r\n","    # clear previously calculated gradients \r\n","    model.zero_grad()        \r\n","\r\n","    # get model predictions for the current batch\r\n","    preds = model(sent_id, mask)\r\n","    \r\n","    # compute the loss between actual and predicted values\r\n","    loss=loss_func(preds,labels)\r\n","    \r\n","    # loss=0\r\n","      \r\n","    # for b in range(batch_size):\r\n","    #   for l in range(classnum):\r\n","    #     print(preds[b][l], labels[b][l])\r\n","    #     loss += loss_func(preds[b][l], labels[b][l])\r\n","\r\n","    # add on to the total loss\r\n","    total_loss = total_loss + loss.item()\r\n","\r\n","    # backward pass to calculate the gradients\r\n","    loss.backward()\r\n","\r\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","    # update parameters\r\n","    optimizer.step()\r\n","\r\n","    # model predictions are stored on GPU. So, push it to CPU\r\n","    preds=preds.detach().cpu().numpy()\r\n","\r\n","    # append the model predictions\r\n","    total_preds.append(preds)\r\n","\r\n","  # compute the training loss of the epoch\r\n","  avg_loss = total_loss / len(train_dataloader)\r\n","  \r\n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  #returns the loss and predictions\r\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNBRQo9WMHey"},"source":["# function for evaluating the model\r\n","def evaluate():\r\n","  \r\n","  print(\"\\nEvaluating...\")\r\n","  \r\n","  # deactivate dropout layers\r\n","  model.eval()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save the model predictions\r\n","  total_preds = []\r\n","\r\n","  # iterate over batches\r\n","  for step,batch in enumerate(val_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","    \r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","      \r\n","      # compute the validation loss between actual and predicted values\r\n","\r\n","      # loss=0\r\n","      # for b in range(batch_size):\r\n","      #   for l in range(classnum):\r\n","      #     loss += loss_func(preds[b][l], labels[b][l])\r\n","      \r\n","      loss=loss_func(preds,labels)\r\n","      total_loss = total_loss + loss.item()\r\n","\r\n","      preds = preds.detach().cpu().numpy()\r\n","\r\n","      total_preds.append(preds)\r\n","\r\n","  # compute the validation loss of the epoch\r\n","  avg_loss = total_loss / len(val_dataloader) \r\n","\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qu5pfrJKtTc0","executionInfo":{"status":"ok","timestamp":1610984282692,"user_tz":-210,"elapsed":2279737,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"dcb7ebe6-c2c8-4763-ab7f-f4a3e806bb84"},"source":["# number of training epochs\r\n","epochs = 10\r\n","\r\n","# set initial loss to infinite\r\n","best_valid_loss = float('inf')\r\n","\r\n","# empty lists to store training and validation loss of each epoch\r\n","train_losses=[]\r\n","valid_losses=[]\r\n","\r\n","#for each epoch\r\n","for epoch in range(epochs):\r\n","     \r\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n","    \r\n","    #train model\r\n","    train_loss, _ = train()\r\n","    \r\n","    #evaluate model\r\n","    valid_loss, _ = evaluate()\r\n","    \r\n","    #save the best model\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'saved_weights_4fc.pt')\r\n","    \r\n","    # append training and validation loss\r\n","    train_losses.append(train_loss)\r\n","    valid_losses.append(valid_loss)\r\n","    \r\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 2 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 3 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 4 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 5 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 6 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 7 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 8 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 9 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n","\n"," Epoch 10 / 10\n","  Batch    50  of    669.\n","  Batch   100  of    669.\n","  Batch   150  of    669.\n","  Batch   200  of    669.\n","  Batch   250  of    669.\n","  Batch   300  of    669.\n","  Batch   350  of    669.\n","  Batch   400  of    669.\n","  Batch   450  of    669.\n","  Batch   500  of    669.\n","  Batch   550  of    669.\n","  Batch   600  of    669.\n","  Batch   650  of    669.\n","\n","Evaluating...\n","  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","\n","Training Loss: 0.015\n","Validation Loss: 0.015\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cQ2_aS0zCLvp"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"cvR-FhPpuLkR"},"source":["# torch.cuda.empty_cache()\r\n","# pass the pre-trained BERT to our define architecture\r\n","model = BERT_Arch(transformer_model)\r\n","\r\n","# push the model to GPU\r\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aOPRZ2jVvNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610509380028,"user_tz":-210,"elapsed":4470,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"8b608ad9-767d-4856-fcbe-443cc3ea4356"},"source":["#load weights of best model\r\n","path = 'saved_weights_4fc.pt'\r\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"PM1uUcZFCPVg"},"source":["After loading model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZhHObMnzuws","executionInfo":{"status":"ok","timestamp":1610984391824,"user_tz":-210,"elapsed":56737,"user":{"displayName":"mohadese rahnama","photoUrl":"","userId":"03886626379062840142"}},"outputId":"32ccab59-a680-440a-cf89-0c9550022430"},"source":["y_pred=[]\r\n","y_true=[]\r\n","for step,batch in enumerate(test_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      # outputs_before_sigmoid = model(sent_id, mask)\r\n","      preds = model(sent_id, mask)\r\n","      # print(preds)\r\n","      # print(preds.cpu().numpy())\r\n","      \r\n","      \r\n","      # outputs_before_sigmoid = outputs_before_sigmoid.cpu().numpy()\r\n","      # model's performance\r\n","    # preds = numpy.argmax(preds, axis = 1)\r\n","    # print(len(preds),len(preds[0]))\r\n","    # measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    # print(measure)\r\n","\r\n","\r\n","    # preds=[]\r\n","    # for l in range(len(outputs_before_sigmoid)):\r\n","    #   preds.append(torch.sigmoid(outputs_before_sigmoid[l]))\r\n","    # preds = preds.cpu().numpy()\r\n","    # print(preds)\r\n","    # measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    # for l in range(len(outputs_before_sigmoid)):\r\n","    #   temp=[]\r\n","    #   for value in preds:\r\n","    #     if value >= measure:\r\n","    #       temp.append(1)\r\n","          \r\n","    #     else:\r\n","    #       temp.append(0)\r\n","    #   y_pred.append(temp)\r\n","    #   y_true.append(labels.cpu().numpy()[l])\r\n","    #   # print( preds[l])\r\n","    #   print(\"temp:\",(temp))\r\n","    #   print(\"labels:\",labels.cpu().numpy()[l])\r\n","    \r\n","    # print(labels.cpu().numpy()[0], preds[0])\r\n","  \r\n","\r\n","    # for l in range(len(preds)):\r\n","    #   preds.append(torch.sigmoid(outputs_before_sigmoid[l]))\r\n","    # preds = preds.cpu().numpy()\r\n","    # print(preds)\r\n","    preds = preds.cpu().numpy()\r\n","    measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    for l in range(len(preds)):\r\n","      temp=[]\r\n","      for value in preds[l]:\r\n","        if value >= measure:\r\n","          temp.append(1)\r\n","          \r\n","        else:\r\n","          temp.append(0)\r\n","      y_pred.append(temp)\r\n","      y_true.append(labels.cpu().numpy()[l])\r\n","      # print( preds[l])\r\n","      # print(\"temp:\",(temp))\r\n","      # print(\"labels:\",labels.cpu().numpy()[l])\r\n","\r\n","print(classification_report(y_true, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Batch    50  of    223.\n","  Batch   100  of    223.\n","  Batch   150  of    223.\n","  Batch   200  of    223.\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       109\n","           1       0.00      0.00      0.00       740\n","           2       0.00      0.00      0.00        14\n","           3       0.03      0.89      0.06       239\n","           4       0.00      0.00      0.00        19\n","           5       0.00      0.15      0.00        13\n","           6       0.00      0.00      0.00        17\n","           7       0.00      0.00      0.00       293\n","           8       0.00      0.00      0.00        57\n","           9       0.00      0.00      0.00        41\n","          10       0.00      0.00      0.00       110\n","          11       0.00      0.00      0.00       190\n","          12       0.00      0.00      0.00         9\n","          13       0.00      1.00      0.00        14\n","          14       0.00      0.00      0.00        13\n","          15       0.00      0.00      0.00        15\n","          16       0.00      0.00      0.00       245\n","          17       0.00      0.00      0.00       119\n","          18       0.09      0.00      0.00       563\n","          19       0.00      0.00      0.00       114\n","          20       0.00      0.04      0.01        74\n","          21       0.00      0.00      0.00       151\n","          22       0.00      0.00      0.00       130\n","          23       0.00      0.00      0.00        31\n","          24       0.02      1.00      0.04       154\n","          25       0.00      0.00      0.00        85\n","          26       0.00      0.00      0.00       233\n","          27       0.00      0.00      0.00       191\n","          28       0.00      0.00      0.00       152\n","          29       0.00      0.00      0.00       291\n","          30       0.00      0.00      0.00       164\n","          31       0.01      1.00      0.02        55\n","          32       0.01      0.28      0.01        86\n","          33       0.00      0.00      0.00       113\n","          34       0.00      0.00      0.00       223\n","          35       0.00      0.00      0.00       105\n","          36       0.00      0.00      0.00       165\n","          37       0.00      0.00      0.00       228\n","          38       0.00      0.00      0.00        17\n","          39       0.01      1.00      0.01        44\n","          40       0.00      0.00      0.00        27\n","          41       0.00      0.00      0.00       269\n","          42       0.00      0.00      0.00       123\n","          43       0.00      0.00      0.00        82\n","          44       0.00      0.00      0.00       221\n","          45       0.03      0.92      0.07       242\n","          46       0.00      0.00      0.00        23\n","          47       0.00      0.00      0.00        29\n","          48       0.04      1.00      0.07       275\n","          49       0.00      0.00      0.00       283\n","          50       0.01      0.89      0.02        61\n","          51       0.00      0.00      0.00        31\n","          52       0.00      0.95      0.01        22\n","          53       0.00      0.00      0.00        53\n","          54       0.00      0.00      0.00        80\n","          55       0.00      0.00      0.00       161\n","          56       0.00      0.00      0.00        44\n","          57       0.00      0.00      0.00        53\n","          58       0.00      0.00      0.00        37\n","          59       0.00      0.00      0.00       612\n","          60       0.00      0.00      0.00       209\n","          61       0.00      0.00      0.00       193\n","          62       0.00      0.00      0.00        64\n","          63       0.00      0.00      0.00       489\n","          64       0.00      0.00      0.00       154\n","          65       0.00      0.00      0.00       113\n","          66       0.00      0.00      0.00        95\n","          67       0.06      0.07      0.07       244\n","          68       0.00      0.32      0.01        28\n","          69       0.00      0.00      0.00       124\n","          70       0.00      0.00      0.00       671\n","          71       0.07      0.98      0.13       493\n","          72       0.00      0.00      0.00       106\n","          73       0.11      0.46      0.17       706\n","          74       0.00      0.00      0.00       131\n","          75       0.00      0.00      0.00        37\n","          76       0.00      0.00      0.00        23\n","          77       0.00      0.00      0.00       201\n","          78       0.01      1.00      0.02        64\n","          79       0.02      0.27      0.03       128\n","\n","   micro avg       0.02      0.16      0.04     12627\n","   macro avg       0.01      0.15      0.01     12627\n","weighted avg       0.02      0.16      0.02     12627\n"," samples avg       0.02      0.15      0.03     12627\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]}]}