{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"bert-base-parsbert-uncased.ipynb","provenance":[{"file_id":"1k6QLcPAJKT5H2yTy61U-YCB267inolmb","timestamp":1610512876213},{"file_id":"1AIm-KimERCJutlpyjiZ2IAwM-cCrVsWM","timestamp":1610440221899},{"file_id":"17mqUcShahUjZjQxywgKQSGU1jV-CZW8o","timestamp":1610336820188},{"file_id":"1FgtzYXY0CXNyE_2FU4IEqJTQzmVZNvDh","timestamp":1610105938882}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"884d65a8b21148238033ea2b5980da82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4199a00743c34c68b1cd4643ef6e9b51","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fadb3599aed649f48cc2eaec5f264568","IPY_MODEL_73a204cfc7f5467eb17f477efdbd4e95"]}},"4199a00743c34c68b1cd4643ef6e9b51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fadb3599aed649f48cc2eaec5f264568":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d5bb7c07815a4002867c47db76dd4e09","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1215509,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1215509,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_200ff086a065426a88f138442c4f2795"}},"73a204cfc7f5467eb17f477efdbd4e95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73e68c1a50ea4a5fb68f010dee5a92f2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.22M/1.22M [00:01&lt;00:00, 939kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b8272b4ef364929a5880b9ef0aa7eb6"}},"d5bb7c07815a4002867c47db76dd4e09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"200ff086a065426a88f138442c4f2795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73e68c1a50ea4a5fb68f010dee5a92f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b8272b4ef364929a5880b9ef0aa7eb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"j1LTPn7IjqTz"},"source":["Source:\r\n","\r\n","huggingface: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","\r\n","Tutorial:https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmPFvCbSqyZF","executionInfo":{"status":"ok","timestamp":1610559990130,"user_tz":-210,"elapsed":44603,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"d618cfd9-698f-4334-ab60-52337ddc4da1"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g7Y1pG9mT2qWSR6C1fibVLAn85I5hEjCwVx4fTm4dj-Clr9bX74G9Y\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRxC0Pz1qzKc","executionInfo":{"status":"ok","timestamp":1610560030493,"user_tz":-210,"elapsed":914,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/sharif/FineTuning/ipython(guide)')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCRkKc3NcgkX","executionInfo":{"status":"ok","timestamp":1610562067241,"user_tz":-210,"elapsed":3100,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"47c7aad8-6fbf-4f51-b21c-0d2d60f4fec5"},"source":["!pip install transformers==2.8"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==2.8 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (0.0.43)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (1.19.5)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (4.41.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (0.1.95)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8) (1.16.53)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8) (1.0.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8) (3.0.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8) (0.3.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8) (0.10.0)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.53 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8) (1.19.53)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.53->boto3->transformers==2.8) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODc44DglgNjZ","executionInfo":{"status":"ok","timestamp":1610560046881,"user_tz":-210,"elapsed":7205,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"0e182804-eafd-4e53-f2de-fffdb27bf3fd"},"source":["!pip3 install sentencepiece\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 14.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 7.5MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJY_L2p9a0t0","executionInfo":{"status":"ok","timestamp":1610560047498,"user_tz":-210,"elapsed":6680,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"e428b446-f56c-4f9b-c967-aee61215eb05"},"source":["!git clone https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","GIT_LFS_SKIP_SMUDGE=1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["fatal: destination path 'bert-fa-base-uncased-clf-persiannews' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eg3Up037nThu","executionInfo":{"status":"ok","timestamp":1610560053588,"user_tz":-210,"elapsed":11791,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import numpy\r\n","import pandas\r\n","import re\r\n","from sklearn.preprocessing import MultiLabelBinarizer\r\n","from sklearn.model_selection import train_test_split\r\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,TFAutoModel,AutoModel\r\n","from transformers import BertConfig, BertTokenizer\r\n","from transformers import TFBertModel, TFBertForSequenceClassification\r\n","from transformers import glue_convert_examples_to_features, InputExample\r\n","from sklearn.metrics import classification_report"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ur9wv1ytrZu","executionInfo":{"status":"ok","timestamp":1610560053589,"user_tz":-210,"elapsed":10455,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# specify GPU\r\n","device = torch.device(\"cuda\")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xax4bHubzpMp"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"TJf6T40glV5g","executionInfo":{"status":"ok","timestamp":1610560055382,"user_tz":-210,"elapsed":8226,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["limit_number = 750\r\n","data = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv',index_col=0)\r\n","data = data.dropna().reset_index(drop=True)\r\n","X = data[\"body\"].values.tolist()\r\n","y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","labels = []\r\n","tag=[]\r\n","for item in y['tag']:\r\n","  labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","  tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","labels = list(set(labels))\r\n","mlb = MultiLabelBinarizer()\r\n","Y=mlb.fit_transform(tag)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH3jCKaZsEWo","executionInfo":{"status":"ok","timestamp":1610560055383,"user_tz":-210,"elapsed":7784,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"d5e2eb99-4abf-42e3-b5ec-12f256065448"},"source":["X_train, X_test, y_train, y_test = train_test_split(X,Y , test_size=0.2)\r\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n","print('train: ', len(X_train) , '\\ntest: ', len(X_test) , '\\nval: ', len(X_val) ,\"\\ny_tain:\",len(y_train) )"],"execution_count":10,"outputs":[{"output_type":"stream","text":["train:  12896 \n","test:  4299 \n","val:  4299 \n","y_tain: 12896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vei6iu9atmyd","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["884d65a8b21148238033ea2b5980da82","4199a00743c34c68b1cd4643ef6e9b51","fadb3599aed649f48cc2eaec5f264568","73a204cfc7f5467eb17f477efdbd4e95","d5bb7c07815a4002867c47db76dd4e09","200ff086a065426a88f138442c4f2795","73e68c1a50ea4a5fb68f010dee5a92f2","7b8272b4ef364929a5880b9ef0aa7eb6"]},"executionInfo":{"status":"ok","timestamp":1610562826267,"user_tz":-210,"elapsed":3219,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"1d4c6022-6e62-4a13-f37a-904257ecf9be"},"source":["##we would load the tokenizer\r\n","# from transformers import BertTokenizer, BertModel\r\n","# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\r\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"],"execution_count":37,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"884d65a8b21148238033ea2b5980da82","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1215509.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7wdU0zejDNq","executionInfo":{"status":"ok","timestamp":1610562826268,"user_tz":-210,"elapsed":2619,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"2fe08ef0-4b4a-4dbd-a8b8-85dc8a951402"},"source":["#example\r\n","text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\r\n","tokenized=tokenizer.tokenize(X_train[0])\r\n","input_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n","print(tokenized)\r\n","print(input_ids)\r\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["['کتابخانه', 'ترفند', 'ها', 'مفیدی', 'ترکیب', 'درخواست', 'ها', 'css', 'جاوااسکریپت', 'فایل', 'اندازه', 'تصویر', 'تلفیق', 'خودکاری', 'sas', '##s', 'فایل', 'ها', 'coffee', '##sc', '##rip', '##t', 'فشرده', 'g', '##zi', '##p', 'عملکرد', 'ها', 'ذکر', 'سرور', 'کاربر', 'بشوند', 'کتابخانه', 'ی', 'mun', '##ee', 'ارايه', 'ی', 'متد', 'ها', 'کلاس', 'ها', 'مفید', '##ش', 'توسعه', 'دهندگان', 'برنامه', 'نویسان', 'توسعه', 'دهندگان', 'php', 'راحتی', 'کار', 'فایل', 'ها', 'css', 'جاوا', 'اسکریپت', 'بپردازند', 'خواست', 'ها', 'ترکیب', 'پروژه', 'ی', 'نمایند', 'امکانات', 'کتابخانه', 'ی', 'mun', '##ee', 'متد', 'ها', 'کتابخانه', 'ی', 'mun', '##ee', 'توسعه', 'دهندگان', 'قادر', 'عکس', 'تصاویر', 'پروژه', 'ی', 'تغییراتی', 'اعمال', 'نمایند', 'تغییرات', 'سایز', 'رنگ', 'وضوح', 'کتابخانه', 'الگو', 'ها', 'امنیتی', 'فایل', 'ها', 'جاوا', 'اسکریپت', 'css', 'قادر', 'فایل', 'ها', 'محافظت', 'ویرایش', 'اصلاح', 'عکس', 'ها', 'حملات', 'مخرب', 'فایل', 'ها', 'جاوا', 'اسکریپت', 'css', 'اتمام', 'کار', 'تصاویر', 'کتابخانه', 'mun', '##ee', 'فایل', 'ها', 'ایجاد', 'مسیر', 'دلخواه', 'ذخیره', 'نمایید', 'نصب', 'فراخوانی', 'کتابخانه', 'comp', '##ose', '##r', 'میتوانید', 'نمایید', 'دریافت', 'فایل', 'ها', 'کتابخانه', 'می', 'توانید', 'ادرس', 'مراجعه', 'نمایید']\n","[7314, 14551, 5526, 15774, 4950, 3799, 5526, 68588, 85208, 6618, 4224, 4175, 9740, 70611, 66769, 1192, 6618, 5526, 72039, 30311, 42642, 1166, 10966, 35, 58730, 1209, 3361, 5526, 4546, 10562, 5663, 15779, 7314, 382, 41345, 13126, 2783, 382, 21073, 5526, 5588, 5526, 5558, 1176, 2569, 10032, 2385, 21214, 2569, 10032, 66903, 5406, 2109, 6618, 5526, 68588, 25103, 44304, 8194, 3181, 5526, 4950, 3600, 382, 5631, 4499, 7314, 382, 41345, 13126, 21073, 5526, 7314, 382, 41345, 13126, 2569, 10032, 5060, 3679, 4696, 3600, 382, 8274, 4167, 5631, 4403, 9598, 3852, 10029, 7314, 7026, 5526, 4105, 6618, 5526, 25103, 44304, 68588, 5060, 6618, 5526, 8817, 10077, 2942, 3679, 5526, 5318, 10178, 6618, 5526, 25103, 44304, 68588, 6193, 2109, 4696, 7314, 41345, 13126, 6618, 5526, 2559, 3270, 11300, 5170, 6853, 4846, 21015, 7314, 20297, 32035, 1194, 3931, 6853, 2750, 6618, 5526, 7314, 2044, 30825, 7092, 4037, 6853]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az4rwU0l5ECn","executionInfo":{"status":"ok","timestamp":1610562826269,"user_tz":-210,"elapsed":2350,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# encode text\r\n","sent_id = tokenizer.batch_encode_plus(X_train[:10], padding=True, return_token_type_ids=False)"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNRU-SH65ZEE","executionInfo":{"status":"ok","timestamp":1610562826269,"user_tz":-210,"elapsed":2147,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"3e151ac1-9718-42d4-cad3-33972ffcc918"},"source":["sent_id"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 7314, 14551, 5526, 15774, 4950, 3799, 5526, 68588, 85208, 6618, 4224, 4175, 9740, 70611, 66769, 1192, 6618, 5526, 72039, 30311, 42642, 1166, 10966, 35, 58730, 1209, 3361, 5526, 4546, 10562, 5663, 15779, 7314, 382, 41345, 13126, 2783, 382, 21073, 5526, 5588, 5526, 5558, 1176, 2569, 10032, 2385, 21214, 2569, 10032, 66903, 5406, 2109, 6618, 5526, 68588, 25103, 44304, 8194, 3181, 5526, 4950, 3600, 382, 5631, 4499, 7314, 382, 41345, 13126, 21073, 5526, 7314, 382, 41345, 13126, 2569, 10032, 5060, 3679, 4696, 3600, 382, 8274, 4167, 5631, 4403, 9598, 3852, 10029, 7314, 7026, 5526, 4105, 6618, 5526, 25103, 44304, 68588, 5060, 6618, 5526, 8817, 10077, 2942, 3679, 5526, 5318, 10178, 6618, 5526, 25103, 44304, 68588, 6193, 2109, 4696, 7314, 41345, 13126, 6618, 5526, 2559, 3270, 11300, 5170, 6853, 4846, 21015, 7314, 20297, 32035, 1194, 3931, 6853, 2750, 6618, 5526, 7314, 2044, 30825, 7092, 4037, 6853, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4899, 3161, 2109, 32510, 18571, 2786, 4813, 3161, 2109, 3538, 7532, 3135, 2801, 4176, 5526, 7633, 2109, 5860, 2827, 2224, 4841, 3135, 4224, 6886, 23010, 2044, 53205, 22846, 4540, 18571, 8322, 6989, 7463, 2534, 5130, 5151, 6893, 2109, 4540, 18571, 7156, 4719, 7973, 5526, 18571, 14482, 2355, 48590, 13646, 18571, 36165, 17278, 4073, 3161, 2109, 4594, 18571, 2902, 2891, 3161, 2109, 3813, 13249, 10526, 6294, 14482, 4813, 6042, 4883, 6598, 2210, 6920, 8912, 5526, 7285, 6886, 2611, 14482, 2855, 3145, 2109, 16435, 6989, 7463, 3760, 18571, 2109, 6598, 3739, 7722, 3135, 16283, 6736, 18571, 3991, 12558, 5526, 10217, 5526, 5105, 6842, 4841, 5289, 2109, 5526, 18571, 12558, 5105, 18571, 2109, 6842, 4841, 56337, 7127, 17904, 5283, 5105, 3971, 6989, 3161, 2109, 4813, 18571, 3197, 4341, 3161, 2109, 8038, 19668, 84261, 4609, 15293, 3201, 5526, 18571, 7885, 6167, 7885, 6167, 2865, 3906, 15293, 4890, 8197, 7885, 6167, 2431, 4106, 13159, 5289, 2712, 5526, 3900, 18571, 3133, 8912, 5105, 56337, 3135, 2431, 6989, 3161, 2109, 5526, 8134, 6638, 4056, 18571, 2580, 6886, 3135, 8689, 18571, 4121, 3587, 2139, 3650, 3135, 71275, 11238, 8038, 7069, 6406, 9419, 12640, 5802, 4199, 22541, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 87810, 2808, 382, 2569, 3619, 2112, 10880, 19122, 9350, 4879, 5526, 3568, 382, 3018, 3031, 3555, 4855, 4149, 21491, 28049, 1166, 4879, 10647, 5465, 2426, 8781, 8590, 9350, 9919, 2232, 3568, 17698, 7922, 6899, 4728, 9792, 13306, 5526, 5080, 2550, 382, 31048, 4149, 21491, 28049, 1166, 3538, 2979, 3930, 14539, 9792, 13306, 5526, 2460, 4853, 24248, 6385, 19708, 31055, 21491, 28049, 1166, 7519, 7420, 3838, 2851, 8252, 4996, 24248, 6385, 20728, 4879, 21491, 28049, 1166, 23203, 6304, 2232, 10880, 19122, 3372, 10880, 19122, 2653, 3568, 382, 3006, 3123, 4149, 4879, 31048, 4149, 10880, 19122, 2109, 21491, 28049, 1166, 37217, 2232, 4879, 4781, 2325, 2919, 4502, 68442, 24248, 6385, 3131, 4879, 21491, 28049, 1166, 10880, 19122, 21491, 28049, 1166, 2510, 2068, 2385, 2156, 10880, 19122, 3838, 5917, 28110, 13053, 2488, 5526, 3892, 2974, 5624, 38007, 2099, 2712, 2044, 30825, 7183, 79620, 2724, 34738, 2210, 5671, 4991, 79620, 8734, 3579, 2325, 10598, 2110, 4841, 79620, 24774, 5624, 38007, 1174, 5526, 10351, 3485, 5624, 38007, 2099, 4356, 3018, 3031, 5526, 2856, 3385, 382, 2920, 2385, 46531, 29006, 1192, 24248, 6385, 3357, 2044, 7954, 24248, 6385, 4660, 2156, 2385, 2044, 39709, 5624, 38007, 1174, 37261, 9691, 48887, 2488, 5526, 35275, 14430, 2109, 2385, 5624, 38007, 2099, 44487, 27930, 2232, 5617, 14847, 2030, 17278, 2109, 8102, 24248, 6385, 3763, 5671, 2524, 3485, 79620, 5526, 8734, 5526, 3579, 2325, 24248, 6385, 46531, 29006, 5526, 3600, 5526, 5616, 19708, 31055, 7592, 2139, 4161, 5526, 48590, 5950, 7803, 4879, 2232, 19708, 31055, 5378, 24248, 6385, 3954, 80424, 69099, 1202, 5378, 69099, 1202, 3954, 50929, 28837, 13881, 46531, 56854, 3779, 2920, 2385, 3813, 4367, 24578, 1161, 2920, 2385, 5406, 8274, 2920, 2385, 2974, 7030, 4360, 4502, 46531, 2044, 30825, 24248, 6385, 3081, 24248, 6385, 3450, 16755, 8037, 2044, 46235, 4103, 4046, 5242, 5526, 7995, 2385, 5151, 10880, 19122, 2833, 4046, 5242, 5526, 29392, 2156, 4103, 2385, 382, 6286, 9405, 4046, 31088, 2385, 382, 4167, 5242, 5526, 5275, 9446, 5950, 19708, 31055, 8104, 21073, 42401, 77142, 8511, 5447, 24248, 6385, 4367, 2569, 382, 2385, 382, 3014, 2232, 84261, 2431, 5305, 3600, 8001, 4012, 5526, 10240, 19708, 31055, 3660, 3657, 5628, 2044, 5565, 7327, 5526, 2563, 4841, 19708, 31055, 4383, 4383, 5526, 3123, 21154, 3881, 2156, 21491, 28049, 1166, 4723, 19708, 31055, 3587, 2139, 2833, 2044, 39709, 3763, 24248, 6385, 24496, 86253, 2569, 10032, 2524, 3485, 79620, 5526, 8734, 5526, 3579, 2325, 24248, 6385, 14923, 4479, 2919, 4502, 2385, 5526, 2919, 24248, 6385, 21491, 28049, 1166, 8734, 382, 21491, 28049, 1166, 4879, 5526, 9749, 4879, 5526, 7891, 42165, 29853, 4383, 10598, 3600, 382, 5616, 11461, 5526, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 57006, 3228, 3217, 21073, 5526, 22661, 14645, 18897, 1168, 6688, 71941, 3372, 2110, 2495, 3555, 26962, 5411, 3217, 3954, 2109, 31896, 5526, 9309, 18897, 1168, 8663, 26962, 39470, 55431, 57006, 87587, 3560, 5526, 2385, 9978, 4891, 11018, 9309, 71941, 86727, 6034, 52449, 72829, 30061, 19963, 66326, 36981, 2061, 2385, 5526, 27441, 67867, 9672, 2564, 2569, 2488, 3905, 1160, 9309, 2232, 6942, 4854, 9672, 4548, 62632, 78625, 7648, 74161, 33079, 14664, 4347, 9309, 2361, 5617, 18897, 1168, 14150, 9309, 2426, 3930, 4403, 30812, 28558, 1202, 31054, 18655, 8137, 11290, 11901, 2286, 5526, 30812, 28558, 1202, 5617, 3560, 5526, 2385, 9978, 2110, 2495, 9785, 66326, 55452, 30812, 28558, 1202, 2783, 18897, 1168, 22232, 11723, 66326, 26962, 4728, 18897, 1170, 9038, 2569, 10032, 9672, 8447, 8038, 12958, 64803, 74161, 33079, 5934, 1159, 21073, 5526, 16742, 2061, 62632, 78625, 7648, 3372, 45818, 2286, 5526, 18897, 1168, 8038, 93481, 8478, 9450, 6251, 64803, 74161, 33079, 12156, 69199, 4479, 4161, 42996, 4977, 3687, 21073, 52395, 70999, 62632, 78625, 7648, 6251, 3372, 8150, 42996, 42493, 64359, 3552, 53857, 46126, 21958, 4719, 5588, 69199, 16777, 7952, 69199, 4719, 42996, 81253, 15114, 21073, 52395, 70999, 69199, 21015, 81253, 42493, 64359, 3552, 24496, 66326, 46126, 25497, 24378, 60265, 4719, 2156, 4281, 2286, 5526, 8478, 2712, 69199, 4719, 2156, 6251, 69199, 69199, 69199, 16777, 5447, 40233, 33284, 2712, 69199, 4719, 2156, 6688, 71941, 13388, 4719, 2156, 6688, 2611, 5447, 40233, 33284, 63299, 1155, 2109, 18897, 1168, 30812, 28558, 1202, 4149, 5588, 5526, 3131, 8781, 23208, 2286, 5526, 8038, 68442, 8478, 5588, 12156, 69199, 4479, 4161, 42996, 74161, 33079, 4977, 3687, 21073, 12156, 52395, 70999, 4479, 4161, 42996, 42493, 64359, 3552, 8478, 18655, 2109, 5526, 11290, 30812, 28558, 1202, 5617, 3560, 5526, 2385, 9978, 15114, 7608, 4347, 33284, 63299, 1155, 39470, 4464, 5588, 21073, 5526, 4281, 9450, 41831, 46126, 9323, 60265, 4532, 56690, 8038, 7608, 3081, 8313, 4532, 26962, 64803, 3926, 2156, 31896, 5526, 51667, 2061, 67112, 11286, 31896, 5526, 4479, 26962, 2524, 89391, 3217, 5588, 5526, 33060, 3884, 48563, 3003, 7021, 2547, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 12692, 2061, 18823, 50053, 4157, 2524, 62762, 4547, 1154, 37020, 12571, 60834, 28773, 23040, 16446, 22541, 7022, 22541, 3122, 3923, 7022, 1177, 60834, 1034, 2092, 3879, 8550, 37020, 46392, 6877, 4885, 88101, 7793, 15293, 4594, 9807, 3062, 5526, 7051, 7459, 11846, 7459, 46596, 2712, 2534, 8252, 64848, 29734, 59168, 5280, 6877, 4885, 5903, 4161, 7524, 67112, 3879, 8550, 3131, 9148, 7394, 4650, 5882, 2139, 8742, 48563, 23634, 8313, 1034, 2800, 23634, 5950, 3879, 5054, 8623, 2061, 10982, 4101, 3201, 7459, 13048, 2712, 10479, 5526, 4452, 21539, 57521, 3097, 1177, 4594, 7894, 6877, 4885, 3294, 26089, 3894, 2653, 6877, 4885, 18281, 3135, 5526, 5401, 6877, 3294, 2895, 4312, 25721, 3987, 4049, 6877, 4885, 5526, 3294, 27441, 32381, 3660, 6877, 4885, 7066, 3294, 43299, 3294, 5526, 2819, 61335, 1034, 2756, 15873, 4723, 6877, 4885, 29445, 2061, 3294, 61335, 5606, 33284, 58262, 6092, 22846, 6055, 33326, 2109, 2109, 2592, 16135, 38961, 1034, 57006, 4431, 2460, 7337, 5226, 11943, 7524, 6877, 4885, 65645, 6877, 4885, 5526, 47292, 21405, 6877, 4885, 56224, 5726, 5526, 2856, 3308, 2976, 37143, 2851, 3308, 3679, 38803, 34059, 7524, 2690, 2559, 32393, 2109, 29495, 2833, 32393, 29644, 2856, 6877, 5526, 4885, 2690, 5526, 3211, 3627, 14347, 5728, 5526, 28716, 2559, 4281, 6877, 4885, 16361, 83973, 2690, 6308, 53063, 15095, 3372, 4175, 11815, 5728, 63792, 7333, 5406, 5273, 2804, 7513, 4281, 83973, 22541, 32529, 1173, 65871, 4706, 21470, 32450, 25967, 8637, 4502, 4086, 8623, 2061, 4360, 17464, 25967, 91777, 80493, 1174, 9972, 2156, 6407, 4360, 58737, 1178, 3234, 3201, 25656, 5406, 2712, 3063, 6877, 17363, 2431, 4797, 2801, 3936, 5790, 6308, 5728, 2559, 3930, 13278, 4269, 6877, 4885, 32450, 5728, 23634, 4660, 4360, 6877, 4885, 6308, 6877, 4885, 2801, 3926, 5496, 4546, 2385, 40104, 5934, 6877, 5526, 6618, 90424, 45351, 3963, 5772, 27441, 4405, 25656, 40104, 3294, 5089, 3122, 7904, 6618, 40104, 10002, 4721, 3114, 2061, 5378, 11400, 2156, 84027, 5406, 6618, 6355, 39343, 1034, 3987, 5355, 9807, 4670, 6610, 64848, 5903, 15293, 5120, 6877, 4885, 5641, 2804, 2385, 5526, 3178, 6860, 3931, 3650, 6877, 4885, 5903, 66198, 3135, 4728, 1034, 5257, 5366, 10855, 5366, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3650, 4614, 2385, 4550, 9972, 2156, 4546, 2110, 2495, 4614, 5526, 4649, 2827, 21064, 2827, 9972, 2156, 2510, 5565, 10474, 2385, 21214, 14786, 2673, 3650, 9747, 2976, 2385, 4550, 9972, 2156, 2385, 4550, 9972, 2156, 3596, 382, 9977, 2384, 3884, 4614, 2044, 9147, 1160, 3681, 4215, 1177, 7853, 22269, 17550, 6679, 6412, 66198, 3308, 3961, 4779, 57324, 3926, 2619, 15293, 3824, 2156, 7030, 5997, 3308, 2851, 8134, 55452, 2385, 9978, 4815, 2865, 3828, 8134, 4173, 2535, 5526, 3627, 9988, 5447, 3308, 2166, 17904, 6903, 4532, 2044, 5565, 2974, 5997, 2865, 5455, 5526, 4269, 5938, 2325, 3270, 3481, 17480, 5882, 2523, 46611, 9221, 6187, 2786, 67112, 2385, 9978, 4367, 6598, 2156, 6405, 2501, 2510, 4529, 6392, 382, 3270, 4347, 5903, 3570, 8134, 2385, 9978, 2865, 3991, 13767, 2685, 2385, 9978, 5427, 2109, 5175, 3176, 5011, 2109, 2554, 4646, 23944, 25889, 2385, 4550, 9972, 2156, 4265, 2044, 30825, 4759, 2109, 7053, 5526, 14466, 3784, 32162, 5270, 2044, 30825, 5406, 12626, 4059, 4281, 2385, 382, 5035, 30168, 2109, 2044, 30825, 3627, 2385, 382, 5520, 3538, 2865, 2385, 9978, 2865, 5011, 3853, 48866, 5406, 2044, 30825, 6392, 382, 2384, 382, 3270, 2166, 17904, 2426, 2865, 7053, 5526, 2426, 7053, 5526, 2385, 382, 5903, 9627, 2044, 30825, 2851, 7051, 2109, 2798, 7053, 5526, 14466, 9419, 2385, 382, 38673, 2044, 5882, 2559, 2044, 30825, 8134, 3560, 2385, 9978, 10601, 4578, 8134, 33284, 2044, 53205, 5743, 2044, 5882, 2385, 9978, 8134, 6621, 2325, 33284, 7190, 5882, 3129, 4532, 2865, 2385, 9978, 2385, 382, 2865, 9627, 6403, 5455, 2385, 4550, 9972, 2156, 3312, 2325, 5455, 3560, 2385, 9978, 11942, 2426, 3451, 2431, 8134, 6853, 4728, 382, 4342, 5526, 5367, 7858, 3560, 5526, 2385, 9978, 23216, 3560, 5526, 2385, 9978, 2101, 11553, 3560, 5526, 2385, 9978, 5035, 2309, 4101, 2385, 9978, 11703, 3974, 2833, 18502, 2156, 4614, 5526, 67112, 2974, 2385, 4550, 3681, 21816, 5950, 2332, 2656, 14995, 3600, 5526, 28396, 1158, 4479, 2109, 3063, 2385, 9978, 2044, 22242, 3824, 2156, 33924, 6392, 2156, 2559, 6392, 11190, 3270, 8134, 33284, 4101, 3270, 5277, 6655, 9449, 4056, 4300, 2426, 67112, 2109, 2806, 382, 2559, 2385, 5526, 8402, 5060, 2109, 2501, 2385, 5526, 2579, 2325, 8026, 4932, 2384, 5646, 2156, 4932, 5882, 2673, 5646, 5526, 28128, 5646, 5526, 4537, 3198, 15868, 2044, 12969, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 12692, 4815, 83896, 3018, 9306, 15293, 4175, 7451, 3627, 11043, 2044, 27626, 3018, 2044, 5869, 7396, 5526, 3018, 5846, 2044, 4150, 5588, 6637, 9708, 4862, 3018, 4556, 2637, 2101, 8468, 8036, 54727, 4556, 5526, 4891, 2685, 4506, 19231, 10008, 28523, 2547, 11754, 8764, 10958, 29174, 29027, 11090, 3627, 5526, 5628, 26265, 6273, 2044, 6838, 5758, 9732, 16792, 4139, 3018, 9552, 9552, 32139, 40925, 9552, 19666, 12546, 24086, 26403, 8036, 6729, 15534, 20439, 5757, 4556, 5526, 3929, 19374, 8314, 13582, 1154, 3077, 2306, 5954, 7246, 5588, 5288, 18086, 7632, 1222, 21758, 23705, 33699, 16792, 5660, 5116, 4957, 10568, 3018, 8086, 3627, 3838, 2044, 44189, 24166, 8099, 3838, 23705, 24166, 8099, 2044, 2591, 2030, 5644, 3627, 15770, 1155, 15848, 24166, 30593, 5588, 5526, 4431, 16692, 8036, 10953, 1172, 34362, 3018, 5526, 16692, 3197, 13288, 8558, 22254, 4950, 7885, 7995, 13275, 6406, 2044, 17987, 4556, 2166, 17131, 5588, 5404, 3018, 4714, 5404, 3009, 4203, 4506, 16932, 7029, 95021, 3219, 5404, 17131, 4556, 12692, 12692, 8644, 8257, 2410, 5526, 7749, 8087, 11043, 4556, 5869, 94030, 5869, 7010, 10577, 32241, 4556, 15637, 33991, 2510, 35063, 10122, 4104, 31938, 9948, 17131, 6149, 10577, 2101, 5526, 27070, 3975, 12640, 9978, 5411, 14284, 12260, 4556, 15637, 84179, 22541, 12153, 14081, 22541, 2044, 5469, 4715, 2044, 4550, 1155, 2101, 5526, 22541, 12640, 9784, 24151, 9085, 10597, 2044, 61976, 2510, 2087, 8447, 3337, 2325, 33488, 8447, 38762, 57466, 83973, 5411, 83973, 3695, 3099, 22541, 17131, 17131, 13278, 15541, 50208, 3838, 15637, 50208, 4968, 3923, 5805, 1176, 4556, 2817, 84179, 5964, 2510, 35063, 3608, 3650, 83973, 4556, 13684, 9948, 4639, 9934, 4360, 4670, 5805, 83973, 2232, 2044, 2527, 4813, 1158, 2384, 4556, 3336, 15712, 16010, 4556, 4624, 13109, 2806, 2384, 2288, 4556, 11372, 2431, 12546, 2524, 4556, 4432, 98179, 2510, 36153, 46717, 2156, 22541, 2109, 2510, 4404, 4432, 8447, 13767, 6100, 3357, 4432, 8447, 4374, 2101, 5526, 5018, 13288, 2232, 4556, 4149, 6303, 4556, 50208, 4670, 18251, 18251, 4043, 1176, 2488, 2057, 2510, 35063, 4556, 50208, 4360, 4360, 83973, 2232, 21773, 7478, 15676, 2979, 1176, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6899, 2712, 21073, 5526, 9842, 25103, 44304, 6078, 22627, 24747, 18231, 64360, 2810, 12035, 9088, 4548, 9557, 8415, 5526, 8065, 6575, 2460, 13692, 3555, 2919, 12094, 2712, 2232, 2044, 4516, 4728, 4639, 9792, 4165, 10819, 8104, 2865, 4728, 6899, 5277, 7591, 5526, 8415, 5526, 12035, 9088, 6575, 5283, 5063, 10258, 9450, 3246, 2385, 21214, 2232, 73501, 2810, 12035, 9088, 4548, 14850, 3450, 2044, 16422, 9450, 5526, 2044, 35666, 2810, 11023, 12035, 9088, 3239, 5620, 2488, 21189, 3129, 13499, 12035, 9088, 9450, 5526, 2832, 5439, 5526, 4817, 7809, 40142, 4715, 2109, 5526, 7503, 6966, 5303, 9450, 5526, 2810, 8549, 9755, 5526, 9716, 5526, 3135, 5933, 2232, 2044, 8740, 2345, 5526, 4269, 2801, 4168, 5883, 25163, 38413, 24747, 11023, 4548, 14850, 22687, 5526, 9450, 3926, 22627, 24747, 21992, 51364, 5526, 2886, 8639, 7697, 14850, 6097, 2166, 68442, 41181, 5526, 22627, 24747, 2510, 22236, 4660, 2156, 2810, 69460, 41181, 5526, 7592, 3129, 4281, 10704, 2810, 9450, 6298, 5257, 2044, 3969, 3141, 4896, 2801, 9450, 12656, 9450, 2783, 9450, 4479, 8483, 9450, 25163, 38413, 24747, 3608, 2384, 3568, 5155, 4418, 2044, 58739, 2920, 4281, 4721, 22259, 66198, 22627, 24747, 2851, 6619, 21015, 5558, 33284, 2920, 22627, 24747, 21992, 51364, 53900, 4479, 5933, 4347, 6637, 4216, 4649, 5554, 2340, 2286, 5526, 3178, 3372, 5215, 3801, 2719, 2384, 3336, 7670, 4879, 2340, 11023, 3372, 9450, 34691, 9450, 80277, 21015, 38393, 68395, 2801, 11868, 34691, 7083, 5663, 6308, 5554, 3568, 9450, 21015, 25163, 38413, 24747, 15896, 9450, 42401, 17077, 98679, 13218, 6298, 13756, 21497, 6619, 11868, 3372, 6308, 9450, 34691, 3926, 4281, 10725, 25163, 38413, 24747, 5170, 26364, 21015, 2920, 2109, 5378, 19156, 10338, 18231, 64360, 38047, 14965, 24747, 3954, 3336, 67112, 8198, 26364, 5170, 3081, 3828, 4639, 11868, 34691, 10966, 68395, 2801, 13218, 5257, 11329, 5874, 3464, 11868, 3657, 5531, 9450, 42401, 17077, 98679, 2856, 2384, 4479, 3596, 9450, 2384, 3372, 5215, 2109, 2109, 3538, 5018, 2719, 13260, 5018, 2044, 30825, 41448, 2817, 2719, 9792, 2976, 25103, 44304, 2044, 32546, 1155, 5151, 7591, 5526, 12035, 9088, 8415, 5526, 6899, 3884, 2110, 2495, 21073, 5526, 4850, 5526, 55452, 2109, 2044, 7954, 8851, 2286, 5526, 2832, 21992, 51364, 5526, 4715, 10526, 13060, 38370, 8851, 11793, 4203, 4728, 4360, 4269, 3531, 3650, 4165, 2044, 30825, 10819, 6308, 3650, 5558, 7519, 72303, 5526, 3660, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2559, 2766, 6886, 2619, 15293, 8912, 5526, 6093, 5617, 3188, 5526, 3451, 39963, 1158, 3926, 4224, 5526, 9036, 5526, 8822, 3564, 2766, 7771, 3145, 16609, 5175, 40142, 3564, 2766, 6886, 6899, 3926, 7063, 3813, 3564, 2766, 2783, 3930, 23079, 2156, 5617, 7676, 11461, 8245, 2766, 17199, 60109, 2766, 6660, 9213, 24496, 11461, 6078, 13577, 16161, 5526, 10143, 2432, 3564, 2766, 24496, 4175, 3579, 23079, 6598, 2156, 5228, 8336, 2766, 2524, 33284, 3530, 16674, 1167, 8674, 11393, 9036, 5526, 2766, 3926, 382, 4656, 5293, 3564, 2766, 23079, 11461, 5526, 3530, 6937, 16674, 1167, 8674, 11393, 54600, 5535, 4175, 3579, 4175, 2766, 25774, 4754, 3564, 2766, 10029, 3816, 2098, 2268, 2766, 18357, 99163, 53454, 26393, 46149, 5811, 3530, 4224, 2766, 2432, 16674, 1167, 8674, 11393, 4781, 8400, 2298, 5293, 2766, 2109, 2431, 4224, 9036, 5708, 7837, 9036, 5526, 4281, 7837, 4224, 391, 345, 396, 9036, 2431, 7837, 9632, 4224, 9036, 5362, 4099, 2766, 3530, 16674, 1167, 8674, 13126, 1192, 5646, 4175, 3579, 5362, 3930, 3926, 6722, 3145, 7503, 6886, 2766, 3450, 2044, 9520, 4281, 4224, 23079, 4752, 9005, 13577, 5526, 10143, 4103, 69415, 2611, 6886, 8305, 8912, 5526, 2766, 4224, 9036, 6886, 2766, 9036, 5526, 2298, 4429, 3451, 3145, 16609, 8006, 2432, 5617, 31480, 1177, 6575, 6273, 5913, 6899, 2559, 94913, 7190, 74589, 24695, 4037, 18726, 8734, 6803, 3427, 22687, 2766, 3451, 2426, 4502, 5293, 6168, 2952, 2940, 2821, 23079, 3123, 4114, 2298, 8734, 6803, 4224, 5293, 3530, 3314, 6167, 4260, 3663, 14304, 4281, 3663, 33296, 1166, 3028, 1156, 8900, 5526, 6481, 2801, 7513, 6078, 38655, 77481, 14581, 11238, 1220, 29525, 44767, 92020, 26541, 11238, 16043, 92569, 1220, 29525, 66903, 9840, 8734, 6803, 23079, 3450, 2224, 2495, 3926, 6886, 2766, 5646, 4014, 3824, 4728, 13178, 5530, 5293, 6783, 2766, 3450, 2044, 9520, 2592, 6886, 2766, 3450, 13178, 2766, 2806, 4260, 2044, 9520, 4624, 5997, 2611, 3564, 2766, 5439, 24496, 8685, 7199, 7837, 2361, 9834, 15293, 9036, 2766, 7199, 2232, 2563, 10469, 6886, 2563, 2098, 2268, 4099, 4360, 23079, 3950, 33284, 4175, 6952, 4099, 3600, 38311, 3046, 2766, 4643, 5526, 6899, 6860, 6886, 2766, 5065, 2488, 3464, 2559, 2766, 3564, 4649, 5018, 5913, 3680, 5526, 2766, 4103, 6860, 3161, 3482, 3663, 5526, 2766, 3564, 13178, 5293, 2298, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6899, 5526, 4639, 24248, 6385, 4281, 4781, 2976, 6899, 2044, 68139, 1155, 4161, 5526, 24248, 6385, 3555, 4161, 5526, 7619, 24248, 6385, 5373, 16437, 6899, 2044, 68139, 1155, 3308, 5447, 21773, 24248, 6385, 23634, 5950, 24248, 6385, 46531, 51424, 5526, 5933, 17371, 5039, 66198, 20747, 1158, 10562, 18823, 5663, 46700, 5726, 5526, 2044, 25441, 3123, 2801, 48563, 5624, 38007, 1174, 5526, 4016, 4203, 2801, 4549, 2801, 2488, 5526, 6949, 5624, 38007, 1174, 5526, 4549, 2750, 2804, 5663, 19963, 4281, 4781, 2156, 30312, 24248, 6385, 2976, 4281, 62966, 4781, 27472, 24248, 6385, 48089, 58161, 4859, 4728, 6899, 2976, 24248, 6385, 4281, 4781, 62966, 46531, 2940, 11723, 30312, 62966, 3434, 2044, 14678, 62966, 5126, 3025, 2044, 43569, 11290, 5624, 38007, 1174, 11723, 2109, 2044, 14678, 41256, 2940, 33840, 24248, 6385, 3308, 2384, 5447, 26450, 2501, 3308, 5526, 5447, 5526, 19952, 25044, 5624, 38007, 1174, 4576, 17076, 8006, 2109, 4781, 2325, 11290, 2724, 52911, 2162, 11214, 7184, 11290, 62966, 5624, 38007, 1174, 2564, 61043, 21522, 3123, 5624, 38007, 1174, 5526, 13819, 10112, 17076, 2232, 13278, 5663, 62966, 2044, 14678, 13245, 10562, 9154, 62966, 3608, 2044, 2323, 4203, 11400, 2156, 62966, 66198, 64297, 52871, 1174, 5152, 10562, 2166, 2044, 79864, 5378, 4030, 2524, 6899, 20712, 1174, 26255, 17776, 2044, 81898, 8006, 5624, 38007, 1174, 3608, 14678, 24248, 6385, 2109, 5743, 14830, 8447, 60314, 8305, 32330, 4157, 2432, 10880, 76036, 23083, 91342, 1220, 5624, 38007, 1174, 27472, 7496, 24578, 1161, 2432, 28278, 2385, 2156, 27472, 3123, 6880, 4856, 14030, 11723, 5624, 38007, 1174, 5526, 2723, 19963, 79620, 1192, 24725, 67509, 6240, 35723, 11214, 7184, 17371, 5624, 38007, 1174, 5624, 38007, 1174, 2724, 35723, 80041, 1177, 2723, 5624, 38007, 1174, 5526, 3123, 12902, 48713, 2385, 4431, 22639, 5624, 38007, 1174, 5526, 5827, 2900, 4885, 11214, 7184, 5526, 5624, 38007, 1174, 2044, 6662, 1177, 3123, 12269, 37261, 4405, 12902, 2298, 7809, 2385, 5822, 46531, 2940, 5624, 38007, 1174, 2044, 6662, 1177, 12902, 5624, 38007, 1174, 27015, 3100, 8182, 11868, 38184, 6512, 2044, 16845, 1155, 46531, 2940, 24248, 6385, 4603, 11723, 21491, 28049, 1166, 31048, 10880, 19122, 11723, 3680, 5526, 21491, 28049, 1166, 51424, 5630, 7480, 6450, 24248, 6385, 7809, 8065, 11723, 21491, 28049, 1166, 31048, 8065, 11290, 13448, 1176, 3600, 24248, 6385, 7198, 6061, 7809, 7198, 9107, 5040, 1177, 6794, 2044, 14394, 24248, 6385, 2786, 19123, 9706, 24248, 6385, 4781, 4281, 4781, 5500, 18655, 18715, 6899, 2976, 24248, 6385, 12411, 3600, 46923, 5645, 2338, 1155, 4624, 3600, 5526, 46923, 24248, 6385, 6899, 5526, 83973, 42670, 32670, 43622, 43493, 36787, 29847, 27642, 17575, 4296, 89920, 1192, 27321, 42355, 11238, 43882, 16230, 29847, 27642, 17575, 4296, 43622, 40687, 11238, 43882, 16230, 29847, 27642, 17575, 4296, 89920, 1192, 27321, 58015, 11238, 77395, 36787, 29847, 27642, 17575, 4296, 39988, 12023, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"uF3FFsPzc6zD","executionInfo":{"status":"ok","timestamp":1610562826270,"user_tz":-210,"elapsed":1951,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["sentence_maxlen=128"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m2Qc2IkrnEp","executionInfo":{"status":"ok","timestamp":1610562847103,"user_tz":-210,"elapsed":22605,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"70cccc52-b58f-425d-f0f3-bccada7f387d"},"source":["##Tokenize training and validation sentences:\r\n","train_encodings = tokenizer.batch_encode_plus(X_train,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","val_encodings = tokenizer.batch_encode_plus(X_val,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","test_encodings=tokenizer.batch_encode_plus(X_test,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1lb-FHzi85f23hoP0mhmQgBKS9NTCNtj_"},"id":"IwjkXARbetX-","executionInfo":{"status":"ok","timestamp":1610562886312,"user_tz":-210,"elapsed":61607,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"96a58030-1910-4d24-e54c-33ed19e6440e"},"source":["train_encodings"],"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"-iCp2PUEupYK","executionInfo":{"status":"ok","timestamp":1610562886313,"user_tz":-210,"elapsed":61393,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","\r\n","# for train set\r\n","train_seq = torch.tensor(train_encodings['input_ids'])\r\n","train_mask = torch.tensor(train_encodings['attention_mask'])\r\n","train_y = torch.tensor(y_train)\r\n","\r\n","# for validation set\r\n","val_seq = torch.tensor(val_encodings['input_ids'])\r\n","val_mask = torch.tensor(val_encodings['attention_mask'])\r\n","val_y = torch.tensor(y_val)\r\n","\r\n","# for test set\r\n","test_seq = torch.tensor(test_encodings['input_ids'])\r\n","test_mask = torch.tensor(test_encodings['attention_mask'])\r\n","test_y = torch.tensor(y_test)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0JkQbxVBmbM","executionInfo":{"status":"ok","timestamp":1610562886314,"user_tz":-210,"elapsed":61178,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"5ea7b39b-3d0e-46f1-f524-04fa69892773"},"source":["train_y[0]"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"T2xiV6Nb0ddZ","executionInfo":{"status":"ok","timestamp":1610562886314,"user_tz":-210,"elapsed":60550,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","#define a batch size\r\n","batch_size = 32\r\n","\r\n","# wrap tensors\r\n","train_data = TensorDataset(train_seq, train_mask, train_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","train_sampler = RandomSampler(train_data)\r\n","\r\n","# dataLoader for train set\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","val_data = TensorDataset(val_seq, val_mask, val_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","val_sampler = SequentialSampler(val_data)\r\n","\r\n","# dataLoader for validation set\r\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","test_data = TensorDataset(test_seq, test_mask, test_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","test_sampler = SequentialSampler(test_data)\r\n","\r\n","# dataLoader for validation set\r\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwGHXIjGfmaN","executionInfo":{"status":"ok","timestamp":1610562886316,"user_tz":-210,"elapsed":60169,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"c1a7dc1c-c87b-48eb-9c9f-fc16809f69af"},"source":["# example\r\n","\r\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n","\r\n","# encode text\r\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\r\n","print(sent_id)\r\n","\r\n","seq = torch.tensor(sent_id['input_ids'])\r\n","mask = torch.tensor(sent_id['attention_mask'])\r\n","train_y = torch.tensor([0,1])\r\n","\r\n","transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\r\n","\r\n","cls_hs=transformer_model(seq,mask)\r\n","print(cls_hs)\r\n","print(cls_hs[0])\r\n","print(cls_hs[1])\r\n","print(cls_hs[1].shape)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["{'input_ids': [[2, 47701, 11684, 29, 41674, 1166, 45084, 19279, 16043, 80349, 4, 0], [2, 19922, 40200, 70992, 14, 63336, 1167, 29, 41674, 1166, 45084, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.8125,  0.1581,  0.8727,  ...,  0.5246,  0.0038, -0.6973],\n","         [-0.2913, -0.2989, -0.2735,  ..., -0.0069,  0.2791, -0.9139],\n","         [ 0.3463,  0.7396,  0.5679,  ...,  1.3241,  0.0418,  0.3093],\n","         ...,\n","         [-1.1403, -0.0708, -0.5872,  ...,  0.3454, -0.1819, -0.4568],\n","         [-0.3126, -0.1210,  0.4067,  ...,  0.2649,  0.5835, -0.3233],\n","         [-1.2195,  0.4703, -0.3364,  ..., -0.0554, -0.2337, -0.5484]],\n","\n","        [[-1.5032, -0.4642,  0.4964,  ...,  0.8154,  0.1530, -0.2859],\n","         [ 0.8593, -0.1923, -0.2904,  ...,  0.0486,  0.6341, -0.0399],\n","         [ 1.1156, -0.8193, -0.0553,  ..., -0.0439,  0.3506,  0.4468],\n","         ...,\n","         [-1.1793, -1.0316, -0.1007,  ...,  0.8715,  0.4843, -0.4112],\n","         [-1.3094, -0.8947, -0.7444,  ...,  0.2073,  0.2773, -0.5733],\n","         [-0.0913, -0.7924,  0.3655,  ...,  0.0437,  0.4312, -0.0743]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.1764,  0.5810, -0.1167,  ..., -0.2929, -0.4690, -0.3336],\n","        [ 0.1098,  0.3696, -0.2275,  ..., -0.3062, -0.4727, -0.1282]],\n","       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","tensor([[[-1.8125,  0.1581,  0.8727,  ...,  0.5246,  0.0038, -0.6973],\n","         [-0.2913, -0.2989, -0.2735,  ..., -0.0069,  0.2791, -0.9139],\n","         [ 0.3463,  0.7396,  0.5679,  ...,  1.3241,  0.0418,  0.3093],\n","         ...,\n","         [-1.1403, -0.0708, -0.5872,  ...,  0.3454, -0.1819, -0.4568],\n","         [-0.3126, -0.1210,  0.4067,  ...,  0.2649,  0.5835, -0.3233],\n","         [-1.2195,  0.4703, -0.3364,  ..., -0.0554, -0.2337, -0.5484]],\n","\n","        [[-1.5032, -0.4642,  0.4964,  ...,  0.8154,  0.1530, -0.2859],\n","         [ 0.8593, -0.1923, -0.2904,  ...,  0.0486,  0.6341, -0.0399],\n","         [ 1.1156, -0.8193, -0.0553,  ..., -0.0439,  0.3506,  0.4468],\n","         ...,\n","         [-1.1793, -1.0316, -0.1007,  ...,  0.8715,  0.4843, -0.4112],\n","         [-1.3094, -0.8947, -0.7444,  ...,  0.2073,  0.2773, -0.5733],\n","         [-0.0913, -0.7924,  0.3655,  ...,  0.0437,  0.4312, -0.0743]]],\n","       grad_fn=<NativeLayerNormBackward>)\n","tensor([[ 0.1764,  0.5810, -0.1167,  ..., -0.2929, -0.4690, -0.3336],\n","        [ 0.1098,  0.3696, -0.2275,  ..., -0.3062, -0.4727, -0.1282]],\n","       grad_fn=<TanhBackward>)\n","torch.Size([2, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByUEn_v4zknn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"n3AjEaHcEMfb","executionInfo":{"status":"ok","timestamp":1610562905126,"user_tz":-210,"elapsed":5630,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaAlYydhxPTd","executionInfo":{"status":"ok","timestamp":1610562914027,"user_tz":-210,"elapsed":1366,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# freeze all the parameters\r\n","for param in transformer_model.parameters():\r\n","    param.requires_grad = False"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUa1R1WQONe6","executionInfo":{"status":"ok","timestamp":1610562914659,"user_tz":-210,"elapsed":1621,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"bf27a510-217e-4372-9d41-a14937a449bb"},"source":["len(labels)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"oyE_ThEms5aZ","executionInfo":{"status":"ok","timestamp":1610562915199,"user_tz":-210,"elapsed":1223,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["class BERT_Arch(nn.Module):\r\n","\r\n","    def __init__(self, bert):\r\n","      \r\n","      super(BERT_Arch, self).__init__()\r\n","\r\n","      self.bert = bert \r\n","      \r\n","      # dropout layer\r\n","      self.dropout = nn.Dropout(0.1)\r\n","      \r\n","      # relu activation function\r\n","      self.relu =  nn.ReLU()\r\n","\r\n","      # dense layer 1\r\n","      self.fc1 = nn.Linear(768,512)\r\n","      \r\n","      # dense layer 2 (Output layer)\r\n","      self.fc2 = nn.Linear(512,78)\r\n","\r\n","      #sigmoid activation function\r\n","      self.sigmoid = nn.Sigmoid()\r\n","\r\n","    #define the forward pass\r\n","    def forward(self, sent_id, mask):\r\n","\r\n","      #pass the inputs to the model  \r\n","      cls_hs = self.bert(sent_id, attention_mask=mask)\r\n","      \r\n","      x = self.fc1(cls_hs[1])\r\n","\r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      # output layer\r\n","      x = self.fc2(x)\r\n","      \r\n","      # apply sigmoid activation\r\n","      x = self.sigmoid(x)\r\n","\r\n","      return x"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDuHzo96z6z8","executionInfo":{"status":"ok","timestamp":1610562921846,"user_tz":-210,"elapsed":1434,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(transformer_model)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUNSLBYcLc9q","executionInfo":{"status":"ok","timestamp":1610562924255,"user_tz":-210,"elapsed":2067,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArHmwhh7JrZh","executionInfo":{"status":"ok","timestamp":1610562924255,"user_tz":-210,"elapsed":1279,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["loss_func =nn.MultiLabelSoftMarginLoss()"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8LjQyDXs0bG","executionInfo":{"status":"ok","timestamp":1610562926395,"user_tz":-210,"elapsed":1848,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function to train the model\r\n","def train():\r\n","  \r\n","  model.train()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save model predictions\r\n","  total_preds=[]\r\n","  \r\n","  # iterate over batches\r\n","  for step,batch in enumerate(train_dataloader):\r\n","    \r\n","    # progress update after every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [r.to(device) for r in batch]\r\n"," \r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # clear previously calculated gradients \r\n","    model.zero_grad()        \r\n","\r\n","    # get model predictions for the current batch\r\n","    preds = model(sent_id, mask)\r\n","\r\n","    # compute the loss between actual and predicted values\r\n","    \r\n","    loss = loss_func(preds, labels)\r\n","    # add on to the total loss\r\n","    total_loss = total_loss + loss.item()\r\n","\r\n","    # backward pass to calculate the gradients\r\n","    loss.backward()\r\n","\r\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","    # update parameters\r\n","    optimizer.step()\r\n","\r\n","    # model predictions are stored on GPU. So, push it to CPU\r\n","    preds=preds.detach().cpu().numpy()\r\n","\r\n","    # append the model predictions\r\n","    total_preds.append(preds)\r\n","\r\n","  # compute the training loss of the epoch\r\n","  avg_loss = total_loss / len(train_dataloader)\r\n","  \r\n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  #returns the loss and predictions\r\n","  return avg_loss, total_preds"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNBRQo9WMHey","executionInfo":{"status":"ok","timestamp":1610562927046,"user_tz":-210,"elapsed":1545,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function for evaluating the model\r\n","def evaluate():\r\n","  \r\n","  print(\"\\nEvaluating...\")\r\n","  \r\n","  # deactivate dropout layers\r\n","  model.eval()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save the model predictions\r\n","  total_preds = []\r\n","\r\n","  # iterate over batches\r\n","  for step,batch in enumerate(val_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","\r\n","      # compute the validation loss between actual and predicted values\r\n","      loss = loss_func(preds,labels)\r\n","\r\n","      total_loss = total_loss + loss.item()\r\n","\r\n","      preds = preds.detach().cpu().numpy()\r\n","\r\n","      total_preds.append(preds)\r\n","\r\n","  # compute the validation loss of the epoch\r\n","  avg_loss = total_loss / len(val_dataloader) \r\n","\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  return avg_loss, total_preds"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qu5pfrJKtTc0","executionInfo":{"status":"ok","timestamp":1610564456703,"user_tz":-210,"elapsed":1528904,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"3af58a5e-41bb-45ae-c495-eb1ad5195cb1"},"source":["# number of training epochs\r\n","epochs = 10\r\n","\r\n","# set initial loss to infinite\r\n","best_valid_loss = float('inf')\r\n","\r\n","# empty lists to store training and validation loss of each epoch\r\n","train_losses=[]\r\n","valid_losses=[]\r\n","\r\n","#for each epoch\r\n","for epoch in range(epochs):\r\n","     \r\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n","    \r\n","    #train model\r\n","    train_loss, _ = train()\r\n","    \r\n","    #evaluate model\r\n","    valid_loss, _ = evaluate()\r\n","    \r\n","    #save the best model\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'saved_weights.pt')\r\n","    \r\n","    # append training and validation loss\r\n","    train_losses.append(train_loss)\r\n","    valid_losses.append(valid_loss)\r\n","    \r\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":58,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"],"name":"stderr"},{"output_type":"stream","text":["  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.696\n","Validation Loss: 0.693\n","\n"," Epoch 2 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 3 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 4 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 5 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 6 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 7 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 8 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 9 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 10 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JIhTtLgOCP9t"},"source":["#### Save model"]},{"cell_type":"code","metadata":{"id":"cNjLfGyoCSp6","executionInfo":{"status":"ok","timestamp":1610565225024,"user_tz":-210,"elapsed":5167,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["path = 'bert-base-parsbert-uncased.pt'\n","torch.save(model.state_dict(), path)"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ2_aS0zCLvp"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"cvR-FhPpuLkR"},"source":["# torch.cuda.empty_cache()\r\n","# pass the pre-trained BERT to our define architecture\r\n","model = BERT_Arch(transformer_model)\r\n","\r\n","# push the model to GPU\r\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"3aOPRZ2jVvNR","executionInfo":{"status":"error","timestamp":1610564459193,"user_tz":-210,"elapsed":2471,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"cc83fa24-4344-461a-a4ad-797be76cc908"},"source":["#load weights of best model\r\n","path = 'bert-base-parsbert-uncased.pt'\r\n","model.load_state_dict(torch.load(path))"],"execution_count":59,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-3b86d84245ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load weights of best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bert-base-parsbert-uncased.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert-base-parsbert-uncased.pt'"]}]},{"cell_type":"markdown","metadata":{"id":"PM1uUcZFCPVg"},"source":["After loading model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZhHObMnzuws","executionInfo":{"status":"ok","timestamp":1610565648750,"user_tz":-210,"elapsed":38117,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"064fbdf5-6e43-4f44-bf93-7bc3f23fc358"},"source":["y_pred=[]\r\n","y_true=[]\r\n","for step,batch in enumerate(test_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","      # print(preds)\r\n","      # print(preds.cpu().numpy())\r\n","      preds = preds.cpu().numpy()\r\n","      # model's performance\r\n","    # preds = numpy.argmax(preds, axis = 1)\r\n","    \r\n","    measure = numpy.mean(preds[0]) + 4*numpy.sqrt(numpy.var(preds[0]))\r\n","    for l in preds:\r\n","      temp=[]\r\n","      for value in l:\r\n","        if value >= measure:\r\n","          temp.append(1)\r\n","        else:\r\n","          temp.append(0)\r\n","      y_pred.append(temp)\r\n","    y_true.extend(labels.cpu().numpy())\r\n","    # print(labels.cpu().numpy()[0], preds[0])\r\n","print(classification_report(y_true, y_pred))"],"execution_count":79,"outputs":[{"output_type":"stream","text":["  Batch    50  of    135.\n","  Batch   100  of    135.\n","              precision    recall  f1-score   support\n","\n","           0       0.03      0.26      0.05       105\n","           1       0.05      0.46      0.08       162\n","           2       0.03      0.38      0.06       140\n","           3       0.00      0.50      0.01        12\n","           4       0.01      0.17      0.03        76\n","           5       0.01      0.39      0.01        18\n","           6       0.03      0.11      0.04       151\n","           7       0.00      0.17      0.01        35\n","           8       0.00      0.14      0.01        28\n","           9       0.02      0.21      0.03       107\n","          10       0.03      0.38      0.05       121\n","          11       0.01      0.23      0.02        60\n","          12       0.06      0.39      0.11       138\n","          13       0.00      0.07      0.00        14\n","          14       0.00      0.06      0.00        18\n","          15       0.02      0.15      0.04       157\n","          16       0.02      0.17      0.03       132\n","          17       0.04      0.20      0.06       146\n","          18       0.03      0.34      0.05       105\n","          19       0.01      0.15      0.02        68\n","          20       0.02      0.14      0.03       127\n","          21       0.02      0.19      0.04       139\n","          22       0.01      0.26      0.02        54\n","          23       0.02      0.20      0.04       133\n","          24       0.01      0.18      0.02        73\n","          25       0.03      0.14      0.04       152\n","          26       0.03      0.24      0.05       149\n","          27       0.02      0.14      0.04       154\n","          28       0.02      0.17      0.03       127\n","          29       0.03      0.25      0.05       133\n","          30       0.02      0.27      0.04        63\n","          31       0.02      0.24      0.04        90\n","          32       0.02      0.21      0.03       119\n","          33       0.03      0.17      0.05       156\n","          34       0.02      0.19      0.03        75\n","          35       0.03      0.18      0.06       136\n","          36       0.03      0.26      0.05       137\n","          37       0.01      0.12      0.02        65\n","          38       0.01      0.21      0.02        43\n","          39       0.01      0.14      0.01        36\n","          40       0.03      0.23      0.05       160\n","          41       0.02      0.16      0.03       111\n","          42       0.01      0.16      0.02        81\n","          43       0.02      0.11      0.04       142\n","          44       0.03      0.30      0.05       154\n","          45       0.00      0.19      0.01        21\n","          46       0.01      0.25      0.02        40\n","          47       0.03      0.25      0.05       156\n","          48       0.03      0.22      0.05       144\n","          49       0.02      0.31      0.03        59\n","          50       0.00      0.19      0.01        31\n","          51       0.00      0.19      0.01        27\n","          52       0.02      0.41      0.03        49\n","          53       0.02      0.24      0.03       101\n","          54       0.02      0.31      0.04       117\n","          55       0.01      0.23      0.02        30\n","          56       0.01      0.15      0.02        55\n","          57       0.01      0.49      0.02        43\n","          58       0.04      0.30      0.06       133\n","          59       0.03      0.34      0.06       147\n","          60       0.04      0.60      0.07       141\n","          61       0.00      0.05      0.01        57\n","          62       0.03      0.24      0.05       143\n","          63       0.02      0.16      0.03       139\n","          64       0.01      0.18      0.02        78\n","          65       0.03      0.28      0.06       163\n","          66       0.00      0.07      0.01        40\n","          67       0.02      0.20      0.04       106\n","          68       0.05      0.44      0.10       147\n","          69       0.03      0.10      0.05       134\n","          70       0.02      0.30      0.04       130\n","          71       0.03      0.36      0.06       135\n","          72       0.02      0.08      0.03       123\n","          73       0.01      0.29      0.02        41\n","          74       0.01      0.28      0.01        25\n","          75       0.02      0.12      0.04       148\n","          76       0.02      0.29      0.03        72\n","          77       0.03      0.31      0.05       120\n","\n","   micro avg       0.02      0.24      0.04      7697\n","   macro avg       0.02      0.23      0.04      7697\n","weighted avg       0.02      0.24      0.04      7697\n"," samples avg       0.01      0.28      0.02      7697\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytalqQK6CYdg","executionInfo":{"status":"ok","timestamp":1610565458779,"user_tz":-210,"elapsed":1089,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"b6cbcfb4-edd9-4ab5-cd61-1b83b6e8df14"},"source":["y_pred[-1][:10]"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 1, 1, 0, 0, 0, 0, 0, 1]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJdoqoXvLE_V","executionInfo":{"status":"ok","timestamp":1610565462908,"user_tz":-210,"elapsed":968,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"d03f4010-c567-444e-a0d4-9620126bd3d3"},"source":["l[:10]"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.1089925e-09, 3.1846412e-09, 3.2408070e-09, 2.1834037e-09,\n","       6.6468608e-10, 1.0195572e-09, 3.0647768e-10, 1.5789945e-09,\n","       1.0513498e-09, 1.9063064e-09], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAksL6IILaSd","executionInfo":{"status":"ok","timestamp":1610565488512,"user_tz":-210,"elapsed":954,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"7b4ac449-cc1a-4c57-842a-5014b2c1ebd0"},"source":["y_true[-1][:10]"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwTTsDlVLwue","executionInfo":{"status":"ok","timestamp":1610565513422,"user_tz":-210,"elapsed":912,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"b93c38e4-7377-4334-e5b9-3f729e62c25e"},"source":["measure"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.5832182009845042e-09"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWKerpH8L20X","executionInfo":{"status":"ok","timestamp":1610565554765,"user_tz":-210,"elapsed":928,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"a2b98105-9e42-4220-d31b-a1bd0d0d7f80"},"source":["numpy.mean(preds[0]) + 3*numpy.sqrt(numpy.var(preds[0]))"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.9233308551646076e-09"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"d_ZALkfFL9Dh"},"source":[""],"execution_count":null,"outputs":[]}]}