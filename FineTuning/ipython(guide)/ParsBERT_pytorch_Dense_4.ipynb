{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ParsBERT_pytorch_Dense_4.ipynb","provenance":[{"file_id":"1k6QLcPAJKT5H2yTy61U-YCB267inolmb","timestamp":1610512876213},{"file_id":"1AIm-KimERCJutlpyjiZ2IAwM-cCrVsWM","timestamp":1610440221899},{"file_id":"17mqUcShahUjZjQxywgKQSGU1jV-CZW8o","timestamp":1610336820188},{"file_id":"1FgtzYXY0CXNyE_2FU4IEqJTQzmVZNvDh","timestamp":1610105938882}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0039549159a145799a63baaae6264208":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_399d958be0f34cf6a8b592b4b8cde6bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d45b051e98114d398ca7640fe54de69e","IPY_MODEL_ed557b003fc74f9ebf043751fd33b7e4"]}},"399d958be0f34cf6a8b592b4b8cde6bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d45b051e98114d398ca7640fe54de69e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8f1f7cc60e1241b19b0b1bc7075998a9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1441,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1441,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f27f2267d5f8420eb9a8ebf382e49749"}},"ed557b003fc74f9ebf043751fd33b7e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c007587ed7024e22ae6ebeb9b52b413b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.44k/1.44k [00:06&lt;00:00, 237B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3efdd16b912c4cce8d61ab98d7bfe9f3"}},"8f1f7cc60e1241b19b0b1bc7075998a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f27f2267d5f8420eb9a8ebf382e49749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c007587ed7024e22ae6ebeb9b52b413b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3efdd16b912c4cce8d61ab98d7bfe9f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b518759a42a47709d5b4e6a09ca33e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47e0835a4c9a42b08a9aa302484a5aa2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b45ab22f7ddd4fbf824d949e069f75df","IPY_MODEL_7582e505f5404c92917b3e424e68142f"]}},"47e0835a4c9a42b08a9aa302484a5aa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b45ab22f7ddd4fbf824d949e069f75df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49695672a251420e997543a63f70dc25","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1198122,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1198122,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9a1d6c6d855448d92fd956944806761"}},"7582e505f5404c92917b3e424e68142f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6f12313316324094b644436e3e370d26","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20M/1.20M [00:04&lt;00:00, 251kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c8c66c5d1214881ad48053c762fe913"}},"49695672a251420e997543a63f70dc25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f9a1d6c6d855448d92fd956944806761":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f12313316324094b644436e3e370d26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6c8c66c5d1214881ad48053c762fe913":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c229fffb69c34220a4e6ebf804143181":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8ec01f6587b04da0a78040242d3bbbe7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33d2aa1827b54a8689d1bef81b505c5f","IPY_MODEL_d19d6f39ccbf40e7be252155a4ffb76f"]}},"8ec01f6587b04da0a78040242d3bbbe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33d2aa1827b54a8689d1bef81b505c5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8d8f9cf894a743b9b05731534cc7957f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44097319a0bf472a84ebaadb3bf21fe9"}},"d19d6f39ccbf40e7be252155a4ffb76f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f0ec2dc0a5b465697a34b8901a2667e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:01&lt;00:00, 78.2B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_faf07d8acf9a46c99fed35cc57d4a078"}},"8d8f9cf894a743b9b05731534cc7957f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"44097319a0bf472a84ebaadb3bf21fe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f0ec2dc0a5b465697a34b8901a2667e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"faf07d8acf9a46c99fed35cc57d4a078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5139ca21f934b27a184b1dbef254ec1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_df17b89a3ab84c9c984b382bb9e47c47","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_805b369f488845ef82a88904604c9464","IPY_MODEL_ac921dff17674dc9bd5ebc9b0555a1ad"]}},"df17b89a3ab84c9c984b382bb9e47c47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"805b369f488845ef82a88904604c9464":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_89a83a0238674a12a9f967d3deca8dcd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":62,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":62,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d15a6598dd534650b26e135c30361298"}},"ac921dff17674dc9bd5ebc9b0555a1ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f35a590455a477aa59bd4b05a81fdb0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62.0/62.0 [00:00&lt;00:00, 403B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ae3221379044bf68690e417e458ccdb"}},"89a83a0238674a12a9f967d3deca8dcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d15a6598dd534650b26e135c30361298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f35a590455a477aa59bd4b05a81fdb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ae3221379044bf68690e417e458ccdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d039d4b69ad04a9591cf97e0ee9cb116":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_557d89fa82a5402a82bcddcd6b0d32b5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5af88c273d0848348371af16fed485bb","IPY_MODEL_3b81df756b594035afe1e3159c6fc561"]}},"557d89fa82a5402a82bcddcd6b0d32b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5af88c273d0848348371af16fed485bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ff33fabb4ebf405c9142fce866409c50","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":651477729,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":651477729,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9c54d97adc9c426282524e3ff4cb700a"}},"3b81df756b594035afe1e3159c6fc561":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a924d171532d446fad7cac9b84d8b33e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 651M/651M [00:43&lt;00:00, 15.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5df9ddc595164814b27a9688088602ad"}},"ff33fabb4ebf405c9142fce866409c50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9c54d97adc9c426282524e3ff4cb700a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a924d171532d446fad7cac9b84d8b33e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5df9ddc595164814b27a9688088602ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"j1LTPn7IjqTz"},"source":["Source:\r\n","\r\n","huggingface: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","\r\n","Tutorial:https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmPFvCbSqyZF","executionInfo":{"status":"ok","timestamp":1610526080979,"user_tz":-210,"elapsed":56864,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"d9cb0d71-4202-49ec-bb08-1ff3cd772e44"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRxC0Pz1qzKc","executionInfo":{"status":"ok","timestamp":1610526085180,"user_tz":-210,"elapsed":4190,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/sharif/FineTuning/ipython(guide)')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCRkKc3NcgkX","executionInfo":{"status":"ok","timestamp":1610526092424,"user_tz":-210,"elapsed":11424,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"9d1f7334-725d-4c75-8774-226a2bd93914"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 15.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 21.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=c962b9f9960c7a372dc84617d4b0bd8fd6bb01be3a265ea182d909352cf444a5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODc44DglgNjZ","executionInfo":{"status":"ok","timestamp":1610526096737,"user_tz":-210,"elapsed":15729,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"82cba2fb-08e7-4cb0-b1e0-f4f68ba31cf4"},"source":["!pip3 install sentencepiece"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 5.1MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJY_L2p9a0t0","executionInfo":{"status":"ok","timestamp":1610526096738,"user_tz":-210,"elapsed":15721,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"f47ed828-a6f7-4051-b14f-d3d505c2f309"},"source":["!git clone https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","GIT_LFS_SKIP_SMUDGE=1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["fatal: destination path 'bert-fa-base-uncased-clf-persiannews' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eg3Up037nThu","executionInfo":{"status":"ok","timestamp":1610526104338,"user_tz":-210,"elapsed":23318,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import numpy\r\n","import pandas\r\n","import re\r\n","from sklearn.preprocessing import MultiLabelBinarizer\r\n","from sklearn.model_selection import train_test_split\r\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,TFAutoModel,AutoModel\r\n","from transformers import BertConfig, BertTokenizer\r\n","from transformers import TFBertModel, TFBertForSequenceClassification\r\n","from transformers import glue_convert_examples_to_features, InputExample\r\n","from sklearn.metrics import classification_report"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ur9wv1ytrZu","executionInfo":{"status":"ok","timestamp":1610526104339,"user_tz":-210,"elapsed":23316,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# specify GPU\r\n","device = torch.device(\"cuda\")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xax4bHubzpMp"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"TJf6T40glV5g","executionInfo":{"status":"ok","timestamp":1610526115800,"user_tz":-210,"elapsed":4486,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["limit_number = 750\r\n","data = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv',index_col=0)\r\n","data = data.dropna().reset_index(drop=True)\r\n","X = data[\"body\"].values.tolist()\r\n","y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","labels = []\r\n","tag=[]\r\n","for item in y['tag']:\r\n","  labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","  tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","labels = list(set(labels))\r\n","mlb = MultiLabelBinarizer()\r\n","Y=mlb.fit_transform(tag)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH3jCKaZsEWo","executionInfo":{"status":"ok","timestamp":1610526115801,"user_tz":-210,"elapsed":1429,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"20b02489-4606-4581-e183-3ceb1bae8a3e"},"source":["X_train, X_test, y_train, y_test = train_test_split(X,Y , test_size=0.2)\r\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n","print('train: ', len(X_train) , '\\ntest: ', len(X_test) , '\\nval: ', len(X_val) ,\"\\ny_tain:\",len(y_train) )"],"execution_count":9,"outputs":[{"output_type":"stream","text":["train:  12896 \n","test:  4299 \n","val:  4299 \n","y_tain: 12896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vei6iu9atmyd","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["0039549159a145799a63baaae6264208","399d958be0f34cf6a8b592b4b8cde6bb","d45b051e98114d398ca7640fe54de69e","ed557b003fc74f9ebf043751fd33b7e4","8f1f7cc60e1241b19b0b1bc7075998a9","f27f2267d5f8420eb9a8ebf382e49749","c007587ed7024e22ae6ebeb9b52b413b","3efdd16b912c4cce8d61ab98d7bfe9f3","7b518759a42a47709d5b4e6a09ca33e5","47e0835a4c9a42b08a9aa302484a5aa2","b45ab22f7ddd4fbf824d949e069f75df","7582e505f5404c92917b3e424e68142f","49695672a251420e997543a63f70dc25","f9a1d6c6d855448d92fd956944806761","6f12313316324094b644436e3e370d26","6c8c66c5d1214881ad48053c762fe913","c229fffb69c34220a4e6ebf804143181","8ec01f6587b04da0a78040242d3bbbe7","33d2aa1827b54a8689d1bef81b505c5f","d19d6f39ccbf40e7be252155a4ffb76f","8d8f9cf894a743b9b05731534cc7957f","44097319a0bf472a84ebaadb3bf21fe9","5f0ec2dc0a5b465697a34b8901a2667e","faf07d8acf9a46c99fed35cc57d4a078","f5139ca21f934b27a184b1dbef254ec1","df17b89a3ab84c9c984b382bb9e47c47","805b369f488845ef82a88904604c9464","ac921dff17674dc9bd5ebc9b0555a1ad","89a83a0238674a12a9f967d3deca8dcd","d15a6598dd534650b26e135c30361298","4f35a590455a477aa59bd4b05a81fdb0","0ae3221379044bf68690e417e458ccdb"]},"executionInfo":{"status":"ok","timestamp":1610526126531,"user_tz":-210,"elapsed":8991,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"fa61f443-2c26-4c53-ca46-8054eb8151a3"},"source":["##we would load the tokenizer\r\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0039549159a145799a63baaae6264208","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1441.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b518759a42a47709d5b4e6a09ca33e5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1198122.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c229fffb69c34220a4e6ebf804143181","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5139ca21f934b27a184b1dbef254ec1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7wdU0zejDNq","executionInfo":{"status":"ok","timestamp":1610526126532,"user_tz":-210,"elapsed":7960,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"c63d0952-7ca9-4987-e80c-7d51bee959c8"},"source":["#example\r\n","text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\r\n","tokenized=tokenizer.tokenize(X_train[0])\r\n","input_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n","print(tokenized)\r\n","print(input_ids)\r\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['تصور', 'دست', 'درب', '##دن', 'داشتهباشیم', 'کابوس', 'کابوسی', 'نمیتوانیم', 'زمین', 'بلند', 'برویم', 'کار', 'ها', 'ساده', 'بدهیم', 'داستان', 'باور', 'نیک', 'ووی', '##یچ', '##یچ', 'nick', 'vu', '##ji', '##ci', '##c', 'دلیل', 'سندروم', 'تترا', 'املیا', 'دست', 'دنیا', 'دادهاست', 'سوال', 'اساسی', 'نیک', 'دلیل', 'مسله', 'خانه', 'نشین', 'تخت', 'انتظار', 'کمک', 'ماند', 'نیک', 'سال', '۱۹۸۲', 'استرالیا', 'دنیا', 'دلیل', 'امکانات', 'تصویربرداری', 'جنین', 'وضعیت', 'نیک', 'خبر', 'تلاش', 'کمک', 'مادرش', 'یاد', 'انگشتی', 'اندامی', 'پايین', 'تنه', 'اش', 'بنویسد', 'کار', 'ها', 'نوشیدن', 'لیوان', 'تایپ', 'پرتاب', 'توپ', 'بیس', 'بال', 'اموخت', 'دبیرستان', 'رهبر', 'گروه', 'موسسه', 'خیریه', 'اعانه', 'سالگی', 'موسسه', 'زندگی', 'دست', 'امید', 'افسرده', 'ناتوان', 'تاسیس', 'نیک', 'ووی', '##یچ', '##یچ', 'جوان', 'سالگی', 'دانشگاه', 'گریفیت', 'مدرک', 'لیسانس', 'رشته', 'ها', 'حسابداری', 'برنامه', 'مالی', 'فارغ', 'التحصیل', 'گردید', 'ادامه', 'فعالیت', 'ها', 'نهایت', 'سخنرانی', 'هایش', 'میلیون', 'ها', 'شنونده', 'جهان', 'امید', 'زندگی', 'انگیزه', 'ارمغان', 'می', 'اورد', 'نیک', 'معلولیت', '##ش', 'شرایط', 'خاصی', 'سرمشق', 'کارافرینان', 'کارافرینانی', 'ایده', 'ها', 'ذهن', 'پویایی', 'امکانات', 'شرایط', 'گمان', 'نکات', 'درس', 'مرد', 'مهار', '##نشدنی', 'اموخت', 'نگاهی', 'زندگی', 'نیک', 'بیندازیم', 'میبینیم', 'قبول', 'ترین', 'بهانه', 'ها', 'دنیا', 'دلیل', 'ها', 'عقل', 'پسند', 'دست', 'دست', 'گذاشتن', 'اختیار', 'داشتهاست', 'خلاف', 'انتظار', 'میرفت', 'ننشست', 'انداختن', 'بنیادی', 'زندگی', 'انتخاب', 'مسیر', 'اشتیاق', 'فعالیت', 'ها', 'دران', 'مسیر', 'دانش', 'اشتیاق', 'بشناسید', 'کمک', 'کسب', 'کار', 'بسازید', 'زندگی', 'کارافرین', 'سرشار', 'سختی', 'سختی', 'کار', 'شکست', 'ها', 'متعدد', 'شنیدن', 'ها', 'مشتریان', 'سرمایه', 'گذاران', 'شرکا', 'غیره', 'موفق', 'مروارید', 'اعماق', 'مشکلات', 'سختی', 'ها', 'کسب', 'کار', 'ها', 'موفق', 'امروزی', 'مسیری', 'طولانی', 'موقعیت', 'امروزش', '##ان', 'پیموده', 'اند', 'نیک', 'کتابش', 'زندگی', 'حدو', 'مرز', 'نوشتهاست', 'سوالی', 'زندگی', 'مدت', 'ها', 'امیدی', 'انتظار', 'جوانه', 'دست', 'روزی', 'پرسیدم', 'دلیل', 'افرینش', 'سوال', 'اساسی', 'مسیر', 'کسب', 'کار', 'بپرسیم', 'هدف', 'کارافرین', 'کسب', 'کار', 'قرار', 'ارزشی', 'جامعه', 'بدهیم', 'نیک', 'شرایطی', 'استثنایی', 'شرایطی', 'میتوانست', 'برای', '##او', 'نقطه', 'ضعفی', 'نقاط', 'ضعف', 'فرصت', 'شرایط', 'خاصش', 'برندی', 'ساخت', 'ذهن', 'پاک', 'مسايل', 'فرصت', 'خلق', 'رمز', 'موفقیت', 'شکست', 'ناکامی', 'احساسی', 'مسیر', 'موفقیت', 'سراغ', 'خواهند', '##امد', 'تجربه', 'بیل', 'گیتس', 'جشن', 'پیروزی', 'مهمتر', 'درس', 'شکست', 'هاست', 'شکست', 'ها', 'متعدد', 'نیک', 'مسیر', 'هایش', 'پیروزی', 'نکته', 'گوشزد', 'کارافرینی', 'نقاط', 'زندگی', 'شکست', 'خواهد', '##رسید', 'اتفاق', 'می', 'افتد', 'نیک', 'میتوانست', 'شکست', 'ادامه', 'مسیر', 'مایوس', 'معلول', 'شکست', 'هایش', 'درس', 'الهام', 'بخ', '##ض', 'میلیون', 'ها', 'نفر', 'جهان', 'تولید', 'گروه', 'اموزشی', 'دانا', '##لو', '##هر', '##گونه', 'کپی', 'مطلب', 'ذکر', 'لینک', 'منبع', 'مجاز']\n","[5585, 2910, 10023, 3483, 63432, 18339, 53239, 9990, 3164, 4384, 10390, 2867, 5929, 4613, 9785, 4059, 4686, 7513, 31705, 88136, 88136, 64284, 98637, 23810, 25223, 2036, 3310, 16780, 29464, 54505, 2910, 3810, 7253, 4639, 4664, 7513, 3310, 74407, 3764, 11193, 5846, 4378, 3469, 4457, 7513, 2844, 11918, 6866, 3810, 3310, 5524, 12912, 9366, 3984, 7513, 3350, 3743, 3469, 7091, 3494, 35623, 29871, 13561, 15285, 3046, 17939, 2867, 5929, 11185, 13522, 13113, 7524, 6010, 9847, 3090, 9704, 7507, 5375, 3434, 5069, 11590, 45009, 5278, 5069, 3351, 2910, 4222, 16731, 14012, 4400, 7513, 31705, 88136, 88136, 3965, 5278, 3363, 69237, 7072, 10140, 4336, 5929, 12872, 3329, 3491, 6635, 12954, 4403, 3251, 3426, 5929, 4459, 5997, 8278, 3399, 5929, 18952, 3381, 4222, 3351, 7578, 10034, 2793, 3922, 7513, 24652, 2014, 3517, 4912, 31112, 14908, 63640, 5676, 5929, 5292, 16517, 5524, 3517, 8656, 7580, 5969, 2999, 8311, 32405, 9704, 7223, 3351, 7513, 18906, 8700, 5671, 15407, 8925, 5929, 3810, 3310, 5929, 7526, 13101, 2910, 2910, 8769, 4209, 6022, 6163, 4378, 6261, 51991, 11856, 10100, 3351, 3229, 3910, 12967, 3426, 5929, 17714, 3910, 3100, 12967, 15230, 3469, 3927, 2867, 17358, 3351, 9950, 8640, 6192, 6192, 2867, 4110, 5929, 6747, 9457, 5929, 5781, 3497, 7961, 27549, 7330, 3598, 14817, 16135, 3966, 6192, 5929, 3927, 2867, 5929, 3598, 7029, 9264, 4977, 4872, 87198, 2783, 30988, 3145, 7513, 16273, 3351, 56648, 4822, 12140, 10322, 3351, 3679, 5929, 16913, 4378, 17206, 2910, 6367, 19516, 3310, 13924, 4639, 4664, 3910, 3927, 2867, 27480, 3736, 9950, 3927, 2867, 2959, 10151, 3780, 9785, 7513, 5799, 14795, 5799, 7558, 2831, 5007, 4832, 30945, 5093, 6542, 5175, 3517, 44161, 26169, 3043, 5292, 4679, 4605, 5175, 5752, 5882, 4710, 4110, 14852, 10469, 3910, 4710, 6757, 4564, 3463, 4246, 7582, 17958, 4262, 4965, 8661, 5969, 4110, 8097, 4110, 5929, 6747, 7513, 3910, 8278, 4965, 4909, 19627, 11370, 5093, 3351, 4110, 3201, 68858, 3929, 2793, 26592, 7513, 7558, 4110, 3251, 3910, 23575, 16263, 4110, 8278, 5969, 7663, 2968, 2019, 3399, 5929, 3432, 3381, 3114, 3434, 5254, 15748, 3448, 2872, 3847, 9546, 5071, 5001, 11827, 5392, 5660]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az4rwU0l5ECn","executionInfo":{"status":"ok","timestamp":1610526126532,"user_tz":-210,"elapsed":6930,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# encode text\r\n","sent_id = tokenizer.batch_encode_plus(X_train[:10], padding=True, return_token_type_ids=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNRU-SH65ZEE","executionInfo":{"status":"ok","timestamp":1610526126533,"user_tz":-210,"elapsed":4187,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"394b12ee-2c6a-45fe-b035-8b75086536a5"},"source":["sent_id"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 5585, 2910, 10023, 3483, 63432, 18339, 53239, 9990, 3164, 4384, 10390, 2867, 5929, 4613, 9785, 4059, 4686, 7513, 31705, 88136, 88136, 64284, 98637, 23810, 25223, 2036, 3310, 16780, 29464, 54505, 2910, 3810, 7253, 4639, 4664, 7513, 3310, 74407, 3764, 11193, 5846, 4378, 3469, 4457, 7513, 2844, 11918, 6866, 3810, 3310, 5524, 12912, 9366, 3984, 7513, 3350, 3743, 3469, 7091, 3494, 35623, 29871, 13561, 15285, 3046, 17939, 2867, 5929, 11185, 13522, 13113, 7524, 6010, 9847, 3090, 9704, 7507, 5375, 3434, 5069, 11590, 45009, 5278, 5069, 3351, 2910, 4222, 16731, 14012, 4400, 7513, 31705, 88136, 88136, 3965, 5278, 3363, 69237, 7072, 10140, 4336, 5929, 12872, 3329, 3491, 6635, 12954, 4403, 3251, 3426, 5929, 4459, 5997, 8278, 3399, 5929, 18952, 3381, 4222, 3351, 7578, 10034, 2793, 3922, 7513, 24652, 2014, 3517, 4912, 31112, 14908, 63640, 5676, 5929, 5292, 16517, 5524, 3517, 8656, 7580, 5969, 2999, 8311, 32405, 9704, 7223, 3351, 7513, 18906, 8700, 5671, 15407, 8925, 5929, 3810, 3310, 5929, 7526, 13101, 2910, 2910, 8769, 4209, 6022, 6163, 4378, 6261, 51991, 11856, 10100, 3351, 3229, 3910, 12967, 3426, 5929, 17714, 3910, 3100, 12967, 15230, 3469, 3927, 2867, 17358, 3351, 9950, 8640, 6192, 6192, 2867, 4110, 5929, 6747, 9457, 5929, 5781, 3497, 7961, 27549, 7330, 3598, 14817, 16135, 3966, 6192, 5929, 3927, 2867, 5929, 3598, 7029, 9264, 4977, 4872, 87198, 2783, 30988, 3145, 7513, 16273, 3351, 56648, 4822, 12140, 10322, 3351, 3679, 5929, 16913, 4378, 17206, 2910, 6367, 19516, 3310, 13924, 4639, 4664, 3910, 3927, 2867, 27480, 3736, 9950, 3927, 2867, 2959, 10151, 3780, 9785, 7513, 5799, 14795, 5799, 7558, 2831, 5007, 4832, 30945, 5093, 6542, 5175, 3517, 44161, 26169, 3043, 5292, 4679, 4605, 5175, 5752, 5882, 4710, 4110, 14852, 10469, 3910, 4710, 6757, 4564, 3463, 4246, 7582, 17958, 4262, 4965, 8661, 5969, 4110, 8097, 4110, 5929, 6747, 7513, 3910, 8278, 4965, 4909, 19627, 11370, 5093, 3351, 4110, 3201, 68858, 3929, 2793, 26592, 7513, 7558, 4110, 3251, 3910, 23575, 16263, 4110, 8278, 5969, 7663, 2968, 2019, 3399, 5929, 3432, 3381, 3114, 3434, 5254, 15748, 3448, 2872, 3847, 9546, 5071, 5001, 11827, 5392, 5660, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6012, 1442, 5402, 6049, 3640, 15619, 20936, 7078, 36711, 21659, 14114, 12271, 17690, 6188, 3083, 3640, 13089, 4321, 15619, 20936, 45611, 74831, 15619, 20936, 3287, 5166, 3494, 20867, 5292, 6049, 6707, 12139, 17690, 6188, 6492, 15799, 3916, 28724, 5166, 2008, 15619, 20936, 49722, 28889, 21659, 17420, 4484, 45147, 2078, 2032, 21659, 15619, 20936, 28724, 36539, 14114, 30498, 32399, 34738, 48212, 12702, 21659, 15619, 20936, 3893, 70989, 21368, 4639, 29969, 15619, 20936, 2867, 12702, 8569, 15619, 20936, 2867, 12702, 4613, 45791, 3712, 11231, 28724, 7494, 3981, 5605, 12702, 71030, 3778, 5929, 3966, 3629, 3171, 5929, 31366, 2793, 6789, 2011, 3171, 5929, 66195, 2816, 45791, 73811, 5522, 13809, 22378, 2011, 15619, 20936, 4664, 3630, 18591, 9613, 15619, 20936, 35284, 35284, 76662, 4613, 35284, 5929, 10634, 45791, 3791, 5755, 35284, 23586, 3171, 5929, 4241, 3088, 3630, 3171, 5929, 4241, 7290, 35284, 92548, 4183, 4321, 62169, 7273, 32285, 76662, 2959, 2793, 9933, 8145, 3329, 12035, 9225, 3171, 5929, 10934, 76662, 14477, 35284, 2959, 26040, 98302, 2793, 3232, 23586, 8755, 35284, 4206, 19994, 47778, 9705, 4196, 16224, 3850, 39230, 31487, 17420, 17802, 4183, 62169, 76662, 3499, 39230, 6945, 14474, 3114, 19791, 67519, 31762, 19994, 4832, 1442, 12822, 4568, 4226, 51250, 34061, 12139, 28724, 7494, 93141, 2003, 15766, 15077, 12139, 8029, 93141, 6110, 24121, 12139, 51250, 34061, 41137, 3814, 76662, 6597, 7473, 4863, 6921, 35088, 5556, 98122, 2025, 5929, 9706, 4909, 5929, 28724, 7158, 2818, 35284, 5641, 28724, 2793, 35150, 17165, 16157, 23116, 98122, 2025, 63432, 35340, 70182, 5806, 54936, 2025, 5536, 5929, 12175, 12555, 45791, 5717, 11231, 4716, 28724, 7494, 5605, 14114, 4691, 26702, 59538, 11426, 2807, 35284, 15766, 17420, 98122, 2025, 3114, 45791, 5605, 4825, 12139, 45791, 5732, 76662, 4613, 38984, 18009, 3792, 4825, 11231, 6749, 45791, 3043, 45791, 54770, 3499, 5556, 98122, 2025, 32357, 18009, 64198, 92957, 15619, 20936, 45791, 32357, 18009, 15988, 82418, 6201, 45791, 23116, 31790, 26040, 52810, 2030, 5929, 52810, 2030, 4241, 4244, 5936, 2008, 7494, 35284, 15988, 82418, 6201, 12702, 45791, 5717, 11231, 28724, 5503, 45791, 5605, 35284, 98122, 2025, 52446, 34340, 54936, 2025, 12555, 98122, 2025, 6079, 3083, 4459, 28724, 3114, 10672, 45791, 5605, 10672, 5605, 71030, 3778, 45791, 8953, 21741, 13630, 36332, 3647, 4995, 54729, 28228, 22618, 19791, 4639, 6173, 56205, 9854, 51719, 35284, 15619, 20936, 12702, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 5875, 3057, 5728, 2871, 5154, 3057, 4526, 6075, 4526, 3873, 2959, 10023, 4526, 5929, 4341, 5823, 4526, 5717, 1442, 3057, 15352, 3731, 3873, 8285, 12126, 3873, 11546, 3873, 8285, 4980, 3972, 4942, 3905, 9937, 11546, 3384, 3873, 4526, 16826, 2959, 8285, 2867, 3598, 2793, 9585, 2013, 3731, 4980, 4980, 12633, 9076, 3873, 4980, 6191, 7182, 32491, 6133, 3003, 4234, 8171, 8034, 17038, 4084, 16883, 3768, 1442, 4428, 7254, 17038, 3777, 7111, 17472, 16410, 16410, 4274, 6730, 3873, 5929, 5000, 3945, 7182, 17038, 6872, 6025, 1442, 5484, 20938, 17038, 6872, 19568, 4428, 14853, 6412, 24324, 4908, 3777, 17472, 8718, 7000, 16410, 3310, 17038, 13207, 3057, 3057, 9523, 6191, 3841, 8285, 2938, 4526, 2959, 4639, 9937, 4980, 6191, 17038, 4316, 43601, 6191, 5929, 3378, 5120, 8849, 6191, 5839, 11440, 3873, 7152, 10885, 4980, 6185, 6435, 5842, 4108, 3743, 6872, 3905, 6191, 17038, 6702, 5929, 3665, 1442, 3841, 4712, 5929, 12518, 12989, 4656, 5929, 13140, 4942, 3873, 4909, 1442, 3211, 3743, 6872, 4712, 5929, 4980, 3620, 3088, 3666, 6865, 3088, 4760, 3561, 3003, 5869, 44448, 5676, 28115, 4639, 4712, 6872, 3841, 10777, 11220, 4992, 5929, 7966, 3873, 5360, 4712, 4206, 5929, 4461, 3698, 5929, 9256, 4992, 5054, 12518, 34406, 6240, 4629, 5653, 3114, 4712, 6177, 2867, 5929, 6730, 3873, 5935, 7844, 4909, 3337, 3088, 3917, 44448, 5585, 3419, 12518, 3841, 3648, 7522, 5929, 16133, 10716, 2844, 10726, 10557, 3280, 12278, 13950, 3280, 3419, 12518, 6347, 5929, 5020, 5360, 4234, 14000, 3819, 8056, 91851, 5719, 61593, 11454, 3873, 4712, 3692, 7276, 4461, 60826, 16273, 10281, 24944, 5292, 3865, 5984, 61593, 11454, 4804, 5101, 19987, 5395, 25270, 8521, 4084, 11502, 3280, 4274, 5929, 22726, 16410, 6508, 6435, 5676, 5527, 10121, 60826, 5664, 2904, 3073, 13316, 9593, 3499, 5120, 4428, 7254, 4430, 8849, 16410, 4804, 7775, 3521, 16410, 5232, 6435, 6762, 5929, 92550, 6762, 12989, 3517, 13973, 3873, 4712, 5395, 5929, 27189, 5676, 5929, 11422, 5676, 5929, 3225, 3145, 5676, 5001, 2871, 3145, 4747, 5629, 4708, 3145, 4454, 4909, 5289, 5974, 4458, 19369, 6690, 13425, 22208, 3469, 4528, 3499, 5839, 5120, 3561, 3743, 5929, 4482, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3250, 6388, 8246, 2793, 74489, 9785, 6388, 3250, 1394, 2793, 74489, 3873, 5929, 4600, 2938, 4386, 9713, 5842, 20343, 57312, 6388, 6012, 3250, 15870, 3777, 3250, 5929, 2793, 4403, 6012, 66016, 3250, 5929, 4196, 4294, 1394, 3250, 4007, 3351, 3469, 6012, 2793, 15870, 2013, 6597, 6794, 1394, 14013, 12355, 6120, 3310, 19562, 3821, 25408, 6540, 3764, 21995, 1394, 32373, 6012, 2938, 26283, 3250, 5929, 6491, 2822, 26283, 6012, 66016, 3250, 5929, 3852, 3027, 4294, 7310, 2793, 21224, 7934, 3250, 5929, 3250, 5929, 14310, 3250, 5929, 8739, 6050, 1394, 3250, 5929, 3250, 5929, 6757, 6388, 6012, 10390, 6012, 4907, 3250, 5929, 3287, 12275, 3250, 5929, 3494, 2793, 43384, 3951, 5929, 5170, 14310, 4196, 4294, 6388, 6012, 5891, 7765, 56971, 3250, 5929, 5546, 3351, 3598, 3250, 8204, 14854, 3250, 3444, 4386, 3250, 5486, 3152, 1394, 3250, 7551, 6347, 5929, 2867, 5929, 4613, 4820, 1394, 2867, 5929, 2844, 3680, 5929, 3250, 16650, 5473, 2904, 3073, 4863, 3250, 5546, 3559, 2844, 3250, 5870, 4863, 2938, 6491, 5154, 4863, 6388, 3559, 2844, 26825, 4241, 3318, 4360, 3777, 4710, 3250, 4893, 4224, 8204, 14854, 4982, 2007, 2867, 5929, 6707, 3055, 4477, 9654, 5929, 10706, 57264, 2849, 23661, 4164, 3990, 3116, 61534, 6506, 17332, 7449, 3146, 75518, 11184, 4710, 3060, 1394, 5817, 3296, 3145, 18796, 6755, 9762, 7511, 5929, 5891, 6329, 33113, 3271, 7511, 5929, 3538, 3145, 4589, 8261, 30650, 4383, 16175, 12236, 6863, 7016, 51981, 6371, 4781, 49762, 2793, 3201, 12011, 6905, 13773, 3821, 4457, 34231, 18824, 9579, 20824, 5137, 3865, 12160, 4872, 5001, 5416, 5929, 14474, 2959, 3296, 5154, 16580, 5929, 6500, 9239, 9351, 13767, 10175, 4465, 4466, 5929, 3598, 9239, 5884, 6684, 4643, 7195, 7560, 3211, 3250, 6282, 5997, 5785, 16651, 5997, 77066, 3777, 4710, 3250, 3768, 5546, 3250, 6282, 5997, 3250, 5929, 27841, 3768, 4196, 10161, 27806, 3775, 4398, 22833, 3768, 4188, 4945, 2008, 1442, 8769, 5220, 5997, 74332, 1442, 5997, 71421, 3972, 5929, 5997, 24214, 3478, 2793, 13101, 2790, 48494, 5929, 5997, 15746, 3661, 4992, 3280, 3490, 4868, 5977, 85970, 1394, 29415, 5590, 3598, 54762, 2790, 87042, 5997, 2793, 45612, 4205, 1442, 2867, 9570, 5154, 1394, 4175, 5997, 6310, 6310, 4347, 5997, 4124, 1394, 4318, 3905, 5929, 15352, 7658, 5292, 25046, 25658, 78569, 18850, 4007, 6755, 4912, 8970, 4402, 12275, 4386, 15898, 5584, 70122, 3743, 31199, 11912, 5673, 4183, 4386, 5997, 5929, 4298, 7560, 6011, 5392, 24954, 29110, 20249, 2031, 27029, 9465, 8596, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6012, 3043, 7494, 12646, 74701, 3841, 6650, 3986, 6603, 24263, 6937, 3893, 28529, 16309, 28938, 97925, 22641, 5369, 5690, 2881, 4790, 30498, 3280, 23180, 4298, 5863, 2938, 2930, 12646, 3893, 7494, 6650, 10763, 2930, 3528, 8332, 5929, 4603, 38267, 32538, 2061, 2959, 2793, 5994, 12614, 3531, 3528, 22633, 20290, 70158, 3885, 55455, 18199, 15180, 51005, 7051, 2867, 4090, 5929, 8705, 14688, 6650, 3280, 2867, 52454, 97925, 22641, 28529, 16309, 28938, 5166, 2793, 74489, 3733, 4484, 2793, 38522, 9258, 3475, 3500, 2793, 74489, 6650, 6958, 11999, 9706, 6650, 66254, 13394, 4366, 5929, 4679, 6921, 62321, 70677, 15425, 63851, 20021, 4863, 3376, 7494, 6312, 28531, 38515, 60193, 2959, 4679, 6364, 2843, 97925, 35723, 73868, 5520, 6728, 7440, 10763, 71030, 3778, 20195, 3647, 71030, 3778, 28531, 4884, 42712, 3005, 5929, 6364, 5929, 6921, 4884, 7223, 6650, 2793, 6501, 2015, 7494, 23852, 5211, 14055, 38188, 2059, 23180, 4790, 4832, 15347, 2930, 5929, 34042, 57430, 67909, 15307, 4298, 5929, 2930, 5929, 3280, 4905, 5929, 31353, 29068, 3469, 2793, 33946, 23180, 5170, 2793, 31149, 2793, 5655, 89395, 6492, 4905, 5929, 4484, 5321, 3813, 5596, 31353, 29068, 5596, 9258, 10357, 24215, 31353, 29068, 6921, 44269, 15425, 63851, 20021, 42712, 3005, 6312, 28531, 38515, 60193, 4905, 4832, 15347, 23180, 3625, 41989, 7473, 7796, 13597, 22848, 2793, 62921, 3171, 5929, 14532, 67909, 4454, 3171, 5929, 4209, 13809, 3171, 5929, 6986, 13809, 98026, 2078, 20804, 2867, 11748, 4484, 3500, 5363, 98026, 2078, 20804, 4484, 4241, 5295, 31683, 33911, 19327, 98026, 2078, 20804, 5363, 98026, 2078, 20804, 6650, 4241, 46227, 6921, 17989, 34567, 7120, 30303, 14878, 15180, 16309, 70473, 11332, 54934, 15526, 7051, 10445, 14991, 4241, 15490, 3088, 6446, 98026, 2078, 20804, 3280, 3027, 4294, 7494, 4241, 6650, 7332, 3726, 4828, 6839, 37745, 4241, 3027, 5154, 3726, 2959, 2793, 74489, 5295, 7494, 5035, 9706, 6012, 3043, 7494, 12646, 74701, 5369, 7494, 8673, 8332, 5929, 3788, 32990, 3810, 3625, 2793, 31149, 17915, 5929, 7775, 5929, 7494, 4241, 12118, 3211, 2959, 3296, 10061, 5071, 5850, 4188, 3911, 5929, 5358, 4360, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2844, 5929, 6958, 4247, 3864, 5929, 3614, 2927, 4059, 5929, 6518, 5929, 7253, 2844, 3363, 13615, 5991, 5929, 4863, 1442, 16646, 12499, 4972, 5929, 29774, 3285, 4655, 5929, 25293, 41413, 17802, 4907, 5929, 15003, 2927, 26851, 3567, 5929, 7594, 11545, 8209, 4189, 4655, 25293, 43641, 2011, 92803, 2011, 43641, 2011, 13200, 3286, 50096, 2008, 11000, 28228, 5423, 4341, 16646, 12499, 6283, 3864, 3614, 10968, 10968, 3475, 10319, 10968, 8942, 4655, 5929, 10063, 6908, 3929, 4613, 3351, 3432, 22367, 3791, 30494, 22329, 17133, 10663, 4655, 5991, 1442, 10231, 14786, 1442, 61685, 5323, 3290, 12386, 30494, 7533, 3290, 13752, 22329, 22329, 6641, 8673, 10672, 21962, 5929, 4387, 9172, 24117, 52905, 3508, 5929, 4992, 95552, 34840, 10968, 3864, 3614, 13589, 3322, 2008, 6111, 15407, 4157, 3598, 3088, 9172, 3444, 25379, 4909, 22367, 3083, 4218, 1442, 3567, 5929, 4997, 3378, 4589, 4528, 4049, 5427, 3300, 11123, 3490, 8081, 30494, 5594, 3318, 18757, 3864, 5929, 5427, 3911, 28228, 13654, 2991, 23881, 2022, 9825, 4244, 13344, 35807, 5929, 30494, 5929, 13505, 2011, 5929, 2867, 5799, 45560, 8237, 2784, 5754, 5138, 2011, 8408, 3864, 5929, 3614, 2927, 6471, 3404, 86979, 2011, 19791, 38437, 26410, 57738, 4893, 19791, 3919, 65603, 3777, 66707, 38437, 3525, 5929, 18153, 2011, 7563, 4476, 7796, 9172, 4476, 7796, 10968, 4476, 5991, 19611, 3567, 4476, 25379, 8237, 74518, 4031, 66550, 2005, 15924, 4244, 2867, 89723, 4007, 12139, 62472, 3864, 3614, 49547, 3777, 19791, 63217, 49547, 9919, 2011, 7863, 5272, 31366, 62472, 13344, 3490, 2784, 8196, 51911, 2816, 61039, 4999, 2011, 5983, 17687, 17530, 2011, 15477, 9719, 63895, 17521, 61707, 3229, 19791, 24663, 2011, 25379, 8237, 15559, 19880, 37157, 9955, 19791, 26290, 2014, 17420, 3864, 1442, 3614, 62472, 36857, 4031, 2006, 5351, 53518, 2005, 3499, 31957, 4628, 15407, 4157, 35082, 5754, 2793, 22827, 3567, 5929, 6468, 43641, 2011, 3557, 2011, 2844, 5929, 2844, 53270, 2816, 6468, 43160, 2909, 10672, 3557, 2011, 53270, 2816, 2959, 21981, 4244, 51676, 4997, 10290, 17420, 7078, 4766, 5929, 20867, 12914, 4541, 9345, 3864, 5929, 3614, 2787, 5754, 2793, 2789, 4766, 3864, 1442, 3614, 14320, 3780, 20516, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 22504, 29125, 22504, 3919, 2867, 22504, 4278, 5673, 3525, 5929, 21852, 8545, 4294, 2793, 82554, 2015, 13719, 22504, 5224, 4219, 16047, 4454, 5495, 11987, 22614, 5929, 22504, 11987, 22614, 5929, 10591, 3661, 22504, 11987, 22614, 66010, 14714, 4106, 7679, 8800, 6640, 4296, 3439, 11987, 22614, 5929, 2867, 3475, 11176, 4321, 3329, 12035, 5891, 2793, 95593, 22504, 3475, 4565, 11987, 22614, 5929, 15725, 2867, 11987, 22614, 5929, 12579, 27542, 3469, 4472, 15407, 3329, 5929, 15561, 17358, 3287, 5209, 6602, 6516, 12579, 27542, 23098, 11987, 22614, 12579, 27542, 30598, 5561, 7443, 6012, 31707, 3640, 12579, 27542, 4463, 4639, 3905, 9785, 12579, 27542, 8408, 11697, 5292, 5520, 8751, 4788, 3473, 12579, 27542, 11987, 22614, 4523, 3419, 22504, 3841, 77066, 4226, 4411, 3088, 4730, 2004, 7716, 11987, 22614, 4766, 5929, 6492, 5929, 6085, 11987, 22614, 5929, 4730, 2004, 5875, 3821, 25408, 38702, 2867, 4772, 5929, 22484, 2867, 6757, 6492, 5929, 38702, 5929, 3731, 6337, 4730, 2004, 4259, 2844, 6465, 42403, 46407, 2867, 19382, 3943, 64777, 66228, 77066, 4730, 2004, 6260, 11987, 22614, 50816, 25317, 4196, 3666, 6168, 23950, 9922, 6754, 12060, 3945, 89150, 5399, 3043, 7494, 4089, 5565, 3229, 63432, 3473, 11987, 22614, 5093, 6542, 9622, 5093, 9622, 4730, 2004, 3329, 17012, 10955, 34052, 12579, 27542, 6629, 12579, 27542, 4484, 6921, 22504, 17358, 2959, 12579, 27542, 39293, 5929, 4484, 4241, 39293, 32961, 24546, 4241, 3473, 3329, 4730, 2004, 4723, 19690, 18101, 3280, 4875, 4905, 5929, 9087, 38305, 37863, 39293, 3629, 6195, 33103, 3376, 52446, 6312, 15766, 8953, 90320, 40440, 4568, 3027, 4294, 5988, 4905, 4863, 25643, 9258, 6312, 6937, 15766, 4183, 26096, 3473, 9169, 5929, 11987, 22614, 3789, 2793, 5506, 2015, 11987, 22614, 2867, 4613, 4366, 5929, 3680, 4484, 4241, 9636, 3329, 4090, 9258, 10865, 3329, 6029, 3171, 57637, 7317, 14446, 35428, 17389, 4863, 4482, 5655, 3328, 11398, 2793, 56608, 3329, 12579, 27542, 3841, 5071, 3640, 7716, 11987, 22614, 12579, 27542, 7164, 8009, 4484, 5929, 14532, 2871, 3145, 4386, 5812, 14446, 4972, 4387, 4285, 3810, 4863, 3528, 4237, 4730, 2004, 47388, 5524, 4517, 2938, 4394, 22439, 2959, 7716, 11987, 22614, 6378, 3841, 4394, 22504, 6506, 9678, 11987, 22614, 5929, 7716, 11987, 22614, 2867, 3027, 5154, 9472, 2793, 8096, 9713, 4816, 9553, 59765, 6468, 7171, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2844, 6520, 4394, 13187, 54995, 3448, 3595, 3813, 10284, 3768, 3329, 12035, 3125, 6180, 2844, 5797, 9691, 16988, 6504, 7834, 5206, 4032, 3692, 10284, 15352, 5526, 5929, 5351, 15057, 39298, 17338, 3329, 17012, 3475, 11176, 3952, 2867, 3419, 5929, 3329, 12035, 7330, 17583, 2904, 3073, 3736, 3376, 2867, 4475, 3351, 3475, 11176, 3625, 5823, 74944, 4824, 4908, 2867, 6012, 4790, 4300, 5206, 6468, 3251, 11847, 8141, 3287, 5272, 3475, 11176, 3567, 10284, 2844, 5797, 13187, 54995, 3448, 8091, 25699, 2056, 3271, 4188, 5206, 10284, 43077, 17729, 3475, 5839, 34408, 2930, 3810, 2910, 7168, 58862, 3052, 24376, 63248, 3475, 5839, 3475, 11176, 7494, 5929, 19209, 45613, 3475, 11176, 6545, 5039, 5929, 2959, 6754, 4686, 8937, 3805, 5929, 3475, 11176, 10153, 2938, 25750, 4908, 4366, 6483, 3225, 3145, 3475, 11176, 10153, 2938, 11160, 2844, 2793, 60311, 3211, 63432, 10284, 18207, 21184, 6145, 5929, 2844, 4246, 39298, 6340, 5161, 7331, 4533, 7253, 4226, 4222, 3329, 17012, 2867, 8473, 3179, 2867, 4909, 5484, 3475, 11176, 6545, 4864, 3475, 11176, 44821, 3521, 3329, 2959, 86365, 81770, 2994, 86365, 2844, 5797, 70192, 3490, 3952, 5929, 39298, 5574, 7043, 36108, 32088, 40923, 29257, 27864, 3475, 11176, 19209, 3229, 24711, 2050, 40302, 44821, 5929, 9625, 3810, 6360, 3329, 17012, 3419, 65658, 12391, 5161, 4014, 58132, 2058, 3700, 2844, 8357, 9227, 44418, 4014, 6360, 3329, 4373, 32558, 3700, 2844, 3490, 3952, 2867, 9913, 3329, 17012, 3052, 5929, 5574, 11378, 4908, 5673, 12835, 5929, 3927, 16069, 5929, 18587, 4908, 14736, 22748, 42003, 43455, 96235, 5672, 42276, 17852, 3700, 52166, 2058, 3700, 2844, 3810, 5161, 4072, 4533, 3171, 3145, 3475, 11176, 48789, 5584, 4072, 43850, 3821, 9933, 76008, 4731, 5929, 3475, 5574, 7043, 69457, 2858, 51462, 55096, 38261, 5161, 9275, 3329, 17012, 3490, 7275, 3329, 17012, 2793, 41169, 55090, 4020, 4643, 33857, 7145, 3952, 14358, 3680, 4032, 3347, 3639, 3475, 5574, 7043, 4908, 2867, 3171, 10061, 17937, 4893, 5206, 4032, 11827, 5563, 4138, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 37620, 4884, 10755, 9258, 5929, 19911, 6596, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 86365, 6492, 3475, 60649, 74701, 5224, 15407, 2988, 15407, 6492, 5929, 11987, 22614, 3475, 60649, 3852, 6308, 74701, 6492, 4175, 44418, 3475, 60649, 66580, 10968, 3625, 10777, 71030, 3778, 5929, 3983, 7273, 4589, 3329, 5929, 17358, 6492, 3469, 4366, 5929, 23460, 44418, 29109, 5427, 2867, 4394, 4259, 74701, 2078, 2032, 24982, 74701, 4394, 72768, 45356, 2049, 2793, 31149, 11827, 4825, 4907, 74701, 8545, 5154, 6378, 2793, 31149, 6012, 4394, 15165, 4893, 4175, 3310, 3475, 60649, 6506, 74701, 3337, 15407, 5495, 3229, 6492, 3310, 6506, 2867, 6492, 3821, 5355, 2793, 31149, 4089, 30529, 3280, 4370, 5929, 3595, 5526, 6506, 21848, 5655, 3328, 4485, 4105, 5318, 3873, 20425, 8246, 6506, 9089, 6506, 74701, 3310, 3511, 5929, 5254, 3310, 6492, 11931, 23157, 26604, 4766, 6325, 12958, 6506, 4246, 2867, 44418, 10153, 2938, 70122, 4613, 3088, 32717, 2793, 31149, 4246, 7273, 30129, 3562, 8473, 3280, 4589, 4863, 7273, 5574, 7043, 4863, 7273, 3841, 5470, 87331, 8718, 8606, 10421, 6206, 3488, 4816, 92550, 4816, 4244, 4863, 7273, 3841, 6857, 6964, 9861, 4626, 2787, 5574, 7043, 4863, 7273, 19626, 25643, 4816, 5574, 4175, 33592, 3495, 5929, 3381, 3541, 5929, 6769, 3469, 7164, 3088, 13343, 5929, 3911, 5929, 5442, 6552, 14215, 24540, 4650, 3911, 5929, 7164, 8941, 8751, 6865, 4997, 5929, 57471, 24540, 6597, 6012, 6492, 67382, 5272, 16053, 20805, 2044, 5775, 72768, 45356, 2049, 22949, 15839, 5934, 5531, 5723, 3680, 8457, 4226, 74701, 84378, 40908, 2041, 3911, 5929, 8941, 4997, 5929, 57471, 5033, 14402, 3469, 33592, 3495, 74701, 7106, 3810, 3332, 5929, 5802, 4913, 3469, 3905, 4639, 5929, 4394, 5929, 6933, 3905, 49688, 30871, 70480, 13443, 28678, 3100, 2793, 61589, 2790, 30529, 74701, 7078, 83703, 2793, 31149, 8707, 4394, 5929, 6933, 3905, 6468, 7171, 3905, 5929, 5040, 10153, 2938, 5929, 5802, 3552, 3434, 5929, 61300, 74701, 5565, 2793, 31149, 3966, 2808, 1022, 1, 5929, 7171, 4402, 6591, 2908, 6012, 5145, 74701, 10702, 23157, 26604, 74701, 23157, 26604, 86365, 86365, 5929, 6492, 5929, 4766, 5972, 2867, 4766, 5929, 3229, 5676, 2867, 12005, 23157, 26604, 74701, 86365, 5470, 14341, 10052, 5526, 5934, 3625, 3318, 23157, 26604, 3404, 74701, 3404, 6506, 23157, 26604, 7688, 86365, 6325, 2938, 23157, 26604, 86365, 6325, 3475, 60649, 74701, 6030, 4613, 15407, 5526, 5929, 3559, 7839, 6506, 74701, 5670, 18214, 5254, 6506, 74701, 4394, 15165, 5563, 5392, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 15619, 20936, 26727, 5506, 3329, 5929, 44418, 6492, 5224, 3916, 28724, 3329, 5929, 74701, 3287, 23586, 59638, 4246, 7106, 3475, 5574, 7043, 3625, 13113, 23772, 3855, 5486, 44418, 5470, 5520, 13113, 6012, 3043, 3329, 2938, 11748, 4884, 3528, 15619, 20936, 4484, 74701, 23586, 59638, 2793, 5994, 3911, 5875, 5520, 4664, 74701, 15619, 20936, 23586, 59638, 7527, 5929, 6629, 8019, 7164, 23586, 59638, 3911, 74701, 15619, 20936, 5546, 10357, 2867, 12733, 15619, 20936, 23586, 59638, 3329, 74701, 3280, 2867, 97942, 3983, 3920, 4090, 13113, 4484, 4856, 4856, 6921, 4484, 4613, 5785, 95751, 2793, 74489, 15619, 20936, 5596, 45791, 3280, 4484, 4090, 23709, 2050, 4303, 8336, 3475, 5596, 23586, 59638, 5520, 6492, 5929, 3469, 4090, 97942, 23709, 2050, 2793, 74489, 4303, 23586, 59638, 4484, 4630, 3280, 13113, 5929, 23586, 59638, 4303, 18248, 10102, 5929, 8012, 3983, 44104, 56900, 48119, 3499, 4912, 11536, 22490, 72328, 20249, 8811, 22490, 48119, 50412, 15988, 82418, 66113, 47836, 28724, 35284, 20223, 18903, 5929, 6312, 15988, 82418, 6201, 3625, 15619, 20936, 4303, 74701, 15619, 20936, 4303, 35284, 3280, 45791, 50412, 31031, 60192, 28419, 11436, 14534, 15619, 20936, 11778, 11748, 4884, 45791, 50412, 27838, 81397, 98757, 2032, 28419, 3055, 3911, 4905, 24954, 7429, 1454, 1393, 1459, 6551, 2938, 5101, 2793, 5655, 9258, 6312, 28807, 10238, 48119, 11778, 2003, 11748, 3351, 37745, 28419, 2002, 35042, 4482, 6312, 98122, 2025, 34433, 18009, 64198, 92957, 5373, 14534, 15619, 20936, 3280, 7043, 15619, 20936, 9540, 35496, 10102, 18009, 2959, 67321, 3916, 4280, 74701, 15619, 20936, 8953, 2793, 74489, 15619, 20936, 4484, 45791, 4825, 71030, 3778, 2910, 12569, 3171, 5929, 3280, 88012, 14747, 50753, 10238, 48119, 28419, 2002, 11436, 11748, 71030, 3778, 4772, 5690, 3328, 6312, 45865, 10238, 48119, 10102, 3552, 5019, 45791, 4241, 22490, 48119, 23586, 59638, 44104, 56900, 48119, 8705, 71030, 3778, 22490, 48119, 22490, 48119, 6312, 96566, 10238, 48119, 45791, 4825, 22490, 48119, 4884, 3310, 22508, 17810, 10130, 47961, 34721, 17652, 15619, 20936, 4884, 4280, 4630, 47348, 52540, 2041, 41250, 3469, 8909, 7977, 8336, 22157, 6404, 19385, 71030, 3778, 5929, 11748, 3528, 22490, 48119, 5929, 6921, 24546, 28419, 2002, 4484, 4241, 24546, 28419, 2002, 21621, 17810, 10130, 47348, 11163, 35327, 4288, 4825, 28724, 14534, 4108, 31065, 89511, 7504, 2033, 12225, 4261, 5196, 34433, 18009, 5373, 15619, 20936, 3640, 4995, 8631, 8196, 4280, 4630, 4855, 35284, 11748, 22490, 48119, 5929, 14534, 47961, 34721, 17652, 5040, 6904, 22490, 48119, 5929, 7069, 71030, 3778, 22490, 48119, 5373, 2793, 74489, 16483, 4484, 6461, 4090, 23709, 2050, 24954, 52248, 93789, 2040, 12386, 11999, 4482, 3329, 2867, 15619, 20936, 3329, 74701, 23586, 59638, 3559, 9475, 4294, 4484, 3559, 2793, 31149, 11827, 4482, 3911, 5891, 5425, 57637, 7285, 4639, 6412, 7171, 5392, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"uF3FFsPzc6zD","executionInfo":{"status":"ok","timestamp":1610526128794,"user_tz":-210,"elapsed":2800,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["sentence_maxlen=128"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m2Qc2IkrnEp","executionInfo":{"status":"ok","timestamp":1610526151994,"user_tz":-210,"elapsed":25645,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"2481c779-e6de-4409-e7aa-ee7ba1e6f751"},"source":["##Tokenize training and validation sentences:\r\n","train_encodings = tokenizer.batch_encode_plus(X_train,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","val_encodings = tokenizer.batch_encode_plus(X_val,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","test_encodings=tokenizer.batch_encode_plus(X_test,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwjkXARbetX-","executionInfo":{"status":"ok","timestamp":1610526151996,"user_tz":-210,"elapsed":23978,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"698310db-5ca8-425a-a63f-60ed44c9f867"},"source":["train_encodings[0]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"-iCp2PUEupYK","executionInfo":{"status":"ok","timestamp":1610526151997,"user_tz":-210,"elapsed":20777,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","\r\n","# for train set\r\n","train_seq = torch.tensor(train_encodings['input_ids'])\r\n","train_mask = torch.tensor(train_encodings['attention_mask'])\r\n","train_y = torch.tensor(y_train)\r\n","\r\n","# for validation set\r\n","val_seq = torch.tensor(val_encodings['input_ids'])\r\n","val_mask = torch.tensor(val_encodings['attention_mask'])\r\n","val_y = torch.tensor(y_val)\r\n","\r\n","# for test set\r\n","test_seq = torch.tensor(test_encodings['input_ids'])\r\n","test_mask = torch.tensor(test_encodings['attention_mask'])\r\n","test_y = torch.tensor(y_test)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0JkQbxVBmbM","executionInfo":{"status":"ok","timestamp":1610526151997,"user_tz":-210,"elapsed":20408,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"aaf16915-5dcd-44a6-c291-f0c8f8bb2aa6"},"source":["train_y[0]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"T2xiV6Nb0ddZ","executionInfo":{"status":"ok","timestamp":1610526151998,"user_tz":-210,"elapsed":15484,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","#define a batch size\r\n","batch_size = 32\r\n","\r\n","# wrap tensors\r\n","train_data = TensorDataset(train_seq, train_mask, train_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","train_sampler = RandomSampler(train_data)\r\n","\r\n","# dataLoader for train set\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","val_data = TensorDataset(val_seq, val_mask, val_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","val_sampler = SequentialSampler(val_data)\r\n","\r\n","# dataLoader for validation set\r\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","test_data = TensorDataset(test_seq, test_mask, test_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","test_sampler = SequentialSampler(test_data)\r\n","\r\n","# dataLoader for validation set\r\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":749,"referenced_widgets":["d039d4b69ad04a9591cf97e0ee9cb116","557d89fa82a5402a82bcddcd6b0d32b5","5af88c273d0848348371af16fed485bb","3b81df756b594035afe1e3159c6fc561","ff33fabb4ebf405c9142fce866409c50","9c54d97adc9c426282524e3ff4cb700a","a924d171532d446fad7cac9b84d8b33e","5df9ddc595164814b27a9688088602ad"]},"id":"UwGHXIjGfmaN","executionInfo":{"status":"ok","timestamp":1610526210147,"user_tz":-210,"elapsed":51341,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"49d780c4-4938-4251-ab1f-06a29c917607"},"source":["# example\r\n","\r\n","\r\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n","\r\n","# encode text\r\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\r\n","print(sent_id)\r\n","\r\n","seq = torch.tensor(sent_id['input_ids'])\r\n","mask = torch.tensor(sent_id['attention_mask'])\r\n","train_y = torch.tensor([0,1])\r\n","\r\n","transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")\r\n","cls_hs=transformer_model(seq,mask)\r\n","print(cls_hs)\r\n","print(cls_hs[0])\r\n","print(cls_hs[1])\r\n","print(cls_hs[1].shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["{'input_ids': [[2, 32071, 9574, 1026, 89390, 36260, 84378, 40908, 2041, 4, 0], [2, 13632, 25909, 70608, 1011, 40716, 2033, 1026, 89390, 36260, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d039d4b69ad04a9591cf97e0ee9cb116","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=651477729.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>), hidden_states=None, attentions=None, cross_attentions=None)\n","tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>)\n","tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>)\n","torch.Size([2, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByUEn_v4zknn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"n3AjEaHcEMfb","executionInfo":{"status":"ok","timestamp":1610526215888,"user_tz":-210,"elapsed":48154,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaAlYydhxPTd","executionInfo":{"status":"ok","timestamp":1610526396101,"user_tz":-210,"elapsed":1192,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# freeze all the parameters\r\n","for param in transformer_model.parameters():\r\n","    param.requires_grad = False"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUa1R1WQONe6","executionInfo":{"status":"ok","timestamp":1610526396619,"user_tz":-210,"elapsed":1227,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"92ae000c-170f-4919-f2fa-ef07a1a6d5c2"},"source":["len(labels)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"oyE_ThEms5aZ","executionInfo":{"status":"ok","timestamp":1610528320139,"user_tz":-210,"elapsed":1129,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["class BERT_Arch(nn.Module):\r\n","\r\n","    def __init__(self, bert):\r\n","      \r\n","      super(BERT_Arch, self).__init__()\r\n","\r\n","      self.bert = bert \r\n","      \r\n","      # dropout layer\r\n","      self.dropout = nn.Dropout(0.1)\r\n","      \r\n","      # relu activation function\r\n","      self.relu =  nn.ReLU()\r\n","\r\n","      # dense layer 1\r\n","      self.fc1 = nn.Linear(768,512)\r\n","      \r\n","      # dense layer 2 (Output layer)\r\n","      self.fc2 = nn.Linear(512,128)\r\n","\r\n","      # dense layer 3 (Output layer)\r\n","      self.fc3 = nn.Linear(128,128)\r\n","\r\n","      # dense layer 4 (Output layer)\r\n","      self.fc4 = nn.Linear(128,78)\r\n","\r\n","\r\n","      #sigmoid activation function\r\n","      self.sigmoid = nn.Sigmoid()\r\n","\r\n","    #define the forward pass\r\n","    def forward(self, sent_id, mask):\r\n","\r\n","      #pass the inputs to the model  \r\n","      cls_hs = self.bert(sent_id, attention_mask=mask)\r\n","      \r\n","      x = self.fc1(cls_hs[1])\r\n","\r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      # output layer\r\n","      x = self.fc2(x)\r\n","      x = self.relu(x)\r\n","      x = self.fc3(x)\r\n","      x = self.relu(x)\r\n","      x = self.fc4(x)\r\n","      \r\n","      # apply sigmoid activation\r\n","      x = self.sigmoid(x)\r\n","\r\n","      return x"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVuYzMqnJGjT"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"rDuHzo96z6z8","executionInfo":{"status":"ok","timestamp":1610528335571,"user_tz":-210,"elapsed":13909,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(transformer_model)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUNSLBYcLc9q","executionInfo":{"status":"ok","timestamp":1610528488505,"user_tz":-210,"elapsed":1527,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArHmwhh7JrZh","executionInfo":{"status":"ok","timestamp":1610528491026,"user_tz":-210,"elapsed":2330,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["loss_func =nn.MultiLabelSoftMarginLoss()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8LjQyDXs0bG","executionInfo":{"status":"ok","timestamp":1610528491547,"user_tz":-210,"elapsed":1182,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function to train the model\r\n","def train():\r\n","  \r\n","  model.train()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save model predictions\r\n","  total_preds=[]\r\n","  \r\n","  # iterate over batches\r\n","  for step,batch in enumerate(train_dataloader):\r\n","    \r\n","    # progress update after every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [r.to(device) for r in batch]\r\n"," \r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # clear previously calculated gradients \r\n","    model.zero_grad()        \r\n","\r\n","    # get model predictions for the current batch\r\n","    preds = model(sent_id, mask)\r\n","\r\n","    # compute the loss between actual and predicted values\r\n","    \r\n","    loss = loss_func(preds, labels)\r\n","    # add on to the total loss\r\n","    total_loss = total_loss + loss.item()\r\n","\r\n","    # backward pass to calculate the gradients\r\n","    loss.backward()\r\n","\r\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","    # update parameters\r\n","    optimizer.step()\r\n","\r\n","    # model predictions are stored on GPU. So, push it to CPU\r\n","    preds=preds.detach().cpu().numpy()\r\n","\r\n","    # append the model predictions\r\n","    total_preds.append(preds)\r\n","\r\n","  # compute the training loss of the epoch\r\n","  avg_loss = total_loss / len(train_dataloader)\r\n","  \r\n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  #returns the loss and predictions\r\n","  return avg_loss, total_preds"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNBRQo9WMHey","executionInfo":{"status":"ok","timestamp":1610528492594,"user_tz":-210,"elapsed":1908,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function for evaluating the model\r\n","def evaluate():\r\n","  \r\n","  print(\"\\nEvaluating...\")\r\n","  \r\n","  # deactivate dropout layers\r\n","  model.eval()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save the model predictions\r\n","  total_preds = []\r\n","\r\n","  # iterate over batches\r\n","  for step,batch in enumerate(val_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","\r\n","      # compute the validation loss between actual and predicted values\r\n","      loss = loss_func(preds,labels)\r\n","\r\n","      total_loss = total_loss + loss.item()\r\n","\r\n","      preds = preds.detach().cpu().numpy()\r\n","\r\n","      total_preds.append(preds)\r\n","\r\n","  # compute the validation loss of the epoch\r\n","  avg_loss = total_loss / len(val_dataloader) \r\n","\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  return avg_loss, total_preds"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qu5pfrJKtTc0","executionInfo":{"status":"ok","timestamp":1610533899285,"user_tz":-210,"elapsed":2581473,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"7d489eee-1a12-4ec8-c405-47bd0a7f28a4"},"source":["# number of training epochs\r\n","epochs = 10\r\n","\r\n","# set initial loss to infinite\r\n","best_valid_loss = float('inf')\r\n","\r\n","# empty lists to store training and validation loss of each epoch\r\n","train_losses=[]\r\n","valid_losses=[]\r\n","\r\n","#for each epoch\r\n","for epoch in range(epochs):\r\n","     \r\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n","    \r\n","    #train model\r\n","    train_loss, _ = train()\r\n","    \r\n","    #evaluate model\r\n","    valid_loss, _ = evaluate()\r\n","    \r\n","    #save the best model\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'saved_weights.pt')\r\n","    \r\n","    # append training and validation loss\r\n","    train_losses.append(train_loss)\r\n","    valid_losses.append(valid_loss)\r\n","    \r\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 2 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 3 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 4 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 5 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 6 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 7 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 8 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 9 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 10 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GlJpADKkIOqX"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"nVrfkSoKIOIV","executionInfo":{"status":"ok","timestamp":1610531099282,"user_tz":-210,"elapsed":4815,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["path = 'saved_weights_dense_4.pt'\n","torch.save(model.state_dict(), path)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ2_aS0zCLvp"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"cvR-FhPpuLkR"},"source":["# torch.cuda.empty_cache()\r\n","# pass the pre-trained BERT to our define architecture\r\n","model = BERT_Arch(transformer_model)\r\n","\r\n","# push the model to GPU\r\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aOPRZ2jVvNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610531105788,"user_tz":-210,"elapsed":3575,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"2c54a4b7-9581-4524-b61d-36604f9c787b"},"source":["#load weights of best model\r\n","path = 'saved_weights_dense_4.pt'\r\n","model.load_state_dict(torch.load(path))"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"PM1uUcZFCPVg"},"source":["After loading model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZhHObMnzuws","executionInfo":{"status":"ok","timestamp":1610533962116,"user_tz":-210,"elapsed":2642356,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"78ec1d11-d1e4-40eb-870f-1188c48c4580"},"source":["y_pred=[]\r\n","y_true=[]\r\n","for step,batch in enumerate(test_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","      # print(preds)\r\n","      # print(preds.cpu().numpy())\r\n","      preds = preds.cpu().numpy()\r\n","      # model's performance\r\n","    # preds = numpy.argmax(preds, axis = 1)\r\n","    \r\n","    measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    for l in preds:\r\n","      temp=[]\r\n","      for value in l:\r\n","        if value >= measure:\r\n","          temp.append(1)\r\n","        else:\r\n","          temp.append(0)\r\n","      y_pred.append(temp)\r\n","    y_true.extend(labels.cpu().numpy())\r\n","    # print(labels.cpu().numpy()[0], preds[0])\r\n","print(classification_report(y_true, y_pred))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["  Batch    50  of    135.\n","  Batch   100  of    135.\n","              precision    recall  f1-score   support\n","\n","           0       0.02      0.12      0.03       113\n","           1       0.03      0.32      0.06       146\n","           2       0.02      0.25      0.04       137\n","           3       0.00      0.21      0.00        14\n","           4       0.02      0.41      0.04        75\n","           5       0.00      0.18      0.00        17\n","           6       0.02      0.13      0.03       162\n","           7       0.01      0.64      0.02        47\n","           8       0.00      0.19      0.01        27\n","           9       0.01      0.10      0.02       105\n","          10       0.01      0.04      0.01       141\n","          11       0.02      0.27      0.04        70\n","          12       0.04      0.35      0.08       158\n","          13       0.00      0.26      0.01        19\n","          14       0.00      0.12      0.00        17\n","          15       0.03      0.30      0.05       159\n","          16       0.01      0.08      0.02       130\n","          17       0.04      0.39      0.07       141\n","          18       0.01      0.22      0.03       107\n","          19       0.01      0.27      0.03        60\n","          20       0.03      0.36      0.06       147\n","          21       0.03      0.28      0.05       127\n","          22       0.01      0.14      0.01        42\n","          23       0.02      0.21      0.04       142\n","          24       0.01      0.19      0.02        69\n","          25       0.07      0.37      0.12       162\n","          26       0.02      0.20      0.04       166\n","          27       0.03      0.16      0.05       145\n","          28       0.03      0.45      0.05       143\n","          29       0.03      0.17      0.05       144\n","          30       0.02      0.25      0.03        60\n","          31       0.05      0.32      0.08       111\n","          32       0.03      0.39      0.05       142\n","          33       0.03      0.18      0.05       146\n","          34       0.02      0.65      0.03        74\n","          35       0.02      0.21      0.04       145\n","          36       0.02      0.20      0.04       169\n","          37       0.01      0.10      0.02        68\n","          38       0.01      0.48      0.02        52\n","          39       0.00      0.21      0.01        28\n","          40       0.02      0.18      0.04       132\n","          41       0.01      0.22      0.03       120\n","          42       0.01      0.18      0.02        91\n","          43       0.03      0.42      0.06       163\n","          44       0.03      0.26      0.05       149\n","          45       0.00      0.21      0.01        29\n","          46       0.01      0.14      0.01        36\n","          47       0.03      0.61      0.06       152\n","          48       0.02      0.13      0.03       139\n","          49       0.02      0.44      0.04        77\n","          50       0.00      0.03      0.00        32\n","          51       0.01      0.48      0.01        25\n","          52       0.02      0.47      0.03        40\n","          53       0.01      0.08      0.02       110\n","          54       0.02      0.16      0.03       120\n","          55       0.01      0.65      0.02        43\n","          56       0.01      0.21      0.03        67\n","          57       0.03      0.41      0.05        54\n","          58       0.05      0.70      0.09       146\n","          59       0.02      0.07      0.03       130\n","          60       0.09      0.60      0.16       167\n","          61       0.01      0.37      0.02        46\n","          62       0.02      0.19      0.04       166\n","          63       0.02      0.40      0.04       142\n","          64       0.01      0.05      0.01        84\n","          65       0.02      0.10      0.03       141\n","          66       0.01      0.53      0.02        36\n","          67       0.02      0.29      0.03       101\n","          68       0.10      0.57      0.17       145\n","          69       0.02      0.22      0.04       147\n","          70       0.03      0.16      0.05       143\n","          71       0.03      0.45      0.05       149\n","          72       0.03      0.48      0.06       120\n","          73       0.01      0.15      0.01        40\n","          74       0.01      0.42      0.02        31\n","          75       0.02      0.15      0.04       169\n","          76       0.02      0.56      0.04        71\n","          77       0.04      0.45      0.08       139\n","\n","   micro avg       0.02      0.29      0.04      8019\n","   macro avg       0.02      0.29      0.04      8019\n","weighted avg       0.03      0.29      0.05      8019\n"," samples avg       0.02      0.33      0.03      8019\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"T3JMIuylInOv"},"source":[""],"execution_count":null,"outputs":[]}]}