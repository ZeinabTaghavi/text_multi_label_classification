{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ParsBERT_pytorch_SGD_Dense_3_arch_2.ipynb","provenance":[{"file_id":"1k6QLcPAJKT5H2yTy61U-YCB267inolmb","timestamp":1610512876213},{"file_id":"1AIm-KimERCJutlpyjiZ2IAwM-cCrVsWM","timestamp":1610440221899},{"file_id":"17mqUcShahUjZjQxywgKQSGU1jV-CZW8o","timestamp":1610336820188},{"file_id":"1FgtzYXY0CXNyE_2FU4IEqJTQzmVZNvDh","timestamp":1610105938882}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9acc1a13a08b42978644d187385d18c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_59ee68b9fc7f48fc91646221904acd35","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0422c0542a64425ab8a58a4e0fdd8b7b","IPY_MODEL_99cae3b8cb14486abec7fea25811694e"]}},"59ee68b9fc7f48fc91646221904acd35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0422c0542a64425ab8a58a4e0fdd8b7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a8075ea61ecb4c8c8be8db80d6fe29be","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1441,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1441,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e07aa6a79d3a4cb2bd2ace4f72cf1a85"}},"99cae3b8cb14486abec7fea25811694e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_abbc8fbf5e1249179af08f7f292d76d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.44k/1.44k [00:02&lt;00:00, 672B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba86bb89f5ba46268f7829eb0afd3524"}},"a8075ea61ecb4c8c8be8db80d6fe29be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e07aa6a79d3a4cb2bd2ace4f72cf1a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abbc8fbf5e1249179af08f7f292d76d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba86bb89f5ba46268f7829eb0afd3524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"725343b0eb074baeb01218676f90de2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5a6444a78584484fa96c91e84dbec240","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8aa29be66bd94d26b312451b12eef48f","IPY_MODEL_6bdb4e592c5a4bc086aa351729b67aae"]}},"5a6444a78584484fa96c91e84dbec240":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8aa29be66bd94d26b312451b12eef48f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b4570d852294c08a5f2fa2aff7dfabb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1198122,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1198122,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ab206ccc9174ca6a62f4be250c329f7"}},"6bdb4e592c5a4bc086aa351729b67aae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f60a094af2384fd689e2e7120bb42451","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20M/1.20M [00:01&lt;00:00, 717kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9924094a3ef344d79d4a9d1f3328e2b0"}},"6b4570d852294c08a5f2fa2aff7dfabb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0ab206ccc9174ca6a62f4be250c329f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f60a094af2384fd689e2e7120bb42451":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9924094a3ef344d79d4a9d1f3328e2b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a51a8547b3714aeda8bc3256ac155142":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b5ea8df8202241ef8ad61a79f35eee28","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9dcf5caf9f9541b2956e5fc216475e29","IPY_MODEL_5427045d37814b849c6c33bc3c2fcc8d"]}},"b5ea8df8202241ef8ad61a79f35eee28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9dcf5caf9f9541b2956e5fc216475e29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4f758e4549544ea0ad4a3c5d3220fbe1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd1d33595cdb40cda712e303779d57ad"}},"5427045d37814b849c6c33bc3c2fcc8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ddcbd977a7cb4650a141709bb591f855","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 243B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c79b4b5b4a334665b21178fb0420127d"}},"4f758e4549544ea0ad4a3c5d3220fbe1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fd1d33595cdb40cda712e303779d57ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ddcbd977a7cb4650a141709bb591f855":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c79b4b5b4a334665b21178fb0420127d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"174ab04256204ba79ff910b37e68eccf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ac5ceaebb4bf4c68a5d737e4f877845c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_65ec530e9eb647a89d63efb2f7e0e029","IPY_MODEL_72c5a61d0b3c48d98a0475481c0cc29f"]}},"ac5ceaebb4bf4c68a5d737e4f877845c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65ec530e9eb647a89d63efb2f7e0e029":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ee5d22dfcd11438ba667b650a1bf4ef2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":62,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":62,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_53b933e076574d3a8e932415585319aa"}},"72c5a61d0b3c48d98a0475481c0cc29f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6d5d3ab90b464d67bad1ab9cdf52386c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62.0/62.0 [00:11&lt;00:00, 5.52B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe510263ff2a4e0bb63931ee86c11f61"}},"ee5d22dfcd11438ba667b650a1bf4ef2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"53b933e076574d3a8e932415585319aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d5d3ab90b464d67bad1ab9cdf52386c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fe510263ff2a4e0bb63931ee86c11f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fd50aa1e02c4cf99cfab8cd1ab3a9d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_681d35f5a344403d917aaec0dd4e8e02","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0ab5157be9b64419b4f6cdf0cd86ad0e","IPY_MODEL_aa17575627c146419f0179ea8f9e2e75"]}},"681d35f5a344403d917aaec0dd4e8e02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ab5157be9b64419b4f6cdf0cd86ad0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9906a0b34aa0444ca8c6fc965b8d17b2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":651477729,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":651477729,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23cb06a3ab584367ad4609216f9b41f6"}},"aa17575627c146419f0179ea8f9e2e75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_db51ffddccbf47d48b53ffd344525a21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 651M/651M [00:17&lt;00:00, 38.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6bbdcc8602f14d5481910d40fb79d1a0"}},"9906a0b34aa0444ca8c6fc965b8d17b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"23cb06a3ab584367ad4609216f9b41f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db51ffddccbf47d48b53ffd344525a21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6bbdcc8602f14d5481910d40fb79d1a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"j1LTPn7IjqTz"},"source":["Source:\r\n","\r\n","huggingface: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","\r\n","Tutorial:https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"]},{"cell_type":"code","metadata":{"id":"OmPFvCbSqyZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610703882451,"user_tz":-210,"elapsed":249417,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"3f571612-9a0f-4be7-f7e0-538ed704065a"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRxC0Pz1qzKc","executionInfo":{"status":"ok","timestamp":1610703882828,"user_tz":-210,"elapsed":249501,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/sharif/FineTuning/ipython(guide)')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCRkKc3NcgkX","executionInfo":{"status":"ok","timestamp":1610703904837,"user_tz":-210,"elapsed":7141,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"f3f7cd8c-355b-40d8-f7ea-376ca3965834"},"source":["!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 33.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 57.0MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=eb46a0208ba9687d47538def24e7b733c02c31a7169adebdd056b0d3bc29d098\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODc44DglgNjZ","executionInfo":{"status":"ok","timestamp":1610703907958,"user_tz":-210,"elapsed":10038,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"a9bd15e8-9f23-40ee-dda7-f9412182abce"},"source":["!pip3 install sentencepiece"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 7.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJY_L2p9a0t0","executionInfo":{"status":"ok","timestamp":1610703907960,"user_tz":-210,"elapsed":9817,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"8b55cc44-656d-4b6b-9f78-308717331930"},"source":["!git clone https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","GIT_LFS_SKIP_SMUDGE=1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["fatal: destination path 'bert-fa-base-uncased-clf-persiannews' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eg3Up037nThu","executionInfo":{"status":"ok","timestamp":1610703913856,"user_tz":-210,"elapsed":15506,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import numpy\r\n","import pandas\r\n","import re\r\n","from sklearn.preprocessing import MultiLabelBinarizer\r\n","from sklearn.model_selection import train_test_split\r\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,TFAutoModel,AutoModel\r\n","from transformers import BertConfig, BertTokenizer\r\n","from transformers import TFBertModel, TFBertForSequenceClassification\r\n","from transformers import glue_convert_examples_to_features, InputExample\r\n","from sklearn.metrics import classification_report"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ur9wv1ytrZu","executionInfo":{"status":"ok","timestamp":1610703913858,"user_tz":-210,"elapsed":15310,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# specify GPU\r\n","device = torch.device(\"cuda\")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xax4bHubzpMp"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"TJf6T40glV5g","executionInfo":{"status":"ok","timestamp":1610703916324,"user_tz":-210,"elapsed":17299,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["limit_number = 750\r\n","data = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv',index_col=0)\r\n","data = data.dropna().reset_index(drop=True)\r\n","X = data[\"body\"].values.tolist()\r\n","y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","labels = []\r\n","tag=[]\r\n","for item in y['tag']:\r\n","  labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","  tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","labels = list(set(labels))\r\n","mlb = MultiLabelBinarizer()\r\n","Y=mlb.fit_transform(tag)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH3jCKaZsEWo","executionInfo":{"status":"ok","timestamp":1610703916326,"user_tz":-210,"elapsed":16931,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"def8d6b4-a02e-4263-92ee-b6d144c62b5d"},"source":["X_train, X_test, y_train, y_test = train_test_split(X,Y , test_size=0.2)\r\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n","print('train: ', len(X_train) , '\\ntest: ', len(X_test) , '\\nval: ', len(X_val) ,\"\\ny_tain:\",len(y_train) )"],"execution_count":10,"outputs":[{"output_type":"stream","text":["train:  12896 \n","test:  4299 \n","val:  4299 \n","y_tain: 12896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vei6iu9atmyd","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["9acc1a13a08b42978644d187385d18c1","59ee68b9fc7f48fc91646221904acd35","0422c0542a64425ab8a58a4e0fdd8b7b","99cae3b8cb14486abec7fea25811694e","a8075ea61ecb4c8c8be8db80d6fe29be","e07aa6a79d3a4cb2bd2ace4f72cf1a85","abbc8fbf5e1249179af08f7f292d76d7","ba86bb89f5ba46268f7829eb0afd3524","725343b0eb074baeb01218676f90de2b","5a6444a78584484fa96c91e84dbec240","8aa29be66bd94d26b312451b12eef48f","6bdb4e592c5a4bc086aa351729b67aae","6b4570d852294c08a5f2fa2aff7dfabb","0ab206ccc9174ca6a62f4be250c329f7","f60a094af2384fd689e2e7120bb42451","9924094a3ef344d79d4a9d1f3328e2b0","a51a8547b3714aeda8bc3256ac155142","b5ea8df8202241ef8ad61a79f35eee28","9dcf5caf9f9541b2956e5fc216475e29","5427045d37814b849c6c33bc3c2fcc8d","4f758e4549544ea0ad4a3c5d3220fbe1","fd1d33595cdb40cda712e303779d57ad","ddcbd977a7cb4650a141709bb591f855","c79b4b5b4a334665b21178fb0420127d","174ab04256204ba79ff910b37e68eccf","ac5ceaebb4bf4c68a5d737e4f877845c","65ec530e9eb647a89d63efb2f7e0e029","72c5a61d0b3c48d98a0475481c0cc29f","ee5d22dfcd11438ba667b650a1bf4ef2","53b933e076574d3a8e932415585319aa","6d5d3ab90b464d67bad1ab9cdf52386c","fe510263ff2a4e0bb63931ee86c11f61"]},"executionInfo":{"status":"ok","timestamp":1610703918930,"user_tz":-210,"elapsed":19215,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"4012da6e-5bd8-4dbe-9764-f19bd2b52640"},"source":["##we would load the tokenizer\r\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9acc1a13a08b42978644d187385d18c1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1441.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"725343b0eb074baeb01218676f90de2b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1198122.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a51a8547b3714aeda8bc3256ac155142","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"174ab04256204ba79ff910b37e68eccf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7wdU0zejDNq","executionInfo":{"status":"ok","timestamp":1610703918931,"user_tz":-210,"elapsed":18833,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"c74ade5a-248e-41ba-8e0d-ba3fc24f715b"},"source":["#example\r\n","text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\r\n","tokenized=tokenizer.tokenize(X_train[0])\r\n","input_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n","print(tokenized)\r\n","print(input_ids)\r\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['رمان', 'بیگانه', 'معروف', 'نویسنده', 'فرانسوی', 'البر', 'کامو', 'قصد', 'نوشته', 'کتاب', 'براتون', 'معرفی', 'توجه', 'قرار', 'بگیره', 'کتاب', 'مدت', 'ها', 'خواندهبود', '##م', 'موقع', 'برام', 'جالب', 'اخیرا', 'خواندن', 'کتاب', 'کامو', 'ترغیب', 'کتاب', 'مجدد', 'بخوانم', 'خواندن', 'کتاب', 'خواندم', 'واقع', 'کتاب', 'ها', 'دوبار', 'چندبار', 'خواند', 'کتاب', 'داستان', 'جمله', 'اغاز', 'میشه', 'مادرم', 'مرد', 'نمی', 'دانم', 'جمله', 'تنهایی', 'حدودی', 'شخصیت', 'مورسو', 'شخصیت', 'اصلی', 'داستان', 'میده', 'تفاوت', 'بیگانگی', 'عادی', 'نشون', 'میده', 'مورسو', 'مادرش', 'نوان', '##خانه', 'خانه', 'سالمندان', 'سپرده', 'دلیل', 'کار', 'می', 'کنه', 'اندازه', 'پول', 'مخارج', '##ش', 'طرفی', 'حرفی', 'باهم', 'سوال', 'مادرم', 'دوست', 'مراسم', 'تدفین', 'تعریف', 'میکنه', 'حرفی', 'ناراحتی', 'اذیت', 'می', 'کنه', 'افتاب', 'تند', 'خستگی', 'بیخوابی', 'نامزدش', 'میگذر', '##ونه', 'مورسو', 'دعوت', 'دوستاش', 'ساحل', 'میره', 'ظهر', 'افتابی', 'داغ', 'مرد', 'عرب', 'می', 'کش', '##ه', 'ادامه', 'داستان', 'جلسات', 'دادگاه', 'صحبت', 'ها', 'دادستان', 'وکیل', 'مدافع', 'شاهدان', 'مورسو', 'موقعیت', 'قضاوت', 'بقیه', 'نشون', 'میده', 'عجیب', 'غریب', 'بیگانه', 'می', 'دون', '##ن', 'صورتی', 'مورسو', 'عادی', 'قتل', 'می', 'پرس', '##ند', 'منکر', 'نمیشه', 'افتاب', 'تند', 'ادامه', 'ی', 'جلسات', 'دادگاه', 'صحبتها', 'مورسو', 'احساس', 'شخص', 'صحبت', 'حضور', 'وکیل', 'مدافع', 'مورسو', 'صحبت', 'لفظ', 'مورسو', 'نمی', 'بینند', 'غریب', 'ناشناخته', 'دادگاه', 'خاطر', 'قیدی', 'حسی', 'مرگ', 'مادرش', 'متهم', 'عقیده', 'دادستان', 'مردی', 'روح', 'خطرناک', 'پرونده', 'محاکمه', 'مورسو', 'پرونده', 'بررسی', 'پسری', 'پدرش', 'کشتهاست', 'مورسو', 'دادستان', 'می', 'اندیش', '##د', 'مورسو', 'بیخیالی', 'تفاوتی', 'همیشگی', '##ش', 'اعتراف', 'مرد', 'کشتهاست', 'احساس', 'گناه', 'احساس', 'ملال', 'اندوه', 'کلافه', 'کردهاست', 'نکته', 'جالب', 'مورسو', '##ی', 'بیخیال', 'قید', 'حکم', 'اعدام', 'تشویش', 'بامداد', 'اعدام', 'ها', 'ساعت', 'ها', 'نحوی', 'سرزنش', 'اعدام', 'فرار', 'زندان', 'مطالعه', 'نکردهاست', 'واقع', 'فرار', 'نتیجه', 'نمی', 'رسد', 'فرار', 'نشدنی', 'حکم', 'قطعی', 'می', 'بیند', 'شرایط', 'عادت', 'ملاقات', 'کشیش', 'نهایت', 'واقع', 'مجبور', 'شنیدن', 'صحبت', 'ها', 'کشیش', 'ادامه', 'بحث', 'گفتگو', 'ها', 'کشیش', 'اوم', '##ورس', '##و', 'خشم', 'هیجان', 'میشه', 'درونش', 'کشیش', 'فریاد', 'میکشه', 'میاد', 'مورسو', 'ازادی', 'رهایی', 'میرسه', 'وحدت', 'اتحاد', 'طبیعت', 'میرسه', 'بخشهایی', 'کتاب', 'داه', 'روزی', 'رام', 'درم', 'ادم', 'عادت', 'درک', 'ادر', '##ا', 'ده', 'ام', 'ارزو', 'برسانند', 'فریاد', 'ها', 'کینه', 'پیشواز']\n","[5917, 11086, 4387, 5317, 5719, 8287, 27612, 4790, 4196, 3250, 34233, 3852, 3211, 2959, 41425, 3250, 3679, 5929, 99452, 2015, 6905, 21667, 5484, 7249, 6388, 3250, 27612, 10955, 3250, 6404, 32054, 6388, 3250, 21289, 3473, 3250, 5929, 17560, 26198, 6819, 3250, 4059, 3525, 3500, 10672, 14560, 2999, 3821, 62047, 3525, 7272, 9149, 4821, 87338, 4821, 3376, 4059, 3083, 5000, 35347, 6111, 22367, 3083, 87338, 7091, 52527, 3653, 3764, 16026, 5340, 3310, 2867, 2793, 14114, 4205, 3928, 14288, 2014, 7644, 12445, 12175, 4639, 14560, 4219, 4495, 21537, 4568, 12702, 12445, 9747, 11801, 2793, 14114, 7531, 5945, 8976, 20422, 32176, 11374, 3285, 87338, 5728, 80711, 6432, 22067, 9313, 15854, 7742, 2999, 4363, 2793, 2885, 2008, 3251, 4059, 7295, 5637, 4386, 5929, 8548, 8703, 8317, 20188, 87338, 4872, 9182, 6379, 22367, 3083, 6293, 10631, 11086, 2793, 7018, 2011, 5426, 87338, 6111, 5464, 2793, 4268, 2790, 13744, 26702, 7531, 5945, 3251, 1442, 7295, 5637, 20926, 87338, 4189, 3504, 4386, 3470, 8703, 8317, 87338, 4386, 15866, 87338, 3821, 76050, 10631, 11201, 5637, 3804, 50010, 11260, 4128, 7091, 6394, 7571, 8548, 7871, 3825, 7673, 5307, 10000, 87338, 5307, 3640, 13212, 5731, 49415, 87338, 8548, 2793, 7376, 2013, 87338, 73413, 12160, 10814, 2014, 8888, 2999, 49415, 4189, 8810, 4189, 35547, 14696, 31325, 4454, 4909, 5484, 87338, 2003, 28347, 10606, 5336, 7064, 24484, 12757, 7064, 5929, 3551, 5929, 8554, 14934, 7064, 5711, 5511, 4893, 17028, 3473, 5711, 3680, 3821, 19355, 5711, 17531, 5336, 7465, 2793, 8259, 3517, 8145, 6967, 14262, 4459, 3473, 5449, 9457, 4386, 5929, 14262, 3251, 4316, 6771, 5929, 14262, 13115, 18211, 2005, 7599, 6766, 10672, 33336, 14262, 10589, 49832, 19994, 87338, 4880, 10996, 34885, 7757, 6583, 5318, 34885, 8935, 3250, 48027, 6367, 8331, 6996, 5983, 8145, 5520, 14822, 2006, 3031, 2822, 12988, 11411, 10589, 5929, 16231, 37869]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az4rwU0l5ECn","executionInfo":{"status":"ok","timestamp":1610703918931,"user_tz":-210,"elapsed":18473,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# encode text\r\n","sent_id = tokenizer.batch_encode_plus(X_train[:10], padding=True, return_token_type_ids=False)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNRU-SH65ZEE","executionInfo":{"status":"ok","timestamp":1610703918932,"user_tz":-210,"elapsed":18100,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"ea3822a4-4565-4ae5-931b-47771444ea4e"},"source":["sent_id"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 5917, 11086, 4387, 5317, 5719, 8287, 27612, 4790, 4196, 3250, 34233, 3852, 3211, 2959, 41425, 3250, 3679, 5929, 99452, 2015, 6905, 21667, 5484, 7249, 6388, 3250, 27612, 10955, 3250, 6404, 32054, 6388, 3250, 21289, 3473, 3250, 5929, 17560, 26198, 6819, 3250, 4059, 3525, 3500, 10672, 14560, 2999, 3821, 62047, 3525, 7272, 9149, 4821, 87338, 4821, 3376, 4059, 3083, 5000, 35347, 6111, 22367, 3083, 87338, 7091, 52527, 3653, 3764, 16026, 5340, 3310, 2867, 2793, 14114, 4205, 3928, 14288, 2014, 7644, 12445, 12175, 4639, 14560, 4219, 4495, 21537, 4568, 12702, 12445, 9747, 11801, 2793, 14114, 7531, 5945, 8976, 20422, 32176, 11374, 3285, 87338, 5728, 80711, 6432, 22067, 9313, 15854, 7742, 2999, 4363, 2793, 2885, 2008, 3251, 4059, 7295, 5637, 4386, 5929, 8548, 8703, 8317, 20188, 87338, 4872, 9182, 6379, 22367, 3083, 6293, 10631, 11086, 2793, 7018, 2011, 5426, 87338, 6111, 5464, 2793, 4268, 2790, 13744, 26702, 7531, 5945, 3251, 1442, 7295, 5637, 20926, 87338, 4189, 3504, 4386, 3470, 8703, 8317, 87338, 4386, 15866, 87338, 3821, 76050, 10631, 11201, 5637, 3804, 50010, 11260, 4128, 7091, 6394, 7571, 8548, 7871, 3825, 7673, 5307, 10000, 87338, 5307, 3640, 13212, 5731, 49415, 87338, 8548, 2793, 7376, 2013, 87338, 73413, 12160, 10814, 2014, 8888, 2999, 49415, 4189, 8810, 4189, 35547, 14696, 31325, 4454, 4909, 5484, 87338, 2003, 28347, 10606, 5336, 7064, 24484, 12757, 7064, 5929, 3551, 5929, 8554, 14934, 7064, 5711, 5511, 4893, 17028, 3473, 5711, 3680, 3821, 19355, 5711, 17531, 5336, 7465, 2793, 8259, 3517, 8145, 6967, 14262, 4459, 3473, 5449, 9457, 4386, 5929, 14262, 3251, 4316, 6771, 5929, 14262, 13115, 18211, 2005, 7599, 6766, 10672, 33336, 14262, 10589, 49832, 19994, 87338, 4880, 10996, 34885, 7757, 6583, 5318, 34885, 8935, 3250, 48027, 6367, 8331, 6996, 5983, 8145, 5520, 14822, 2006, 3031, 2822, 12988, 11411, 10589, 5929, 16231, 37869, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4285, 10166, 5440, 3404, 5596, 13834, 94385, 3310, 4549, 5929, 2927, 3205, 16719, 10214, 4394, 4084, 31020, 6173, 8459, 4651, 17420, 43218, 4062, 7670, 3777, 16719, 6839, 12139, 9678, 4912, 12139, 3680, 5850, 4613, 19761, 39761, 5929, 9258, 4957, 39761, 5929, 4875, 3567, 8875, 4875, 3841, 4394, 2927, 4549, 12733, 3680, 6602, 6516, 3404, 5842, 4089, 5596, 3329, 12269, 90783, 6763, 2049, 50309, 9258, 16884, 5642, 39761, 2793, 14760, 2013, 3595, 39761, 5929, 6921, 39761, 5929, 4875, 8875, 2793, 14760, 2013, 3552, 4090, 4875, 51234, 47598, 3680, 2867, 5891, 7765, 16698, 4892, 4957, 67661, 4219, 30373, 2011, 3876, 3426, 5929, 50738, 2816, 8038, 3337, 2816, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 5117, 5196, 3511, 5254, 2793, 17355, 2015, 3841, 5196, 4484, 3511, 7580, 6901, 6343, 10319, 3640, 5321, 2959, 5690, 5246, 4484, 5117, 5071, 5117, 3511, 44945, 33742, 79068, 14592, 25961, 32538, 3903, 4482, 3511, 4002, 4002, 3511, 44945, 33742, 79068, 14592, 25961, 32538, 3903, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 17690, 6188, 4484, 81680, 2867, 7026, 3475, 3607, 29238, 19262, 2008, 5690, 2881, 4059, 29574, 3171, 5929, 4645, 2959, 7085, 34727, 5536, 5929, 71030, 4647, 5421, 3855, 38984, 71030, 3778, 70096, 2003, 3171, 2938, 69503, 71030, 3778, 19791, 73811, 4055, 12914, 79566, 5321, 11231, 55748, 4313, 12441, 10897, 8120, 2816, 48449, 6895, 11912, 5757, 22546, 3494, 27409, 2867, 17690, 6188, 8473, 4059, 12139, 6192, 17690, 6188, 6503, 55633, 2015, 24215, 47388, 32395, 96255, 34340, 20936, 21825, 24215, 10595, 15611, 4613, 4905, 71117, 2793, 3347, 17690, 6188, 3490, 6138, 12702, 3680, 2938, 17690, 6188, 3083, 22546, 6079, 3083, 5101, 5988, 19919, 12478, 2959, 11231, 2867, 4613, 2793, 14114, 31765, 9405, 6188, 98122, 2025, 15988, 82418, 6201, 35340, 70182, 5806, 54936, 2025, 35284, 4884, 18009, 26162, 92957, 26162, 63093, 8811, 71030, 3778, 5929, 5166, 2867, 5330, 6347, 5596, 4766, 3469, 4090, 6602, 6516, 6518, 98122, 2025, 35340, 70182, 5806, 54936, 2025, 28724, 5929, 4568, 4461, 84886, 2011, 26040, 6921, 45791, 27784, 96255, 34340, 20936, 18009, 64198, 92957, 50145, 4244, 4568, 28724, 5929, 17690, 6188, 7977, 4461, 28724, 27784, 18009, 64198, 92957, 10102, 3629, 42528, 12784, 7886, 2793, 3543, 2008, 7535, 10102, 5782, 28724, 98122, 2025, 17690, 6188, 2793, 3232, 4461, 2938, 21668, 2959, 4461, 28724, 2959, 10672, 5863, 3171, 2938, 17690, 6188, 63858, 10672, 5988, 11802, 4855, 7843, 12702, 5863, 3984, 62169, 10028, 5988, 4957, 17690, 6188, 9574, 42822, 6288, 83803, 5620, 10672, 70096, 2805, 6820, 9050, 62169, 22367, 28075, 9258, 4855, 12139, 17690, 6188, 11464, 71030, 3778, 17420, 5019, 20516, 17802, 4856, 4613, 45991, 18009, 5929, 24215, 74701, 34340, 20936, 2867, 16945, 2823, 18009, 45991, 28531, 28724, 54176, 2032, 3629, 7349, 71030, 90857, 26162, 92957, 8029, 40098, 71030, 3778, 15968, 23344, 32717, 26874, 69383, 40006, 18009, 5929, 14335, 71030, 3778, 4183, 71030, 3778, 15968, 23344, 6251, 32023, 40098, 17690, 6188, 10214, 10595, 96255, 34340, 20936, 4568, 28724, 17690, 6188, 6597, 8513, 28724, 4461, 4459, 3171, 5929, 4869, 5673, 18009, 26162, 92957, 10429, 4613, 5739, 4366, 5929, 4884, 4735, 4366, 5929, 3191, 50317, 4366, 5929, 7644, 10595, 6260, 3191, 7830, 40856, 2017, 3318, 4390, 35284, 7535, 35284, 48879, 98122, 2025, 4568, 2793, 8320, 17690, 6188, 7535, 15988, 82418, 6201, 98122, 2025, 5929, 48879, 5929, 1038, 3891, 98122, 2025, 63432, 5257, 1039, 3891, 48879, 36132, 5589, 4459, 35772, 3891, 7093, 15988, 82418, 6201, 7093, 15988, 82418, 6201, 24215, 7535, 28724, 48879, 4183, 7093, 15988, 82418, 6201, 4459, 13135, 21083, 2021, 32834, 15407, 4157, 3971, 6602, 17797, 17176, 2843, 6062, 39230, 5358, 2793, 69964, 32602, 14314, 12139, 4, 0, 0, 0, 0, 0, 0], [2, 8943, 7563, 9345, 4207, 5929, 4259, 7563, 2927, 3531, 19082, 10884, 5442, 6471, 2959, 3171, 5929, 5740, 7238, 14487, 45650, 11163, 21989, 4855, 10777, 4825, 7440, 5882, 5238, 12707, 6712, 9793, 70767, 9551, 6023, 3625, 5839, 6762, 3361, 3171, 5929, 4884, 7563, 4207, 5929, 4259, 11396, 2871, 3145, 49295, 3329, 5392, 7647, 11030, 4463, 4207, 5929, 3329, 4259, 7563, 2927, 8943, 3318, 4816, 4207, 5929, 4259, 2793, 70933, 5823, 6578, 3347, 2959, 4795, 6037, 5307, 3171, 5929, 4816, 3567, 24169, 5929, 5939, 5929, 7207, 4795, 3821, 3435, 4825, 4206, 5929, 7417, 7563, 10423, 5856, 2867, 2793, 3168, 45716, 2811, 3171, 5929, 16389, 3531, 3731, 4795, 4816, 6578, 2959, 2927, 7563, 6637, 4630, 4532, 4108, 7034, 6637, 9330, 4301, 5108, 4723, 4532, 9585, 5929, 4218, 9330, 21910, 10804, 5962, 5407, 11546, 4635, 5407, 4301, 5108, 5373, 4855, 4431, 4431, 5929, 10036, 6729, 16204, 4366, 5929, 4603, 4825, 3552, 4825, 4206, 5929, 4431, 5929, 3329, 3552, 4816, 3666, 4635, 5407, 5929, 3731, 4795, 6578, 44228, 14418, 2959, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 16409, 6293, 10631, 6629, 36132, 13093, 4219, 13826, 5929, 3810, 22788, 2910, 6072, 6075, 4972, 12555, 4913, 2910, 2793, 12945, 28347, 7727, 8145, 5929, 5256, 7727, 2867, 5929, 2793, 4820, 13459, 5929, 3821, 4820, 3777, 20678, 8757, 6468, 7171, 4484, 2867, 10166, 6537, 5484, 6428, 5929, 4484, 74292, 42661, 4723, 2867, 5929, 54052, 4710, 3469, 8055, 10622, 4710, 5929, 3944, 47022, 4484, 3972, 5484, 6823, 5929, 3972, 4256, 5929, 7086, 21582, 15751, 2867, 6110, 78930, 15227, 8473, 2793, 31149, 3511, 3972, 14532, 8366, 19469, 79317, 4484, 6079, 10166, 3271, 7775, 10153, 67758, 2910, 2793, 4820, 12126, 5209, 3841, 4378, 2793, 5655, 2867, 3736, 5929, 5020, 15751, 5351, 70122, 3736, 5929, 3972, 3057, 9083, 3027, 2822, 2793, 56608, 4183, 4790, 4723, 2867, 10153, 2938, 17358, 6428, 19330, 5330, 11682, 4790, 6428, 5929, 22683, 13340, 4241, 8757, 3864, 18005, 5929, 4209, 70122, 13719, 4370, 5929, 40528, 75904, 5341, 2793, 31149, 6428, 20848, 11017, 4723, 2867, 5929, 10415, 4157, 5101, 3351, 7727, 3057, 7216, 3559, 28089, 2793, 3349, 3057, 5926, 4664, 5209, 11697, 15229, 2793, 4393, 3469, 5020, 3251, 3057, 9866, 2793, 9933, 4275, 3145, 39709, 16875, 3310, 4485, 11672, 63546, 6111, 4992, 51041, 5890, 8408, 9866, 17007, 3310, 6895, 3552, 3666, 79317, 7175, 6429, 13792, 6428, 9866, 6537, 4484, 5929, 23763, 4274, 5929, 13432, 5762, 12537, 2959, 4274, 5929, 9660, 4484, 24207, 7078, 5938, 2822, 5540, 3444, 6537, 6428, 5929, 2867, 7560, 4219, 11307, 6428, 5929, 7560, 6839, 9708, 3821, 3939, 6537, 5540, 3777, 5207, 2793, 15110, 2793, 76050, 3552, 2793, 63913, 17177, 5929, 6079, 2793, 4820, 2867, 7727, 9182, 4723, 2867, 5929, 4484, 5929, 4613, 25643, 2793, 5276, 6428, 7727, 4386, 2867, 22683, 30941, 8290, 5977, 3211, 6428, 5175, 3884, 6428, 6757, 3821, 5154, 4613, 6428, 7727, 5754, 3229, 5929, 4723, 2867, 5929, 7223, 2793, 6501, 2013, 10336, 13719, 3229, 5714, 6428, 4484, 5929, 3821, 10941, 70122, 6428, 17358, 3559, 85082, 31448, 9455, 3351, 6428, 5428, 4723, 2867, 5929, 6766, 5473, 4992, 19330, 3841, 4251, 6428, 5929, 3444, 2867, 5929, 15950, 8661, 2867, 5929, 15950, 70122, 11667, 2009, 4630, 2867, 2793, 10941, 11672, 4992, 5520, 6428, 5520, 3422, 5929, 36132, 6177, 5977, 3559, 3841, 3598, 4723, 2867, 5929, 3598, 6428, 6949, 8122, 4106, 5392, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 5071, 6049, 13956, 8536, 21813, 53276, 10215, 72470, 5321, 3419, 5929, 3329, 12035, 6205, 6774, 6774, 14322, 37027, 22504, 58235, 23195, 46558, 3318, 4906, 4007, 67682, 3736, 2959, 11940, 13956, 8536, 5127, 5426, 4007, 13359, 4804, 6904, 6308, 18821, 10763, 7977, 4906, 5929, 9034, 7449, 5444, 6308, 67682, 5127, 13956, 8536, 3736, 12102, 2938, 13359, 18821, 4318, 4906, 5929, 3970, 3088, 10301, 4598, 11940, 3229, 3229, 4343, 6049, 13956, 8536, 6049, 13956, 8536, 6668, 4613, 6049, 3855, 5929, 11940, 5935, 4906, 4007, 12102, 5929, 6316, 3229, 4906, 10301, 4459, 12102, 5929, 4906, 10301, 12102, 2938, 4007, 13359, 3229, 6573, 4157, 7449, 4157, 7449, 3855, 7014, 2959, 4157, 7449, 3855, 7014, 2959, 3821, 5355, 4007, 13359, 2938, 7449, 2910, 13359, 4007, 4906, 7449, 3167, 3088, 7449, 3821, 25408, 13956, 8536, 2959, 4157, 4108, 3251, 5321, 14604, 4856, 14604, 3625, 3318, 13699, 5001, 6312, 76074, 3996, 5929, 4942, 4183, 4716, 14604, 5935, 9678, 14604, 4613, 11104, 4455, 3211, 76074, 3996, 5929, 5935, 6049, 19079, 76074, 3996, 5929, 11562, 6049, 13956, 8536, 19079, 4605, 3329, 12035, 10822, 47796, 74191, 4605, 3329, 12035, 10822, 5935, 6404, 76074, 3996, 5929, 4942, 3043, 6904, 7782, 4062, 9089, 3251, 5321, 6049, 3329, 12035, 10822, 3625, 3318, 9678, 4455, 6573, 5127, 13956, 8536, 4196, 5850, 4188, 3911, 5929, 4360, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6012, 5929, 5402, 6597, 9484, 15710, 3469, 58212, 14322, 5369, 6012, 3835, 58212, 22504, 8473, 3419, 22504, 58212, 15710, 3419, 3841, 4319, 4089, 8473, 9484, 7164, 15710, 3114, 3251, 4909, 58212, 22504, 3211, 15710, 9484, 5929, 24376, 18481, 23000, 5350, 9258, 6251, 4431, 5929, 4855, 2871, 1442, 4816, 10393, 3552, 6492, 5929, 19626, 3490, 24376, 18481, 23000, 5350, 22504, 4319, 3027, 3145, 3310, 5318, 98350, 9087, 10146, 9854, 51719, 45294, 4850, 3893, 2793, 5602, 6492, 5929, 3469, 9854, 51719, 10306, 6947, 3114, 9258, 5929, 7863, 3280, 3027, 3145, 15799, 15799, 5929, 3329, 7069, 1442, 6628, 62721, 47176, 9854, 51719, 10306, 3948, 9484, 5929, 15710, 6947, 5929, 3114, 9258, 58212, 22504, 15710, 3469, 4680, 1442, 3250, 3764, 5929, 24376, 18481, 23000, 5350, 3948, 7069, 1442, 6628, 4209, 8279, 1442, 9484, 2959, 15799, 9854, 51719, 10306, 4207, 1442, 1457, 1393, 1457, 6492, 1442, 4732, 10146, 9854, 51719, 45294, 4850, 4207, 1442, 1457, 1393, 1459, 3419, 22504, 4241, 5358, 5254, 6062, 6895, 11912, 3251, 1442, 6012, 5655, 3030, 2867, 6335, 5929, 2867, 6325, 2938, 4856, 3329, 3561, 7069, 1442, 6628, 3499, 3329, 7069, 1442, 6628, 4367, 6500, 6628, 5929, 4912, 3905, 2867, 36655, 20984, 41250, 4160, 2904, 3939, 3419, 10146, 9854, 51719, 45294, 4850, 5399, 3329, 20984, 41250, 15766, 4691, 4680, 1442, 4912, 8012, 13440, 7965, 13597, 96913, 3905, 6037, 4175, 4431, 3728, 2904, 3073, 11409, 1442, 12494, 5929, 3434, 5929, 3885, 4299, 4431, 5929, 13597, 6628, 8269, 7038, 3905, 2867, 3201, 9324, 6628, 9718, 9823, 3280, 3318, 13597, 6628, 6037, 29252, 5240, 6224, 6288, 39230, 13597, 3905, 6335, 2904, 3073, 3905, 4855, 15766, 76008, 30577, 20844, 95380, 2025, 72786, 3905, 5988, 15710, 4855, 32717, 5785, 3364, 9484, 6604, 9258, 7273, 6310, 1442, 5238, 3473, 4206, 7273, 7152, 5988, 9484, 4090, 3043, 4214, 4090, 3213, 44821, 4049, 3972, 4431, 15710, 4214, 10102, 5929, 4090, 7273, 9484, 9992, 5094, 4090, 3043, 4214, 2967, 95698, 4214, 8724, 8999, 4214, 6496, 3813, 4090, 6310, 1442, 5238, 9484, 4336, 1442, 4431, 3528, 3171, 2904, 3073, 4723, 2938, 3813, 4090, 4497, 4482, 4497, 4723, 2938, 5902, 1442, 6339, 5988, 9484, 4049, 15710, 4482, 8473, 4723, 21630, 13615, 8473, 58212, 22504, 15710, 58212, 24689, 23344, 3647, 4613, 4855, 3552, 4905, 5929, 24954, 9258, 15710, 4157, 5503, 6597, 9484, 6667, 3469, 58212, 12131, 5929, 7342, 4816, 8745, 3469, 55287, 3625, 1442, 23180, 58212, 5929, 7152, 9484, 15490, 3088, 8646, 4613, 4735, 5612, 4319, 6597, 9484, 12271, 5330, 6878, 12131, 12682, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3951, 5402, 4274, 5929, 4386, 3951, 7075, 4605, 1876, 3064, 3951, 4893, 27145, 11827, 4825, 4639, 12139, 6012, 3951, 20240, 2793, 21224, 17143, 16576, 56619, 8707, 4957, 46765, 3951, 33670, 6173, 19910, 4013, 6545, 6012, 2793, 9073, 2013, 11827, 7563, 6839, 57730, 4157, 72620, 28612, 13093, 3951, 6012, 4157, 3528, 26702, 5574, 7043, 5398, 4549, 70122, 45309, 26851, 14335, 32023, 54770, 18009, 26162, 63093, 8811, 4869, 3404, 4664, 11231, 45779, 32023, 11231, 3561, 5503, 14335, 2793, 14114, 14335, 32023, 32023, 5402, 35088, 20984, 41250, 20984, 41250, 3329, 40097, 35514, 10672, 24101, 2793, 69964, 15335, 24801, 6201, 32023, 3791, 2867, 20984, 41250, 4613, 38644, 54770, 18009, 30675, 5028, 26040, 93141, 71030, 3778, 22869, 47348, 63093, 8811, 2867, 22869, 47348, 63093, 8811, 5319, 28724, 5732, 71030, 3778, 5929, 3152, 2926, 3211, 11802, 52801, 70018, 7843, 68120, 68120, 68120, 2026, 2793, 14114, 76662, 32023, 10672, 71030, 3778, 93141, 71030, 3778, 22869, 17374, 7843, 11802, 52801, 70018, 32023, 10672, 5884, 3027, 57637, 40098, 32023, 4884, 12160, 38644, 4386, 4485, 20984, 41250, 32023, 5402, 32023, 4241, 3116, 2043, 4461, 47348, 63093, 8811, 4183, 4461, 28724, 38644, 5444, 5935, 4461, 2959, 2867, 3229, 12675, 4863, 38329, 15716, 24664, 7843, 5196, 6917, 3531, 71030, 3778, 3499, 7473, 4237, 10207, 10332, 20640, 2002, 11231, 7579, 17690, 6188, 4613, 3088, 38644, 82680, 3399, 5929, 91087, 42565, 38644, 41862, 38636, 38188, 19538, 63807, 38636, 38188, 19538, 9545, 10745, 38636, 38188, 19538, 14805, 4261, 38636, 38188, 19538, 20516, 17690, 6188, 15988, 82418, 6201, 29111, 63093, 8811, 3968, 5166, 10672, 24718, 71030, 3778, 22157, 3972, 31353, 29068, 3984, 5988, 17143, 16576, 4905, 11704, 39703, 23024, 54770, 71030, 3778, 71030, 3778, 5929, 22157, 3592, 26325, 6012, 13441, 3951, 6591, 38984, 4183, 17143, 16576, 4484, 5929, 5321, 5565, 22869, 41862, 5806, 54936, 2025, 84506, 2015, 17294, 3511, 3911, 22869, 26127, 4369, 2006, 10672, 6012, 47348, 63093, 8811, 47348, 38515, 60193, 41862, 38515, 7075, 47348, 84480, 20021, 2040, 4386, 3951, 5402, 3951, 47348, 84480, 20021, 2040, 4836, 19910, 5458, 6012, 6907, 54545, 35514, 4836, 47348, 14132, 31481, 47348, 52540, 2041, 41250, 7947, 3944, 14183, 6012, 14183, 22869, 5929, 18025, 5929, 22869, 21825, 22869, 47348, 38515, 60193, 41862, 38515, 47348, 25961, 52288, 21949, 14105, 14814, 4836, 9231, 11912, 6012, 3777, 32602, 2816, 5850, 12139, 4219, 7560, 8707, 4957, 17143, 16576, 6012, 56619, 14233, 12473, 3777, 5166, 44418, 21503, 2793, 4373, 2015, 14233, 55341, 3777, 10992, 31193, 6012, 2793, 4373, 2015, 7366, 17521, 6386, 7563, 3777, 6386, 11827, 13343, 5929, 5254, 92582, 2015, 3511, 74701, 90320, 2025, 6012, 2793, 4373, 2015, 3951, 6767, 4, 0, 0, 0], [2, 58164, 6629, 7649, 5929, 3841, 17802, 30133, 10199, 2011, 6343, 3419, 14322, 6012, 1442, 10878, 14322, 30133, 10199, 2011, 5929, 29256, 67429, 2032, 5563, 34451, 45309, 7649, 12102, 1442, 7649, 5929, 8718, 62379, 4261, 4568, 12570, 47975, 5540, 7615, 14324, 6628, 18975, 5321, 21939, 13129, 10102, 12702, 43123, 4869, 5673, 6312, 4912, 10884, 7332, 3444, 9615, 45611, 7649, 46178, 6878, 3984, 22827, 86706, 6379, 11940, 2793, 14114, 74137, 2793, 7453, 25003, 4183, 9706, 5732, 96402, 13334, 4907, 86706, 10884, 4568, 45309, 8700, 5732, 14235, 11231, 4907, 43137, 10672, 96402, 13334, 2008, 7114, 34169, 5732, 12322, 3367, 14114, 6312, 5321, 54918, 5732, 74137, 4106, 85861, 5732, 10884, 4568, 5732, 15307, 86706, 5929, 11231, 14235, 4907, 28807, 86706, 5754, 4241, 12702, 6312, 29111, 63093, 8811, 26162, 63093, 8811, 8012, 23998, 3647, 29111, 7304, 10102, 28724, 22827, 4459, 6312, 92084, 2033, 6312, 43137, 86706, 5929, 3731, 5754, 4869, 37800, 5732, 4568, 5732, 68539, 9465, 11163, 2025, 12300, 28724, 68539, 5732, 29645, 9465, 11163, 2025, 12300, 5444, 28724, 29645, 4459, 5732, 38621, 9465, 11163, 2025, 12300, 5444, 28724, 23851, 4075, 5732, 5929, 5732, 86706, 12322, 6329, 6312, 43137, 3211, 4224, 28228, 5321, 7623, 5556, 8012, 51484, 88874, 15802, 32929, 4241, 4884, 86706, 22827, 6312, 15936, 43082, 9465, 11163, 2025, 43811, 31487, 86706, 5929, 9615, 14114, 29111, 14543, 15189, 8811, 31487, 43137, 86706, 8197, 3285, 15879, 34885, 5732, 86706, 5732, 39697, 3852, 10672, 45309, 12844, 6312, 43137, 31487, 86706, 5145, 17420, 4869, 5473, 17420, 6312, 26162, 73589, 2049, 22551, 74137, 86706, 3083, 6347, 4630, 7152, 74137, 5732, 37288, 45309, 8700, 6312, 51484, 10102, 5641, 2014, 86706, 44955, 45243, 5754, 86706, 57384, 4241, 12702, 6312, 88874, 15802, 32929, 86706, 5754, 4884, 12702, 9032, 8012, 6378, 8220, 4224, 3329, 41747, 24075, 74529, 24309, 2059, 19791, 6312, 26029, 53401, 14747, 10741, 69863, 3083, 86706, 52394, 4869, 37800, 7539, 2783, 44594, 5732, 86706, 5019, 3280, 6312, 26162, 73589, 2049, 22551, 86706, 5796, 74137, 4241, 12702, 6312, 43137, 4336, 1442, 21033, 2053, 74137, 35814, 3731, 12139, 69521, 3083, 3731, 45243, 4336, 1442, 14745, 69863, 4907, 86706, 5796, 4237, 12702, 7623, 5556, 37288, 4241, 3150, 6831, 24025, 8700, 25520, 22551, 5796, 4241, 6312, 26029, 53401, 14747, 10741, 5706, 2827, 4237, 12702, 86706, 57384, 5706, 2827, 3552, 19791, 6945, 10884, 10672, 4183, 5540, 5321, 36091, 3692, 55341, 3628, 31152, 2783, 43428, 27060, 31087, 5684, 42476, 31481, 22323, 56548, 24307, 65598, 11232, 43428, 27060, 31087, 5684, 42476, 31481, 22323, 56548, 24307, 65598, 11232, 30784, 27945, 9465, 11163, 2025, 12300, 57246, 56549, 82418, 27827, 2025, 2002, 2031, 39097, 2032, 2078, 2049, 2002, 2039, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"uF3FFsPzc6zD","executionInfo":{"status":"ok","timestamp":1610703918933,"user_tz":-210,"elapsed":17829,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["sentence_maxlen=128"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m2Qc2IkrnEp","executionInfo":{"status":"ok","timestamp":1610703937241,"user_tz":-210,"elapsed":35788,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"0717cbc2-750e-4354-c11d-1a307afa7f17"},"source":["##Tokenize training and validation sentences:\r\n","train_encodings = tokenizer.batch_encode_plus(X_train,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","val_encodings = tokenizer.batch_encode_plus(X_val,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","test_encodings=tokenizer.batch_encode_plus(X_test,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IwjkXARbetX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610703937243,"user_tz":-210,"elapsed":35235,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"81923b06-6643-4665-b18e-31b9518d1d4e"},"source":["train_encodings[0]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"-iCp2PUEupYK","executionInfo":{"status":"ok","timestamp":1610703937244,"user_tz":-210,"elapsed":34956,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","\r\n","# for train set\r\n","train_seq = torch.tensor(train_encodings['input_ids'])\r\n","train_mask = torch.tensor(train_encodings['attention_mask'])\r\n","train_y = torch.tensor(y_train)\r\n","\r\n","# for validation set\r\n","val_seq = torch.tensor(val_encodings['input_ids'])\r\n","val_mask = torch.tensor(val_encodings['attention_mask'])\r\n","val_y = torch.tensor(y_val)\r\n","\r\n","# for test set\r\n","test_seq = torch.tensor(test_encodings['input_ids'])\r\n","test_mask = torch.tensor(test_encodings['attention_mask'])\r\n","test_y = torch.tensor(y_test)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0JkQbxVBmbM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610703937244,"user_tz":-210,"elapsed":34733,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"007be155-d8fa-46ee-c7d9-9fc85daa97a0"},"source":["train_y[0]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"T2xiV6Nb0ddZ","executionInfo":{"status":"ok","timestamp":1610703937245,"user_tz":-210,"elapsed":34305,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","#define a batch size\r\n","batch_size = 32\r\n","\r\n","# wrap tensors\r\n","train_data = TensorDataset(train_seq, train_mask, train_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","train_sampler = RandomSampler(train_data)\r\n","\r\n","# dataLoader for train set\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","val_data = TensorDataset(val_seq, val_mask, val_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","val_sampler = SequentialSampler(val_data)\r\n","\r\n","# dataLoader for validation set\r\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","test_data = TensorDataset(test_seq, test_mask, test_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","test_sampler = SequentialSampler(test_data)\r\n","\r\n","# dataLoader for validation set\r\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"UwGHXIjGfmaN","colab":{"base_uri":"https://localhost:8080/","height":749,"referenced_widgets":["4fd50aa1e02c4cf99cfab8cd1ab3a9d6","681d35f5a344403d917aaec0dd4e8e02","0ab5157be9b64419b4f6cdf0cd86ad0e","aa17575627c146419f0179ea8f9e2e75","9906a0b34aa0444ca8c6fc965b8d17b2","23cb06a3ab584367ad4609216f9b41f6","db51ffddccbf47d48b53ffd344525a21","6bbdcc8602f14d5481910d40fb79d1a0"]},"executionInfo":{"status":"ok","timestamp":1610703958329,"user_tz":-210,"elapsed":55105,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"b55f1a48-3c98-4ae3-cfaf-386cd539df84"},"source":["# example\r\n","\r\n","\r\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n","\r\n","# encode text\r\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\r\n","print(sent_id)\r\n","\r\n","seq = torch.tensor(sent_id['input_ids'])\r\n","mask = torch.tensor(sent_id['attention_mask'])\r\n","train_y = torch.tensor([0,1])\r\n","\r\n","transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")\r\n","cls_hs=transformer_model(seq,mask)\r\n","print(cls_hs)\r\n","print(cls_hs[0])\r\n","print(cls_hs[1])\r\n","print(cls_hs[1].shape)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["{'input_ids': [[2, 32071, 9574, 1026, 89390, 36260, 84378, 40908, 2041, 4, 0], [2, 13632, 25909, 70608, 1011, 40716, 2033, 1026, 89390, 36260, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fd50aa1e02c4cf99cfab8cd1ab3a9d6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=651477729.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>)\n","tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>)\n","torch.Size([2, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByUEn_v4zknn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"n3AjEaHcEMfb","executionInfo":{"status":"ok","timestamp":1610703962101,"user_tz":-210,"elapsed":57945,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaAlYydhxPTd","executionInfo":{"status":"ok","timestamp":1610703962104,"user_tz":-210,"elapsed":57742,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# freeze all the parameters\r\n","for param in transformer_model.parameters():\r\n","    param.requires_grad = False"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUa1R1WQONe6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610703962105,"user_tz":-210,"elapsed":57544,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"db5810fe-f2a7-477a-bb29-a439b32da90d"},"source":["len(labels)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"oyE_ThEms5aZ","executionInfo":{"status":"ok","timestamp":1610703962105,"user_tz":-210,"elapsed":57337,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["class BERT_Arch(nn.Module):\r\n","\r\n","    def __init__(self, bert):\r\n","      \r\n","      super(BERT_Arch, self).__init__()\r\n","\r\n","      self.bert = bert \r\n","      \r\n","      # dropout layer\r\n","      self.dropout = nn.Dropout(0.1)\r\n","      \r\n","      # relu activation function\r\n","      self.relu =  nn.ReLU()\r\n","\r\n","      # dense layer 1\r\n","      self.fc1 = nn.Linear(768,512)\r\n","      \r\n","      # dense layer 2 (Output layer)\r\n","      self.fc2 = nn.Linear(512,256)\r\n","\r\n","      # dense layer 3 (Output layer)\r\n","      self.fc3 = nn.Linear(256,78)\r\n","\r\n","      #sigmoid activation function\r\n","      self.sigmoid = nn.Sigmoid()\r\n","\r\n","    #define the forward pass\r\n","    def forward(self, sent_id, mask):\r\n","\r\n","      #pass the inputs to the model  \r\n","      cls_hs = self.bert(sent_id, attention_mask=mask)\r\n","      \r\n","      x = self.fc1(cls_hs[1])\r\n","\r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      # output layer\r\n","      x = self.fc2(x)\r\n","      x = self.relu(x)\r\n","      x = self.fc3(x)\r\n","      \r\n","      # apply sigmoid activation\r\n","      x = self.sigmoid(x)\r\n","\r\n","      return x"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDuHzo96z6z8","executionInfo":{"status":"ok","timestamp":1610703976797,"user_tz":-210,"elapsed":71853,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(transformer_model)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUNSLBYcLc9q","executionInfo":{"status":"ok","timestamp":1610703976798,"user_tz":-210,"elapsed":71677,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArHmwhh7JrZh","executionInfo":{"status":"ok","timestamp":1610703976799,"user_tz":-210,"elapsed":71448,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["loss_func =nn.MultiLabelSoftMarginLoss()"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8LjQyDXs0bG","executionInfo":{"status":"ok","timestamp":1610703976799,"user_tz":-210,"elapsed":71253,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function to train the model\r\n","def train():\r\n","  \r\n","  model.train()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save model predictions\r\n","  total_preds=[]\r\n","  \r\n","  # iterate over batches\r\n","  for step,batch in enumerate(train_dataloader):\r\n","    \r\n","    # progress update after every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [r.to(device) for r in batch]\r\n"," \r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # clear previously calculated gradients \r\n","    model.zero_grad()        \r\n","\r\n","    # get model predictions for the current batch\r\n","    preds = model(sent_id, mask)\r\n","\r\n","    # compute the loss between actual and predicted values\r\n","    \r\n","    loss = loss_func(preds, labels)\r\n","    # add on to the total loss\r\n","    total_loss = total_loss + loss.item()\r\n","\r\n","    # backward pass to calculate the gradients\r\n","    loss.backward()\r\n","\r\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","    # update parameters\r\n","    optimizer.step()\r\n","\r\n","    # model predictions are stored on GPU. So, push it to CPU\r\n","    preds=preds.detach().cpu().numpy()\r\n","\r\n","    # append the model predictions\r\n","    total_preds.append(preds)\r\n","\r\n","  # compute the training loss of the epoch\r\n","  avg_loss = total_loss / len(train_dataloader)\r\n","  \r\n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  #returns the loss and predictions\r\n","  return avg_loss, total_preds"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNBRQo9WMHey","executionInfo":{"status":"ok","timestamp":1610703976800,"user_tz":-210,"elapsed":71069,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function for evaluating the model\r\n","def evaluate():\r\n","  \r\n","  print(\"\\nEvaluating...\")\r\n","  \r\n","  # deactivate dropout layers\r\n","  model.eval()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save the model predictions\r\n","  total_preds = []\r\n","\r\n","  # iterate over batches\r\n","  for step,batch in enumerate(val_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","\r\n","      # compute the validation loss between actual and predicted values\r\n","      loss = loss_func(preds,labels)\r\n","\r\n","      total_loss = total_loss + loss.item()\r\n","\r\n","      preds = preds.detach().cpu().numpy()\r\n","\r\n","      total_preds.append(preds)\r\n","\r\n","  # compute the validation loss of the epoch\r\n","  avg_loss = total_loss / len(val_dataloader) \r\n","\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  return avg_loss, total_preds"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qu5pfrJKtTc0","executionInfo":{"status":"ok","timestamp":1610705360292,"user_tz":-210,"elapsed":1454336,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"eaae1c9b-a234-4dcf-8809-f14334255aa1"},"source":["# number of training epochs\r\n","epochs = 10\r\n","\r\n","# set initial loss to infinite\r\n","best_valid_loss = float('inf')\r\n","\r\n","# empty lists to store training and validation loss of each epoch\r\n","train_losses=[]\r\n","valid_losses=[]\r\n","\r\n","#for each epoch\r\n","for epoch in range(epochs):\r\n","     \r\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n","    \r\n","    #train model\r\n","    train_loss, _ = train()\r\n","    \r\n","    #evaluate model\r\n","    valid_loss, _ = evaluate()\r\n","    \r\n","    #save the best model\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'saved_weights.pt')\r\n","    \r\n","    # append training and validation loss\r\n","    train_losses.append(train_loss)\r\n","    valid_losses.append(valid_loss)\r\n","    \r\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.960\n","Validation Loss: 0.958\n","\n"," Epoch 2 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.954\n","Validation Loss: 0.950\n","\n"," Epoch 3 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.944\n","Validation Loss: 0.937\n","\n"," Epoch 4 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.924\n","Validation Loss: 0.908\n","\n"," Epoch 5 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.879\n","Validation Loss: 0.847\n","\n"," Epoch 6 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.805\n","Validation Loss: 0.771\n","\n"," Epoch 7 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.745\n","Validation Loss: 0.731\n","\n"," Epoch 8 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.720\n","Validation Loss: 0.715\n","\n"," Epoch 9 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.710\n","Validation Loss: 0.708\n","\n"," Epoch 10 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.705\n","Validation Loss: 0.705\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GlJpADKkIOqX"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"nVrfkSoKIOIV","executionInfo":{"status":"ok","timestamp":1610705363820,"user_tz":-210,"elapsed":1456941,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["path = 'saved_weights_dense_3_SGD.pt'\n","torch.save(model.state_dict(), path)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ2_aS0zCLvp"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"cvR-FhPpuLkR"},"source":["# torch.cuda.empty_cache()\r\n","# pass the pre-trained BERT to our define architecture\r\n","model = BERT_Arch(transformer_model)\r\n","\r\n","# push the model to GPU\r\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aOPRZ2jVvNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610538546884,"user_tz":-210,"elapsed":1935433,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"c4aa9dad-58f0-4197-90b2-72607cbaed8e"},"source":["#load weights of best model\r\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"PM1uUcZFCPVg"},"source":["After loading model:"]},{"cell_type":"code","metadata":{"id":"XZhHObMnzuws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610705399529,"user_tz":-210,"elapsed":35682,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"a8deff7b-592e-43ca-997d-2610cd4e7698"},"source":["y_pred=[]\r\n","y_true=[]\r\n","for step,batch in enumerate(test_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","      # print(preds)\r\n","      # print(preds.cpu().numpy())\r\n","      preds = preds.cpu().numpy()\r\n","      # model's performance\r\n","    # preds = numpy.argmax(preds, axis = 1)\r\n","    \r\n","    measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    for l in preds:\r\n","      temp=[]\r\n","      for value in l:\r\n","        if value >= measure:\r\n","          temp.append(1)\r\n","        else:\r\n","          temp.append(0)\r\n","      y_pred.append(temp)\r\n","    y_true.extend(labels.cpu().numpy())\r\n","    # print(labels.cpu().numpy()[0], preds[0])\r\n","print(classification_report(y_true, y_pred))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["  Batch    50  of    135.\n","  Batch   100  of    135.\n","              precision    recall  f1-score   support\n","\n","           0       0.03      0.52      0.06       132\n","           1       0.03      0.46      0.06       146\n","           2       0.02      0.15      0.04       151\n","           3       0.00      0.35      0.01        17\n","           4       0.02      0.41      0.04        78\n","           5       0.00      0.08      0.00        12\n","           6       0.03      0.24      0.05       153\n","           7       0.01      0.40      0.01        43\n","           8       0.00      0.31      0.01        26\n","           9       0.02      0.35      0.04       101\n","          10       0.03      0.26      0.05       167\n","          11       0.02      0.37      0.03        76\n","          12       0.05      0.42      0.09       171\n","          13       0.00      0.25      0.00        12\n","          14       0.00      0.17      0.00        23\n","          15       0.03      0.50      0.05       145\n","          16       0.02      0.26      0.04       127\n","          17       0.05      0.56      0.09       158\n","          18       0.02      0.51      0.04       108\n","          19       0.01      0.23      0.02        57\n","          20       0.03      0.33      0.06       133\n","          21       0.03      0.20      0.05       148\n","          22       0.01      0.47      0.02        38\n","          23       0.02      0.17      0.04       161\n","          24       0.01      0.20      0.02        66\n","          25       0.05      0.52      0.09       123\n","          26       0.02      0.24      0.04       155\n","          27       0.03      0.26      0.05       146\n","          28       0.03      0.44      0.05       144\n","          29       0.03      0.45      0.05       154\n","          30       0.02      0.34      0.03        62\n","          31       0.03      0.47      0.06        98\n","          32       0.03      0.39      0.06       138\n","          33       0.04      0.61      0.07       167\n","          34       0.02      0.34      0.04        93\n","          35       0.02      0.29      0.05       157\n","          36       0.04      0.67      0.07       153\n","          37       0.01      0.41      0.02        69\n","          38       0.01      0.51      0.03        51\n","          39       0.00      0.22      0.01        27\n","          40       0.03      0.24      0.05       153\n","          41       0.01      0.11      0.01       110\n","          42       0.02      0.44      0.03        82\n","          43       0.03      0.40      0.05       159\n","          44       0.02      0.37      0.04       136\n","          45       0.00      0.25      0.01        24\n","          46       0.00      0.28      0.01        25\n","          47       0.02      0.28      0.04       141\n","          48       0.01      0.08      0.02       155\n","          49       0.02      0.35      0.04        75\n","          50       0.00      0.07      0.00        29\n","          51       0.00      0.45      0.01        22\n","          52       0.01      0.57      0.03        44\n","          53       0.02      0.27      0.04        98\n","          54       0.01      0.13      0.02       137\n","          55       0.01      0.28      0.02        32\n","          56       0.02      0.26      0.03        53\n","          57       0.02      0.53      0.03        60\n","          58       0.05      0.61      0.10       132\n","          59       0.03      0.31      0.05       150\n","          60       0.06      0.54      0.11       142\n","          61       0.01      0.61      0.02        38\n","          62       0.03      0.32      0.05       162\n","          63       0.02      0.20      0.04       181\n","          64       0.01      0.10      0.02       102\n","          65       0.02      0.16      0.03       146\n","          66       0.01      0.50      0.03        32\n","          67       0.02      0.25      0.05       101\n","          68       0.04      0.91      0.08       143\n","          69       0.03      0.52      0.06       165\n","          70       0.03      0.29      0.06       156\n","          71       0.03      0.27      0.06       149\n","          72       0.04      0.33      0.06       134\n","          73       0.00      0.19      0.01        37\n","          74       0.00      0.19      0.01        26\n","          75       0.03      0.28      0.05       141\n","          76       0.01      0.39      0.03        54\n","          77       0.04      0.46      0.07       107\n","\n","   micro avg       0.02      0.36      0.04      8019\n","   macro avg       0.02      0.35      0.04      8019\n","weighted avg       0.03      0.36      0.05      8019\n"," samples avg       0.02      0.39      0.03      8019\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7ytQvgyzHt8v"},"source":[""],"execution_count":null,"outputs":[]}]}