{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ParsBERT_pytorch_Dense_3.ipynb","provenance":[{"file_id":"1k6QLcPAJKT5H2yTy61U-YCB267inolmb","timestamp":1610512876213},{"file_id":"1AIm-KimERCJutlpyjiZ2IAwM-cCrVsWM","timestamp":1610440221899},{"file_id":"17mqUcShahUjZjQxywgKQSGU1jV-CZW8o","timestamp":1610336820188},{"file_id":"1FgtzYXY0CXNyE_2FU4IEqJTQzmVZNvDh","timestamp":1610105938882}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bc54661597404315b0ab9f5d3d1a4702":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a6c08ee082bc4224a18f8ff58f4be312","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fa95a752e3b84dc7a0410d044e08f2da","IPY_MODEL_8a9e2da5764c4c8a816d51e2244d0306"]}},"a6c08ee082bc4224a18f8ff58f4be312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fa95a752e3b84dc7a0410d044e08f2da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_52c94e7c00a44fcaaf62e1f2718e7f0f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1441,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1441,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d1926e7f87a0465ea93b3df03d642966"}},"8a9e2da5764c4c8a816d51e2244d0306":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8850064f4fb04af19071683cf1cbf4b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.44k/1.44k [00:06&lt;00:00, 239B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_52b300d2b2344515be8c13e95f81a02d"}},"52c94e7c00a44fcaaf62e1f2718e7f0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d1926e7f87a0465ea93b3df03d642966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8850064f4fb04af19071683cf1cbf4b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"52b300d2b2344515be8c13e95f81a02d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0e2f0b1ee664358a8c389237459c3e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ba635652360e4a3caad29fbf591e98cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a98cae62ad814b5fb871fcee1b799d45","IPY_MODEL_dd8f9257c7654fc0924c21177c17f8bb"]}},"ba635652360e4a3caad29fbf591e98cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a98cae62ad814b5fb871fcee1b799d45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8239053bded04e348d11b4e0d821e6a9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1198122,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1198122,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_561b86b153cc40bba05e94ebe6053eee"}},"dd8f9257c7654fc0924c21177c17f8bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_30e6c13429ea4a5c982d3a6509dffaa0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20M/1.20M [00:04&lt;00:00, 269kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f5e3fad164e436b8a35800f2ceb8ce1"}},"8239053bded04e348d11b4e0d821e6a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"561b86b153cc40bba05e94ebe6053eee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30e6c13429ea4a5c982d3a6509dffaa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8f5e3fad164e436b8a35800f2ceb8ce1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4cec81945f08477094b02aceca68b282":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d0692bf8d234627a21c34e88ded9e8b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f90fce072f544137b61fae1c0f9dc6c7","IPY_MODEL_73018c0394c84e4a8e5fb7776223bb42"]}},"0d0692bf8d234627a21c34e88ded9e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f90fce072f544137b61fae1c0f9dc6c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f88d1c820e664b1989f66fde78f9daad","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3b11728afe864f9f97c25d2c0a8bd0c1"}},"73018c0394c84e4a8e5fb7776223bb42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7d43030dd68249f5b73a8c4fff0bdc83","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:01&lt;00:00, 97.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3ce0acbfc9c4112b8b0dfaf71421b7a"}},"f88d1c820e664b1989f66fde78f9daad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3b11728afe864f9f97c25d2c0a8bd0c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d43030dd68249f5b73a8c4fff0bdc83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f3ce0acbfc9c4112b8b0dfaf71421b7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53941a2ba66a4440beed28493d7c20cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_671a326fc9094469a6f9ca2bbde90182","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_90f2891cd5c34791afa3b58374cb8d17","IPY_MODEL_687c8e86bf8543c09f0a3e886728738d"]}},"671a326fc9094469a6f9ca2bbde90182":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90f2891cd5c34791afa3b58374cb8d17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_69aea0f8b7434ce094954ee4ccf6d4ba","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":62,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":62,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f9d175ddd374e2d903b9b830eadce06"}},"687c8e86bf8543c09f0a3e886728738d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c324cc0151f41b68e0372aeef6539ea","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62.0/62.0 [00:00&lt;00:00, 374B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b01aa326bbc4efbb05db36caa2b6632"}},"69aea0f8b7434ce094954ee4ccf6d4ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9f9d175ddd374e2d903b9b830eadce06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c324cc0151f41b68e0372aeef6539ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b01aa326bbc4efbb05db36caa2b6632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f3425c41fcf440aa36e206c5a82de8a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cb3836b1b30b439797d31ee805d607c7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1e103eb11a8d4b82acf9eaae41b54c15","IPY_MODEL_7c506caecd1b4761926c8a5c350eac36"]}},"cb3836b1b30b439797d31ee805d607c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e103eb11a8d4b82acf9eaae41b54c15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_41cfb540a8074f6d89062a4e68291928","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":651477729,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":651477729,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_233992760f7045ce98c3bddbdc850beb"}},"7c506caecd1b4761926c8a5c350eac36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be38373d2d834285ae4fabf552bcefc2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 651M/651M [00:15&lt;00:00, 42.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b973508ee23741b2aa01099e492086b6"}},"41cfb540a8074f6d89062a4e68291928":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"233992760f7045ce98c3bddbdc850beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be38373d2d834285ae4fabf552bcefc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b973508ee23741b2aa01099e492086b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"j1LTPn7IjqTz"},"source":["Source:\r\n","\r\n","huggingface: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","\r\n","Tutorial:https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmPFvCbSqyZF","executionInfo":{"status":"ok","timestamp":1610530786734,"user_tz":-210,"elapsed":287714,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"3119a6f6-8169-410f-82f9-c354ce9c58df"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRxC0Pz1qzKc","executionInfo":{"status":"ok","timestamp":1610530787876,"user_tz":-210,"elapsed":288565,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/sharif/FineTuning/ipython(guide)')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCRkKc3NcgkX","executionInfo":{"status":"ok","timestamp":1610530795465,"user_tz":-210,"elapsed":295725,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"b30ed9d8-fa4d-43dd-ae1c-7dddcd1366a7"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\r\u001b[K     |▏                               | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 27.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 30.2MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 716kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 727kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 737kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 747kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 890kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 911kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 921kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 931kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 952kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 962kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 972kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 983kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 4.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 37.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 37.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=84a6d195c33b2cf94b83a7ef1b96c56602b1a5252f1852b7950c5429693434c5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODc44DglgNjZ","executionInfo":{"status":"ok","timestamp":1610530806537,"user_tz":-210,"elapsed":4931,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"53b45feb-93ee-4bae-cb84-c93482b33204"},"source":["!pip3 install sentencepiece"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJY_L2p9a0t0","executionInfo":{"status":"ok","timestamp":1610530806538,"user_tz":-210,"elapsed":4732,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"d75436af-eff1-4fd3-fee6-40aea732cdec"},"source":["!git clone https://huggingface.co/HooshvareLab/bert-fa-base-uncased-clf-persiannews\r\n","GIT_LFS_SKIP_SMUDGE=1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["fatal: destination path 'bert-fa-base-uncased-clf-persiannews' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eg3Up037nThu","executionInfo":{"status":"ok","timestamp":1610530813655,"user_tz":-210,"elapsed":11644,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import numpy\r\n","import pandas\r\n","import re\r\n","from sklearn.preprocessing import MultiLabelBinarizer\r\n","from sklearn.model_selection import train_test_split\r\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,TFAutoModel,AutoModel\r\n","from transformers import BertConfig, BertTokenizer\r\n","from transformers import TFBertModel, TFBertForSequenceClassification\r\n","from transformers import glue_convert_examples_to_features, InputExample\r\n","from sklearn.metrics import classification_report"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ur9wv1ytrZu","executionInfo":{"status":"ok","timestamp":1610530813655,"user_tz":-210,"elapsed":11370,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# specify GPU\r\n","device = torch.device(\"cuda\")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xax4bHubzpMp"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"TJf6T40glV5g","executionInfo":{"status":"ok","timestamp":1610530819172,"user_tz":-210,"elapsed":16414,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["limit_number = 750\r\n","data = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv',index_col=0)\r\n","data = data.dropna().reset_index(drop=True)\r\n","X = data[\"body\"].values.tolist()\r\n","y = pandas.read_csv('../Data/limited_to_'+str(limit_number)+'.csv')\r\n","labels = []\r\n","tag=[]\r\n","for item in y['tag']:\r\n","  labels += [i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' ']\r\n","  tag.append([i for i in re.sub('\\\"|\\[|\\]|\\'| |=','',item.lower()).split(\",\") if i!='' and i!=' '])\r\n","labels = list(set(labels))\r\n","mlb = MultiLabelBinarizer()\r\n","Y=mlb.fit_transform(tag)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH3jCKaZsEWo","executionInfo":{"status":"ok","timestamp":1610530819173,"user_tz":-210,"elapsed":15733,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"eccc4010-407a-4b2c-b5f8-47be6fd84278"},"source":["X_train, X_test, y_train, y_test = train_test_split(X,Y , test_size=0.2)\r\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n","print('train: ', len(X_train) , '\\ntest: ', len(X_test) , '\\nval: ', len(X_val) ,\"\\ny_tain:\",len(y_train) )"],"execution_count":9,"outputs":[{"output_type":"stream","text":["train:  12896 \n","test:  4299 \n","val:  4299 \n","y_tain: 12896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vei6iu9atmyd","colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["bc54661597404315b0ab9f5d3d1a4702","a6c08ee082bc4224a18f8ff58f4be312","fa95a752e3b84dc7a0410d044e08f2da","8a9e2da5764c4c8a816d51e2244d0306","52c94e7c00a44fcaaf62e1f2718e7f0f","d1926e7f87a0465ea93b3df03d642966","8850064f4fb04af19071683cf1cbf4b6","52b300d2b2344515be8c13e95f81a02d","a0e2f0b1ee664358a8c389237459c3e0","ba635652360e4a3caad29fbf591e98cd","a98cae62ad814b5fb871fcee1b799d45","dd8f9257c7654fc0924c21177c17f8bb","8239053bded04e348d11b4e0d821e6a9","561b86b153cc40bba05e94ebe6053eee","30e6c13429ea4a5c982d3a6509dffaa0","8f5e3fad164e436b8a35800f2ceb8ce1","4cec81945f08477094b02aceca68b282","0d0692bf8d234627a21c34e88ded9e8b","f90fce072f544137b61fae1c0f9dc6c7","73018c0394c84e4a8e5fb7776223bb42","f88d1c820e664b1989f66fde78f9daad","3b11728afe864f9f97c25d2c0a8bd0c1","7d43030dd68249f5b73a8c4fff0bdc83","f3ce0acbfc9c4112b8b0dfaf71421b7a","53941a2ba66a4440beed28493d7c20cc","671a326fc9094469a6f9ca2bbde90182","90f2891cd5c34791afa3b58374cb8d17","687c8e86bf8543c09f0a3e886728738d","69aea0f8b7434ce094954ee4ccf6d4ba","9f9d175ddd374e2d903b9b830eadce06","2c324cc0151f41b68e0372aeef6539ea","7b01aa326bbc4efbb05db36caa2b6632"]},"executionInfo":{"status":"ok","timestamp":1610530824382,"user_tz":-210,"elapsed":20670,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"efd866e8-4ef1-496e-90e5-1aecaafde507"},"source":["##we would load the tokenizer\r\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc54661597404315b0ab9f5d3d1a4702","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1441.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0e2f0b1ee664358a8c389237459c3e0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1198122.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cec81945f08477094b02aceca68b282","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53941a2ba66a4440beed28493d7c20cc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7wdU0zejDNq","executionInfo":{"status":"ok","timestamp":1610530824382,"user_tz":-210,"elapsed":20498,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"c931164f-68d7-49d9-baa8-86b48aac98a7"},"source":["#example\r\n","text = \"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد میتوانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\r\n","tokenized=tokenizer.tokenize(X_train[0])\r\n","input_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n","print(tokenized)\r\n","print(input_ids)\r\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['قسمت', 'میخوام', 'مطلب', 'کار', 'web', '##pack', 'ادامه', 'بدم', 'اطلاعات', 'module', 'bund', '##ling', 'اختیار', '##تون', 'قرار', 'بدم', 'قسمت', 'code', 'split', '##ting', 'بصورت', 'مقدماتی', 'تعریف', 'بهتون', 'توضیح', 'مدتی', 'اپلیکیشن', 'بزرگتر', 'میشه', 'حجم', 'bund', '##le', 'خروجی', 'میشه', 'میکشه', 'مرورگر', 'bund', '##le', 'دانلود', 'کنه', 'میخوام', 'مفهوم', 'کاربردی', 'مفید', 'مثال', 'کاربردی', 'بهتون', 'اموزش', 'بدم', 'فرض', 'میخوایم', 'ماژول', 'بنام', 'chat', 'اضافه', 'میخوام', 'ماژول', 'لود', 'بشه', 'بشه', 'کاربر', 'دکمه', 'کلیک', 'کنه', 'قدم', 'تغییراتی', 'فایل', 'web', '##pack', 'config', 'بیاریم', 'تغییرات', 'بصورت', 'همونطور', 'جلسات', 'گفتهشد', 'خطوطی', 'معنای', 'اینه', 'حذف', 'میشه', 'خطهایی', 'اضافه', 'میشن', 'همونطور', 'میبینید', 'entry', 'بصورت', 'object', 'اوردم', 'بشه', 'ورودی', 'تعریف', 'output', 'name', 'قرار', 'اینکار', 'web', '##pack', 'میگیم', 'فایل', 'خروجی', 'داینامیک', 'قرار', 'بده', 'web', '##pack', 'فایل', 'خروجی', 'قرار', 'میده', 'فایل', 'بنام', 'chat', 'دایرکتوری', 'src', 'میارم', 'کد', 'ها', 'قرار', 'میدم', 'همونطور', 'میبینید', 'بصورت', 'ساده', 'فایل', 'pe', '##ap', '##le', 'ارایه', 'ای', 'لود', 'div', 'ساختیم', 'اطلاعاتی', 'pe', '##ap', '##le', 'قرار', 'کد', 'ها', 'index', 'قرار', 'میدیم', 'میبینید', 'دکمه', 'ساختیم', 'متن', 'open', 'chat', 'قرار', 'دکمه', 'کلیک', 'میشه', 'دستور', 'import', 'لود', 'ماژول', 'chat', 'ادامه', 'توضیح', 'میشه', 'استایل', 'فایل', 'app', 'sc', '##ss', 'اضافه', 'میتونه', 'باشه', 'بتونی', '##ن', 'برنامه', 'ها', 'سنگین', 'خودتون', 'تکنیک', 'جداسازی', 'performance', 'سرعت', 'لود', 'سایت', '##تون', 'افزایش', 'همونطور', 'دیدیم', 'load', '##er', 'ها', 'فایل', 'ها', 'بصورت', 'تکی', 'میشن', 'کار', 'ها', 'میدن', 'plug', '##in', 'ها', 'میتونین', 'بزرگتری', 'کدها', '##تون', 'اصطلاح', 'ch', '##unk', 'ها', 'تمرکز', 'کدها', '##مون', 'بصورت', 'bund', '##le', 'اوردیم', 'زودی', 'اضافه', 'ابزار', 'ها', 'extern', '##al', 'ass', '##est', 'ها', 'سایز', 'bund', '##le', 'زیادتر', 'میشه', 'plug', '##in', 'ها', 'کمک', 'میتونیم', 'اونا', 'کدها', '##مون', 'تکه', 'تکه', 'split', 'محیط', 'production', 'اماده', 'ویژگی', 'mode', 'صحنه', 'پلاگین', 'ها', 'فرض', 'react', 'plug', '##in', 'حالت', 'development', 'اونا', 'میشه', 'بصورت', 'plug', '##in', 'حالت', 'production', 'اونا', 'میشه', 'بصورت', 'plug', '##in', 'ها', 'دیگه', 'ای', 'اضافه', 'تنظیمات', 'web', '##pack', 'بتونیم', 'پلاگین', 'ها', 'مختلفی', 'حالت', 'development', 'production', 'اینکار', 'فایل', 'web', '##pack', 'config', 'web', '##pack', 'common', 'میدم', 'فایل', 'ها', 'web', '##pack', 'prod', 'web', '##pack', 'dev', 'اضافه', 'اینکار', 'ساختار', 'بصورت', 'میشه', 'ترکیب', 'فایل', 'ها', 'تنظیماتی', 'ابزار', 'web', '##pack', '##mer', '##ge', 'نصب', 'ابزار', 'بصورت', 'کد', 'ها', 'فایل', 'web', '##pack', 'dev', 'قرار', 'میدم', 'کد', 'ها', 'فایل', 'web', '##pack', 'prod', 'قرار', 'میدم', 'فایل', 'package', 'json', 'بصورت', 'میدم', 'همونطور', 'میبینید', 'script', '##s', 'بجای', 'mode', 'config', 'حالت', 'تنظیماتی', 'قرار', 'بگیره', 'کار', 'ها', 'میتونیم', 'پلاگین', 'ها', 'متفاوتی', 'production', 'development', 'قسمت', 'plug', '##in', 'ها', 'توضیح', 'میدم', 'اطلاعات', 'web', '##pack', 'اختیار', '##تون', 'قرار', 'میدم']\n","[3951, 17953, 5071, 2867, 24376, 74349, 3251, 19910, 3531, 85233, 57205, 31544, 4209, 4957, 2959, 19910, 3951, 30765, 66346, 90877, 10884, 9607, 4568, 32701, 4836, 5129, 7494, 5958, 10672, 4735, 57205, 5028, 6945, 10672, 49832, 11999, 57205, 5028, 7510, 14114, 17953, 5996, 7164, 5850, 4183, 7164, 32701, 3911, 19910, 5875, 44594, 15799, 8094, 47123, 4241, 17953, 15799, 15305, 17420, 17420, 5988, 11802, 7843, 14114, 6347, 8900, 6921, 24376, 74349, 65261, 71442, 4995, 10884, 45309, 7295, 95751, 31520, 5208, 9225, 4884, 10672, 73733, 4241, 10207, 45309, 8146, 77477, 10884, 39293, 24883, 17420, 5641, 4568, 60491, 32759, 2959, 13459, 24376, 74349, 56392, 6921, 6945, 52774, 2959, 3791, 24376, 74349, 6921, 6945, 2959, 3083, 6921, 8094, 47123, 33103, 62321, 82434, 4366, 5929, 2959, 27409, 45309, 8146, 10884, 4613, 6921, 16765, 7051, 5028, 6904, 2938, 15305, 29511, 32357, 5823, 16765, 7051, 5028, 2959, 4366, 5929, 37745, 2959, 42528, 8146, 11802, 32357, 4175, 21614, 47123, 2959, 11802, 7843, 10672, 4090, 50145, 15305, 15799, 47123, 3251, 4836, 10672, 17915, 6921, 24546, 10445, 14991, 4241, 21659, 12139, 34727, 2011, 3329, 5929, 5758, 27989, 6901, 12637, 52796, 4089, 15305, 4394, 4957, 3191, 45309, 13615, 45497, 3647, 5929, 6921, 5929, 10884, 19604, 10207, 2867, 5929, 25382, 67293, 3943, 5929, 58164, 14405, 28552, 4957, 5831, 7418, 35496, 5929, 5565, 28552, 5796, 10884, 57205, 5028, 24107, 7816, 4241, 4766, 5929, 90250, 4261, 17989, 9253, 5929, 12101, 57205, 5028, 25836, 10672, 67293, 3943, 5929, 3469, 23116, 17687, 28552, 5796, 9636, 9636, 66346, 3952, 54570, 4788, 4274, 24378, 5665, 28918, 5929, 5875, 74701, 67293, 3943, 4157, 42794, 17687, 10672, 10884, 67293, 3943, 4157, 54570, 17687, 10672, 10884, 67293, 3943, 5929, 11904, 2938, 4241, 11384, 24376, 74349, 43123, 28918, 5929, 5042, 4157, 42794, 54570, 13459, 6921, 24376, 74349, 65261, 24376, 74349, 57158, 27409, 6921, 5929, 24376, 74349, 33486, 24376, 74349, 22748, 4241, 13459, 4856, 10884, 10672, 4524, 6921, 5929, 55988, 4766, 24376, 74349, 21068, 10741, 5596, 4766, 10884, 4366, 5929, 6921, 24376, 74349, 22748, 2959, 27409, 4366, 5929, 6921, 24376, 74349, 33486, 2959, 27409, 6921, 85429, 86822, 10884, 27409, 45309, 8146, 43586, 2032, 13344, 24378, 65261, 4157, 55988, 2959, 41425, 2867, 5929, 23116, 28918, 5929, 6909, 54570, 42794, 3951, 67293, 3943, 5929, 4836, 27409, 3531, 24376, 74349, 4209, 4957, 2959, 27409]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az4rwU0l5ECn","executionInfo":{"status":"ok","timestamp":1610530824383,"user_tz":-210,"elapsed":20005,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# encode text\r\n","sent_id = tokenizer.batch_encode_plus(X_train[:10], padding=True, return_token_type_ids=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNRU-SH65ZEE","executionInfo":{"status":"ok","timestamp":1610530824384,"user_tz":-210,"elapsed":19525,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"b1e8b433-5368-4fbe-ea8f-1492ece4df42"},"source":["sent_id"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 3951, 17953, 5071, 2867, 24376, 74349, 3251, 19910, 3531, 85233, 57205, 31544, 4209, 4957, 2959, 19910, 3951, 30765, 66346, 90877, 10884, 9607, 4568, 32701, 4836, 5129, 7494, 5958, 10672, 4735, 57205, 5028, 6945, 10672, 49832, 11999, 57205, 5028, 7510, 14114, 17953, 5996, 7164, 5850, 4183, 7164, 32701, 3911, 19910, 5875, 44594, 15799, 8094, 47123, 4241, 17953, 15799, 15305, 17420, 17420, 5988, 11802, 7843, 14114, 6347, 8900, 6921, 24376, 74349, 65261, 71442, 4995, 10884, 45309, 7295, 95751, 31520, 5208, 9225, 4884, 10672, 73733, 4241, 10207, 45309, 8146, 77477, 10884, 39293, 24883, 17420, 5641, 4568, 60491, 32759, 2959, 13459, 24376, 74349, 56392, 6921, 6945, 52774, 2959, 3791, 24376, 74349, 6921, 6945, 2959, 3083, 6921, 8094, 47123, 33103, 62321, 82434, 4366, 5929, 2959, 27409, 45309, 8146, 10884, 4613, 6921, 16765, 7051, 5028, 6904, 2938, 15305, 29511, 32357, 5823, 16765, 7051, 5028, 2959, 4366, 5929, 37745, 2959, 42528, 8146, 11802, 32357, 4175, 21614, 47123, 2959, 11802, 7843, 10672, 4090, 50145, 15305, 15799, 47123, 3251, 4836, 10672, 17915, 6921, 24546, 10445, 14991, 4241, 21659, 12139, 34727, 2011, 3329, 5929, 5758, 27989, 6901, 12637, 52796, 4089, 15305, 4394, 4957, 3191, 45309, 13615, 45497, 3647, 5929, 6921, 5929, 10884, 19604, 10207, 2867, 5929, 25382, 67293, 3943, 5929, 58164, 14405, 28552, 4957, 5831, 7418, 35496, 5929, 5565, 28552, 5796, 10884, 57205, 5028, 24107, 7816, 4241, 4766, 5929, 90250, 4261, 17989, 9253, 5929, 12101, 57205, 5028, 25836, 10672, 67293, 3943, 5929, 3469, 23116, 17687, 28552, 5796, 9636, 9636, 66346, 3952, 54570, 4788, 4274, 24378, 5665, 28918, 5929, 5875, 74701, 67293, 3943, 4157, 42794, 17687, 10672, 10884, 67293, 3943, 4157, 54570, 17687, 10672, 10884, 67293, 3943, 5929, 11904, 2938, 4241, 11384, 24376, 74349, 43123, 28918, 5929, 5042, 4157, 42794, 54570, 13459, 6921, 24376, 74349, 65261, 24376, 74349, 57158, 27409, 6921, 5929, 24376, 74349, 33486, 24376, 74349, 22748, 4241, 13459, 4856, 10884, 10672, 4524, 6921, 5929, 55988, 4766, 24376, 74349, 21068, 10741, 5596, 4766, 10884, 4366, 5929, 6921, 24376, 74349, 22748, 2959, 27409, 4366, 5929, 6921, 24376, 74349, 33486, 2959, 27409, 6921, 85429, 86822, 10884, 27409, 45309, 8146, 43586, 2032, 13344, 24378, 65261, 4157, 55988, 2959, 41425, 2867, 5929, 23116, 28918, 5929, 6909, 54570, 42794, 3951, 67293, 3943, 5929, 4836, 27409, 3531, 24376, 74349, 4209, 4957, 2959, 27409, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6044, 3743, 11667, 11440, 2793, 7021, 11667, 9424, 4072, 4756, 3832, 2844, 2867, 5886, 3832, 3057, 99904, 6462, 6462, 4999, 52586, 3810, 13642, 60787, 3810, 19321, 3329, 9016, 6462, 2014, 4568, 12607, 3810, 4896, 2014, 4896, 4656, 3632, 4896, 3810, 4914, 10634, 3821, 3168, 4313, 10634, 3280, 3632, 3632, 4656, 3905, 5929, 14474, 3552, 3351, 3632, 11667, 11440, 3310, 61155, 2793, 13555, 8575, 3151, 17966, 3632, 6044, 13823, 13887, 8333, 5585, 6002, 25516, 4917, 1442, 5292, 19178, 6044, 4686, 9011, 6044, 30903, 4628, 4459, 52886, 4459, 2793, 14151, 3632, 5987, 4313, 10144, 3821, 3890, 10634, 3280, 3632, 6031, 8204, 9494, 3821, 76050, 28738, 3632, 5929, 7580, 5801, 5020, 3151, 17966, 6347, 5929, 3351, 4392, 3088, 15138, 11269, 2003, 3743, 3337, 15407, 8278, 4014, 70126, 55523, 6146, 70126, 8695, 4710, 9472, 60326, 14702, 16175, 5891, 9472, 8629, 3494, 4014, 5612, 11419, 5983, 7085, 22669, 26276, 6195, 9011, 3469, 22669, 4206, 3250, 3351, 58347, 3928, 13693, 7002, 1442, 5590, 3491, 9774, 6071, 3632, 5929, 7078, 8433, 2009, 30407, 2009, 3789, 15795, 3919, 5929, 16640, 2014, 3805, 5929, 16376, 2015, 49336, 3919, 24207, 6347, 3351, 63984, 12910, 3328, 6601, 70692, 3588, 2003, 3347, 3632, 5929, 3821, 55265, 5392, 40501, 30162, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4082, 2005, 4900, 7510, 68857, 6921, 5929, 9258, 4912, 24540, 14404, 6468, 8769, 39662, 37044, 9092, 4175, 4655, 8942, 7330, 6921, 71117, 24540, 27876, 8097, 8097, 9258, 2008, 4209, 32701, 3625, 3083, 3733, 5605, 1442, 3531, 2008, 42709, 6921, 9258, 8471, 20516, 6921, 7510, 10093, 6075, 5605, 10672, 9258, 3432, 4630, 29373, 6921, 7510, 14114, 4875, 42709, 11733, 4224, 7510, 6921, 6075, 7510, 14114, 88795, 63038, 3432, 4296, 4875, 42709, 6921, 6075, 28228, 15559, 6921, 3218, 1442, 7510, 12702, 14324, 37044, 7510, 24540, 2014, 6379, 5536, 5929, 9258, 24843, 15559, 59590, 3647, 10672, 7510, 6921, 42709, 47243, 3686, 2014, 12139, 35088, 6728, 6921, 7510, 9258, 93891, 47959, 29538, 10672, 10322, 19994, 9225, 65889, 42709, 7510, 3773, 2015, 3470, 4957, 26868, 42709, 5596, 26868, 42709, 5574, 11378, 2008, 7510, 6921, 5929, 7918, 6251, 5536, 5929, 10672, 3473, 37044, 9258, 7510, 68857, 5574, 7043, 5536, 5929, 9774, 6251, 12702, 4183, 5722, 9225, 29112, 21284, 41786, 19364, 88169, 5653, 4630, 9225, 4576, 5353, 3809, 2911, 27069, 6921, 5929, 42709, 19899, 42709, 3310, 9258, 5722, 12702, 4766, 5929, 44342, 5996, 11904, 2938, 42709, 4185, 2867, 19994, 25528, 98404, 3312, 2007, 5929, 3490, 4616, 5929, 42709, 5929, 6138, 19791, 3312, 2007, 5929, 3885, 3312, 2007, 5929, 4387, 6839, 3088, 31366, 3312, 2007, 5929, 3885, 6025, 4062, 3312, 2007, 5929, 4299, 3312, 2007, 5929, 4299, 3804, 4603, 19096, 4002, 5536, 5929, 17420, 29879, 38696, 89754, 2816, 12914, 11904, 2938, 47959, 29538, 12702, 4449, 17078, 42709, 3804, 4089, 4062, 4900, 2927, 16958, 2795, 37044, 79566, 2013, 3731, 12139, 21280, 7510, 7164, 20516, 32764, 7510, 6921, 5929, 29735, 5049, 63919, 4909, 77909, 12139, 21280, 3169, 5929, 4394, 5929, 47648, 3211, 4244, 3169, 5428, 8458, 6075, 3821, 3359, 2008, 3211, 4089, 42709, 7585, 4157, 6367, 49832, 6921, 7510, 9225, 5574, 7043, 22518, 34451, 2867, 10351, 25382, 4576, 5929, 5353, 6921, 34451, 4394, 5929, 4691, 4616, 1442, 11827, 42709, 5536, 5929, 11827, 7510, 34451, 13265, 42709, 8387, 4089, 69452, 5711, 74375, 7078, 11231, 3541, 5929, 5402, 6905, 7510, 37044, 4900, 11231, 8458, 6921, 3138, 2008, 23099, 5929, 6603, 5929, 8458, 45243, 27450, 11193, 6704, 6603, 4957, 14335, 12139, 4394, 5929, 6839, 6921, 42709, 27989, 34451, 3531, 15838, 3424, 1442, 42709, 2867, 17521, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6012, 3494, 21803, 4766, 5929, 4972, 48250, 4766, 5929, 5625, 7579, 4613, 3088, 71442, 4013, 6545, 6012, 2793, 9073, 2013, 11827, 7563, 6839, 57730, 4157, 72620, 28612, 13093, 3951, 6012, 4157, 3528, 26702, 4360, 6012, 4893, 5145, 57730, 17690, 6188, 45309, 2003, 6012, 3364, 5605, 9533, 3531, 71030, 3778, 5929, 4766, 28982, 28724, 36539, 18045, 3521, 5166, 3329, 4373, 2006, 10332, 17690, 6188, 2793, 24843, 3531, 71030, 3778, 5929, 9533, 24843, 6506, 17690, 6188, 7644, 6325, 38644, 3805, 27633, 27633, 7786, 17143, 16576, 4207, 5929, 4461, 5166, 17143, 16576, 23024, 2015, 65757, 88887, 27789, 6293, 11362, 5200, 28982, 28724, 36539, 5929, 4207, 5166, 27789, 17143, 16576, 3852, 5524, 5594, 28982, 28724, 36539, 5929, 17420, 5342, 15932, 4484, 5166, 17690, 6188, 12139, 3329, 4373, 2784, 5166, 31957, 3251, 10292, 17143, 16576, 18757, 3422, 2008, 3852, 4461, 63432, 17143, 3233, 5369, 57730, 2867, 17143, 3233, 4613, 38644, 17143, 16576, 5019, 32023, 71030, 3778, 54770, 5166, 5369, 68007, 17143, 16576, 4853, 3494, 40986, 2805, 17143, 16576, 10025, 38644, 10672, 3531, 71030, 3778, 5929, 9533, 32328, 14877, 3731, 53873, 24254, 6728, 17420, 71030, 3778, 5929, 6251, 10025, 22157, 10207, 1876, 8039, 53873, 24254, 92803, 24107, 38644, 19994, 4825, 18936, 15559, 41961, 18936, 4547, 8953, 14114, 43046, 17690, 6188, 29197, 9533, 14877, 71030, 3778, 5929, 5166, 3444, 38644, 5166, 17143, 16576, 11231, 9494, 20516, 17690, 6188, 5596, 27989, 53027, 17690, 6188, 74292, 2008, 58232, 22221, 11231, 10672, 28313, 14877, 15619, 20936, 9540, 35496, 14877, 20800, 9477, 15619, 20936, 63852, 9477, 32023, 3182, 5929, 9269, 2839, 23764, 27987, 2827, 10672, 17143, 3233, 5321, 4183, 20800, 9477, 14877, 11231, 29197, 14877, 7440, 47882, 29197, 23764, 3531, 3182, 5929, 11999, 54770, 60113, 23180, 27989, 53027, 3916, 32023, 98122, 2025, 10672, 18757, 3439, 7289, 5166, 17802, 98122, 2025, 17690, 6188, 38644, 9546, 22869, 41862, 5806, 54936, 2025, 17690, 6188, 35284, 20223, 18903, 11231, 44131, 5317, 17143, 16576, 2793, 69964, 35284, 20223, 18903, 12139, 2793, 69964, 12139, 7290, 11231, 3972, 2793, 6070, 2003, 7085, 7909, 4230, 11961, 2015, 60291, 17690, 6188, 13222, 4183, 4909, 40097, 35514, 17143, 16576, 3951, 6012, 33670, 3364, 3251, 5358, 3944, 5094, 6012, 14183, 5770, 11827, 2014, 22004, 2959, 19910, 2793, 14760, 2013, 25029, 17143, 16576, 21825, 6914, 83838, 44129, 42779, 46836, 5481, 6820, 4863, 4863, 5929, 3573, 9942, 15655, 4241, 6820, 8597, 7082, 3083, 4196, 3951, 38984, 11827, 4893, 9472, 22673, 56619, 6012, 5850, 3473, 17143, 16576, 17690, 6188, 4], [2, 2793, 5506, 2015, 4449, 10600, 5929, 3376, 3927, 16069, 5929, 5442, 3318, 4426, 6266, 27289, 2910, 8464, 5929, 5442, 7330, 3497, 5739, 4449, 7494, 5929, 18826, 4997, 4790, 15307, 6316, 3625, 4536, 10413, 3088, 7494, 5929, 2793, 15110, 4321, 3475, 5574, 7043, 24664, 7237, 24664, 24215, 5929, 69007, 59401, 2003, 5929, 86365, 43716, 4196, 5154, 6378, 4804, 4766, 5929, 24215, 4536, 14769, 2007, 3419, 46558, 4060, 3318, 8473, 5311, 5655, 3290, 24215, 5929, 69007, 59401, 2003, 5929, 14335, 6629, 4766, 6012, 14769, 2007, 85229, 3647, 5563, 7173, 5988, 49722, 51250, 3625, 44751, 5988, 4772, 5929, 5040, 5102, 57637, 9258, 27975, 11516, 4816, 3419, 9258, 25317, 22504, 6528, 4875, 5929, 7014, 15407, 3499, 3595, 19327, 44129, 41989, 6629, 11997, 6012, 6629, 5996, 74214, 2059, 19327, 44129, 3475, 5574, 7043, 5563, 7173, 43716, 4196, 5154, 8333, 2959, 24664, 5929, 7215, 6012, 6629, 5996, 24680, 63674, 74191, 3639, 3475, 5574, 7043, 5563, 7173, 66823, 3475, 7494, 3229, 5884, 4226, 3521, 3916, 17700, 3229, 4449, 66823, 3743, 2867, 28918, 5929, 28918, 5929, 6832, 7173, 38702, 3531, 7692, 4816, 4206, 5929, 62598, 2793, 13555, 7201, 10682, 3804, 63432, 6907, 31065, 2045, 37200, 3318, 60576, 21705, 2033, 75532, 5344, 48230, 14991, 38636, 68491, 32538, 3903, 5818, 14038, 2017, 5929, 4816, 7173, 9258, 3229, 3052, 88810, 5884, 4226, 9719, 3229, 10761, 5526, 5653, 6110, 6378, 4804, 8822, 5929, 9258, 24376, 35125, 94983, 13870, 89420, 6378, 6637, 25944, 5929, 9484, 5929, 7330, 3296, 20199, 6378, 4804, 4875, 5929, 3885, 3864, 4482, 4875, 6339, 6378, 4804, 4905, 5929, 13222, 6914, 3792, 3329, 4244, 24201, 5929, 4875, 5929, 9258, 9258, 55783, 12512, 9000, 7173, 3347, 3968, 24592, 7971, 20268, 54015, 2807, 3870, 5061, 4251, 8670, 15445, 3870, 5061, 47046, 4315, 19288, 5039, 7168, 6649, 4418, 3444, 5754, 3337, 15407, 4998, 3841, 3475, 7494, 10413, 47046, 4315, 10153, 2938, 12835, 4790, 6501, 3927, 66417, 5442, 2793, 16118, 19802, 4727, 4290, 47046, 4315, 5884, 8850, 4321, 7525, 8776, 8726, 8309, 5725, 6378, 4804, 7971, 9913, 5754, 8606, 37615, 3884, 8309, 5289, 5892, 6707, 18421, 7550, 5676, 2938, 3490, 3828, 3088, 7494, 5929, 4816, 11847, 8141, 6468, 7171, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 5426, 4002, 6189, 5154, 7843, 11802, 4247, 5469, 7273, 4002, 7173, 37620, 4884, 10755, 9258, 5929, 19911, 6596, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4183, 3911, 4183, 3911, 5585, 4484, 14183, 4983, 4009, 5292, 4761, 3909, 5586, 4484, 1442, 25029, 4002, 2793, 15824, 4774, 4983, 4009, 3491, 7727, 4394, 4484, 1442, 4482, 9084, 4251, 4484, 3919, 5929, 10874, 2793, 41169, 4761, 8126, 13185, 2867, 31261, 5740, 14183, 3916, 4484, 5929, 4298, 5161, 3916, 4484, 3768, 1442, 3916, 5574, 7043, 3329, 12035, 4188, 8126, 13185, 5762, 9076, 3329, 4373, 5929, 4160, 2904, 3073, 3473, 8126, 13185, 5740, 14183, 3916, 4484, 8072, 2008, 5599, 8126, 3521, 3948, 21001, 4183, 5613, 8900, 4484, 5054, 4995, 6605, 26195, 5605, 4459, 7720, 6177, 6945, 5035, 13185, 3726, 3426, 8126, 13185, 3419, 4613, 6649, 5740, 14183, 3948, 3916, 3426, 4484, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 9087, 22137, 38636, 19921, 15826, 23394, 49295, 62795, 9087, 5725, 3736, 5673, 8543, 59775, 7563, 3439, 7563, 23689, 56234, 4705, 15678, 56234, 3475, 67682, 7563, 4207, 9087, 22137, 38636, 19921, 1456, 1393, 1454, 3625, 7253, 9087, 4316, 18087, 4156, 6443, 7886, 9087, 22137, 38636, 19921, 40355, 9087, 43428, 4713, 5000, 9087, 43428, 3983, 5988, 9258, 3531, 5988, 49722, 33903, 2795, 28889, 10682, 4431, 9258, 16738, 9258, 7443, 4431, 15077, 14541, 9087, 4206, 3821, 5154, 9087, 43428, 2867, 11344, 8402, 6921, 3841, 7014, 3318, 16131, 9087, 10592, 9087, 22137, 38636, 19921, 5717, 4348, 4431, 5929, 10682, 5988, 5988, 49722, 49722, 13524, 2008, 4226, 20563, 19921, 12478, 18420, 3490, 5988, 5988, 9111, 4431, 5929, 15245, 9876, 4431, 16738, 4482, 9258, 5311, 16738, 4431, 5929, 4482, 4157, 6104, 4431, 10682, 33488, 52712, 5806, 5988, 15245, 9258, 5373, 16738, 24416, 81959, 4855, 5988, 9876, 10682, 6404, 7981, 20065, 52712, 22137, 38636, 19921, 4431, 10682, 9258, 7951, 2904, 3073, 16738, 4691, 9876, 4855, 31890, 17935, 4431, 5929, 9258, 20513, 8369, 9087, 4524, 10591, 3541, 5929, 10682, 9087, 8402, 5939, 39432, 55062, 61932, 80933, 9469, 12453, 23921, 59619, 6907, 10682, 31065, 16617, 4732, 10682, 6783, 38300, 38669, 38300, 93653, 38300, 93653, 3364, 3648, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6312, 34928, 4274, 3625, 1442, 23040, 2046, 44418, 6312, 14235, 6904, 5929, 3625, 3287, 5902, 1442, 6312, 14314, 5369, 45611, 3625, 1442, 23040, 2046, 6904, 6597, 7069, 1442, 10197, 19328, 5929, 52250, 19328, 1442, 11438, 6904, 2938, 4712, 5929, 44594, 3880, 4712, 2910, 71442, 2867, 7069, 2938, 43711, 21939, 5926, 4907, 4712, 3880, 4712, 6364, 54247, 40706, 7069, 3968, 45560, 19254, 12681, 10992, 31193, 88292, 5536, 5929, 27587, 34928, 17374, 5902, 1442, 34928, 4909, 2938, 34928, 3211, 52796, 6312, 2867, 41800, 46696, 21659, 3926, 3791, 5535, 3088, 7069, 1442, 10197, 2959, 11231, 4485, 38700, 12478, 40791, 31487, 3432, 11999, 2014, 17420, 7078, 3280, 38999, 68563, 6312, 19919, 12478, 2867, 7765, 62851, 2816, 52796, 12139, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 82628, 2013, 53263, 3419, 5929, 3329, 12035, 10594, 4386, 4565, 4836, 42528, 4183, 5484, 5690, 3181, 3850, 4565, 5929, 82628, 2013, 5690, 3328, 5071, 5117, 3511, 8751, 10291, 4482, 3511, 4002, 4002, 3511, 8751, 10291, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 49496, 13181, 2816, 2800, 1442, 84446, 74962, 34684, 2016, 1370, 74962, 74962, 74962, 74962, 2922, 32701, 3852, 8056, 7578, 2938, 9774, 3083, 6558, 2015, 34563, 26702, 8056, 4244, 62338, 2014, 5974, 4158, 2015, 3250, 6094, 12765, 41151, 9113, 2014, 77742, 26702, 24407, 10672, 49160, 34233, 33638, 2014, 3124, 2993, 5891, 40224, 2011, 27440, 3124, 2993, 33637, 86979, 3055, 36553, 2015, 3125, 8587, 37657, 59423, 5329, 4207, 2938, 82256, 4394, 6735, 3733, 2014, 2959, 19645, 4343, 3942, 15738, 3432, 4394, 5428, 7379, 5082, 1876, 2802, 3250, 29825, 5145, 78126, 17004, 2793, 3922, 3351, 4007, 3250, 13984, 10214, 4826, 4313, 20930, 8065, 14304, 23464, 9566, 4890, 2015, 4244, 3892, 14304, 12477, 55348, 4787, 18749, 2787, 2021, 15352, 5292, 23464, 76769, 4890, 2015, 48974, 3873, 22563, 2980, 14304, 3905, 1876, 2802, 4215, 3775, 7042, 89777, 17211, 20849, 3775, 7042, 11583, 25613, 92806, 2910, 6636, 3928, 3928, 8850, 4347, 79903, 6033, 3944, 6575, 9595, 3351, 3944, 3351, 9861, 8083, 23038, 5241, 6504, 3145, 1876, 6512, 12288, 2910, 5129, 13051, 14358, 54393, 19516, 15159, 3873, 5929, 6815, 5969, 3351, 3494, 40986, 2790, 4313, 12477, 3905, 1876, 3494, 40986, 2790, 3936, 5449, 4219, 5224, 26368, 2797, 4864, 3168, 19111, 9011, 5657, 97205, 2797, 4282, 6551, 6027, 9419, 6865, 33852, 3280, 2844, 5929, 4244, 39230, 9078, 23968, 5843, 2926, 12219, 16150, 3494, 40986, 2790, 1876, 3494, 50420, 19277, 4219, 14829, 84808, 6691, 3168, 19111, 3432, 3444, 3926, 10699, 3494, 50420, 28738, 24171, 28738, 2005, 3494, 1876, 4467, 3341, 5282, 2013, 20878, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"uF3FFsPzc6zD","executionInfo":{"status":"ok","timestamp":1610530824384,"user_tz":-210,"elapsed":19154,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["sentence_maxlen=128"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m2Qc2IkrnEp","executionInfo":{"status":"ok","timestamp":1610530846108,"user_tz":-210,"elapsed":39287,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"f29f674c-51dc-443f-e565-811c9a50f5de"},"source":["##Tokenize training and validation sentences:\r\n","train_encodings = tokenizer.batch_encode_plus(X_train,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","val_encodings = tokenizer.batch_encode_plus(X_val,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)\r\n","\r\n","test_encodings=tokenizer.batch_encode_plus(X_test,\r\n","    max_length = sentence_maxlen,\r\n","    pad_to_max_length=True,\r\n","    truncation=True,\r\n","    return_token_type_ids=False)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwjkXARbetX-","executionInfo":{"status":"ok","timestamp":1610530846109,"user_tz":-210,"elapsed":37590,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"238cf1aa-4cd1-435b-81e6-6f55b00ea224"},"source":["train_encodings[0]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"-iCp2PUEupYK","executionInfo":{"status":"ok","timestamp":1610530846110,"user_tz":-210,"elapsed":36684,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","\r\n","# for train set\r\n","train_seq = torch.tensor(train_encodings['input_ids'])\r\n","train_mask = torch.tensor(train_encodings['attention_mask'])\r\n","train_y = torch.tensor(y_train)\r\n","\r\n","# for validation set\r\n","val_seq = torch.tensor(val_encodings['input_ids'])\r\n","val_mask = torch.tensor(val_encodings['attention_mask'])\r\n","val_y = torch.tensor(y_val)\r\n","\r\n","# for test set\r\n","test_seq = torch.tensor(test_encodings['input_ids'])\r\n","test_mask = torch.tensor(test_encodings['attention_mask'])\r\n","test_y = torch.tensor(y_test)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0JkQbxVBmbM","executionInfo":{"status":"ok","timestamp":1610530846111,"user_tz":-210,"elapsed":36326,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"ade94a6a-77b0-4ba7-d8d4-90dca0bd16a0"},"source":["train_y[0]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"T2xiV6Nb0ddZ","executionInfo":{"status":"ok","timestamp":1610530846112,"user_tz":-210,"elapsed":36136,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","#define a batch size\r\n","batch_size = 32\r\n","\r\n","# wrap tensors\r\n","train_data = TensorDataset(train_seq, train_mask, train_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","train_sampler = RandomSampler(train_data)\r\n","\r\n","# dataLoader for train set\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","val_data = TensorDataset(val_seq, val_mask, val_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","val_sampler = SequentialSampler(val_data)\r\n","\r\n","# dataLoader for validation set\r\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\r\n","\r\n","# wrap tensors\r\n","test_data = TensorDataset(test_seq, test_mask, test_y)\r\n","\r\n","# sampler for sampling the data during training\r\n","test_sampler = SequentialSampler(test_data)\r\n","\r\n","# dataLoader for validation set\r\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":749,"referenced_widgets":["3f3425c41fcf440aa36e206c5a82de8a","cb3836b1b30b439797d31ee805d607c7","1e103eb11a8d4b82acf9eaae41b54c15","7c506caecd1b4761926c8a5c350eac36","41cfb540a8074f6d89062a4e68291928","233992760f7045ce98c3bddbdc850beb","be38373d2d834285ae4fabf552bcefc2","b973508ee23741b2aa01099e492086b6"]},"id":"UwGHXIjGfmaN","executionInfo":{"status":"ok","timestamp":1610530865531,"user_tz":-210,"elapsed":55364,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"2b9f6c2e-022a-41f7-962c-3567c74fad66"},"source":["# example\r\n","\r\n","\r\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\r\n","\r\n","# encode text\r\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\r\n","print(sent_id)\r\n","\r\n","seq = torch.tensor(sent_id['input_ids'])\r\n","mask = torch.tensor(sent_id['attention_mask'])\r\n","train_y = torch.tensor([0,1])\r\n","\r\n","transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")\r\n","cls_hs=transformer_model(seq,mask)\r\n","print(cls_hs)\r\n","print(cls_hs[0])\r\n","print(cls_hs[1])\r\n","print(cls_hs[1].shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["{'input_ids': [[2, 32071, 9574, 1026, 89390, 36260, 84378, 40908, 2041, 4, 0], [2, 13632, 25909, 70608, 1011, 40716, 2033, 1026, 89390, 36260, 4]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f3425c41fcf440aa36e206c5a82de8a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=651477729.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>), hidden_states=None, attentions=None, cross_attentions=None)\n","tensor([[[ 0.0526,  0.5571, -0.4614,  ..., -0.0968,  0.4727,  0.1742],\n","         [-0.2566,  1.5509, -2.0229,  ..., -1.1688, -0.4160,  0.1496],\n","         [-0.1851,  0.1336, -1.3189,  ..., -0.5912, -0.4864,  0.4295],\n","         ...,\n","         [-0.2249,  0.1459, -1.4157,  ..., -0.1764,  0.6163, -0.5646],\n","         [-0.3767, -0.2304, -0.3158,  ..., -0.5575,  0.0901,  0.6220],\n","         [-0.2883,  0.2287, -1.5781,  ..., -0.3559,  0.3813,  0.0665]],\n","\n","        [[ 0.0939, -0.5881, -1.2552,  ...,  0.9090,  0.5908, -0.1969],\n","         [-0.2802, -0.9775, -1.5731,  ...,  0.0902,  0.5980, -0.6988],\n","         [-0.2920, -0.6260, -0.9620,  ..., -0.4935,  0.6855, -1.1112],\n","         ...,\n","         [-0.1571, -0.1198, -2.0160,  ...,  0.3612,  0.7098, -0.9345],\n","         [-0.0399, -0.9591, -1.5613,  ...,  0.6662,  0.1020, -0.0502],\n","         [-0.4564, -1.5508, -0.4116,  ..., -0.1108,  1.1311,  0.2711]]],\n","       grad_fn=<NativeLayerNormBackward>)\n","tensor([[ 0.7309, -0.4917, -0.7028,  ...,  0.3011,  0.4490, -0.6379],\n","        [ 0.9530,  0.1168, -0.2492,  ..., -0.4421,  0.0763, -0.8724]],\n","       grad_fn=<TanhBackward>)\n","torch.Size([2, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ByUEn_v4zknn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"n3AjEaHcEMfb","executionInfo":{"status":"ok","timestamp":1610530871734,"user_tz":-210,"elapsed":59493,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["transformer_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-clf-persiannews\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaAlYydhxPTd","executionInfo":{"status":"ok","timestamp":1610530871734,"user_tz":-210,"elapsed":59233,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# freeze all the parameters\r\n","for param in transformer_model.parameters():\r\n","    param.requires_grad = False"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUa1R1WQONe6","executionInfo":{"status":"ok","timestamp":1610530871736,"user_tz":-210,"elapsed":58811,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"60aad659-8af8-43cf-f56b-804d2ce15cfa"},"source":["len(labels)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"oyE_ThEms5aZ","executionInfo":{"status":"ok","timestamp":1610530874073,"user_tz":-210,"elapsed":2332,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["class BERT_Arch(nn.Module):\r\n","\r\n","    def __init__(self, bert):\r\n","      \r\n","      super(BERT_Arch, self).__init__()\r\n","\r\n","      self.bert = bert \r\n","      \r\n","      # dropout layer\r\n","      self.dropout = nn.Dropout(0.1)\r\n","      \r\n","      # relu activation function\r\n","      self.relu =  nn.ReLU()\r\n","\r\n","      # dense layer 1\r\n","      self.fc1 = nn.Linear(768,512)\r\n","      \r\n","      # dense layer 2 (Output layer)\r\n","      self.fc2 = nn.Linear(512,128)\r\n","\r\n","      # dense layer 3 (Output layer)\r\n","      self.fc3 = nn.Linear(128,78)\r\n","\r\n","      #sigmoid activation function\r\n","      self.sigmoid = nn.Sigmoid()\r\n","\r\n","    #define the forward pass\r\n","    def forward(self, sent_id, mask):\r\n","\r\n","      #pass the inputs to the model  \r\n","      cls_hs = self.bert(sent_id, attention_mask=mask)\r\n","      \r\n","      x = self.fc1(cls_hs[1])\r\n","\r\n","      x = self.relu(x)\r\n","\r\n","      x = self.dropout(x)\r\n","\r\n","      # output layer\r\n","      x = self.fc2(x)\r\n","      x = self.relu(x)\r\n","      x = self.fc3(x)\r\n","      \r\n","      # apply sigmoid activation\r\n","      x = self.sigmoid(x)\r\n","\r\n","      return x"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDuHzo96z6z8","executionInfo":{"status":"ok","timestamp":1610530888620,"user_tz":-210,"elapsed":16876,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(transformer_model)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUNSLBYcLc9q","executionInfo":{"status":"ok","timestamp":1610530888621,"user_tz":-210,"elapsed":16875,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArHmwhh7JrZh","executionInfo":{"status":"ok","timestamp":1610530888624,"user_tz":-210,"elapsed":16875,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["loss_func =nn.MultiLabelSoftMarginLoss()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8LjQyDXs0bG","executionInfo":{"status":"ok","timestamp":1610530888624,"user_tz":-210,"elapsed":16873,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function to train the model\r\n","def train():\r\n","  \r\n","  model.train()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save model predictions\r\n","  total_preds=[]\r\n","  \r\n","  # iterate over batches\r\n","  for step,batch in enumerate(train_dataloader):\r\n","    \r\n","    # progress update after every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [r.to(device) for r in batch]\r\n"," \r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # clear previously calculated gradients \r\n","    model.zero_grad()        \r\n","\r\n","    # get model predictions for the current batch\r\n","    preds = model(sent_id, mask)\r\n","\r\n","    # compute the loss between actual and predicted values\r\n","    \r\n","    loss = loss_func(preds, labels)\r\n","    # add on to the total loss\r\n","    total_loss = total_loss + loss.item()\r\n","\r\n","    # backward pass to calculate the gradients\r\n","    loss.backward()\r\n","\r\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","    # update parameters\r\n","    optimizer.step()\r\n","\r\n","    # model predictions are stored on GPU. So, push it to CPU\r\n","    preds=preds.detach().cpu().numpy()\r\n","\r\n","    # append the model predictions\r\n","    total_preds.append(preds)\r\n","\r\n","  # compute the training loss of the epoch\r\n","  avg_loss = total_loss / len(train_dataloader)\r\n","  \r\n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  #returns the loss and predictions\r\n","  return avg_loss, total_preds"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNBRQo9WMHey","executionInfo":{"status":"ok","timestamp":1610530888625,"user_tz":-210,"elapsed":16872,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["# function for evaluating the model\r\n","def evaluate():\r\n","  \r\n","  print(\"\\nEvaluating...\")\r\n","  \r\n","  # deactivate dropout layers\r\n","  model.eval()\r\n","\r\n","  total_loss, total_accuracy = 0, 0\r\n","  \r\n","  # empty list to save the model predictions\r\n","  total_preds = []\r\n","\r\n","  # iterate over batches\r\n","  for step,batch in enumerate(val_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","\r\n","      # compute the validation loss between actual and predicted values\r\n","      loss = loss_func(preds,labels)\r\n","\r\n","      total_loss = total_loss + loss.item()\r\n","\r\n","      preds = preds.detach().cpu().numpy()\r\n","\r\n","      total_preds.append(preds)\r\n","\r\n","  # compute the validation loss of the epoch\r\n","  avg_loss = total_loss / len(val_dataloader) \r\n","\r\n","  # reshape the predictions in form of (number of samples, no. of classes)\r\n","  total_preds  = numpy.concatenate(total_preds, axis=0)\r\n","\r\n","  return avg_loss, total_preds"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qu5pfrJKtTc0","executionInfo":{"status":"ok","timestamp":1610533339668,"user_tz":-210,"elapsed":2410194,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"60d67c1c-782f-40c0-dc25-ed86ced2f49a"},"source":["# number of training epochs\r\n","epochs = 10\r\n","\r\n","# set initial loss to infinite\r\n","best_valid_loss = float('inf')\r\n","\r\n","# empty lists to store training and validation loss of each epoch\r\n","train_losses=[]\r\n","valid_losses=[]\r\n","\r\n","#for each epoch\r\n","for epoch in range(epochs):\r\n","     \r\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n","    \r\n","    #train model\r\n","    train_loss, _ = train()\r\n","    \r\n","    #evaluate model\r\n","    valid_loss, _ = evaluate()\r\n","    \r\n","    #save the best model\r\n","    if valid_loss < best_valid_loss:\r\n","        best_valid_loss = valid_loss\r\n","        torch.save(model.state_dict(), 'saved_weights.pt')\r\n","    \r\n","    # append training and validation loss\r\n","    train_losses.append(train_loss)\r\n","    valid_losses.append(valid_loss)\r\n","    \r\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 2 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 3 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 4 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 5 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 6 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 7 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 8 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 9 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n","\n"," Epoch 10 / 10\n","  Batch    50  of    403.\n","  Batch   100  of    403.\n","  Batch   150  of    403.\n","  Batch   200  of    403.\n","  Batch   250  of    403.\n","  Batch   300  of    403.\n","  Batch   350  of    403.\n","  Batch   400  of    403.\n","\n","Evaluating...\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Training Loss: 0.693\n","Validation Loss: 0.693\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GlJpADKkIOqX"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"nVrfkSoKIOIV","executionInfo":{"status":"ok","timestamp":1610533345190,"user_tz":-210,"elapsed":5507,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["path = 'saved_weights_dense_3.pt'\n","torch.save(model.state_dict(), path)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ2_aS0zCLvp"},"source":["Loading saved model:"]},{"cell_type":"code","metadata":{"id":"cvR-FhPpuLkR"},"source":["# torch.cuda.empty_cache()\r\n","# pass the pre-trained BERT to our define architecture\r\n","model = BERT_Arch(transformer_model)\r\n","\r\n","# push the model to GPU\r\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aOPRZ2jVvNR","executionInfo":{"status":"aborted","timestamp":1610530928910,"user_tz":-210,"elapsed":57135,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":["#load weights of best model\r\n","path = 'saved_weights_dense_3.pt'\r\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PM1uUcZFCPVg"},"source":["After loading model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZhHObMnzuws","executionInfo":{"status":"ok","timestamp":1610533404709,"user_tz":-210,"elapsed":65013,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}},"outputId":"83af60d7-8477-4ca9-e5e5-fc2bad1485b9"},"source":["y_pred=[]\r\n","y_true=[]\r\n","for step,batch in enumerate(test_dataloader):\r\n","    \r\n","    # Progress update every 50 batches.\r\n","    if step % 50 == 0 and not step == 0:\r\n","      \r\n","      # Calculate elapsed time in minutes.\r\n","      # elapsed = format_time(time.time() - t0)\r\n","            \r\n","      # Report progress.\r\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\r\n","\r\n","    # push the batch to gpu\r\n","    batch = [t.to(device) for t in batch]\r\n","\r\n","    sent_id, mask, labels = batch\r\n","\r\n","    # deactivate autograd\r\n","    with torch.no_grad():\r\n","      \r\n","      # model predictions\r\n","      preds = model(sent_id, mask)\r\n","      # print(preds)\r\n","      # print(preds.cpu().numpy())\r\n","      preds = preds.cpu().numpy()\r\n","      # model's performance\r\n","    # preds = numpy.argmax(preds, axis = 1)\r\n","    \r\n","    measure = numpy.mean(preds[0]) + 1.15*numpy.sqrt(numpy.var(preds[0]))\r\n","    for l in preds:\r\n","      temp=[]\r\n","      for value in l:\r\n","        if value >= measure:\r\n","          temp.append(1)\r\n","        else:\r\n","          temp.append(0)\r\n","      y_pred.append(temp)\r\n","    y_true.extend(labels.cpu().numpy())\r\n","    # print(labels.cpu().numpy()[0], preds[0])\r\n","print(classification_report(y_true, y_pred))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["  Batch    50  of    135.\n","  Batch   100  of    135.\n","              precision    recall  f1-score   support\n","\n","           0       0.02      0.17      0.03       107\n","           1       0.04      0.14      0.06       163\n","           2       0.01      0.07      0.02       133\n","           3       0.00      0.05      0.00        22\n","           4       0.02      0.29      0.04        91\n","           5       0.00      0.15      0.00        13\n","           6       0.03      0.25      0.05       158\n","           7       0.01      0.31      0.01        35\n","           8       0.00      0.08      0.01        25\n","           9       0.02      0.54      0.05       109\n","          10       0.03      0.49      0.06       148\n","          11       0.03      0.24      0.05        70\n","          12       0.05      0.50      0.08       131\n","          13       0.00      0.00      0.00        12\n","          14       0.00      0.15      0.00        13\n","          15       0.02      0.16      0.03       146\n","          16       0.02      0.11      0.03       142\n","          17       0.05      0.17      0.08       160\n","          18       0.02      0.17      0.03       124\n","          19       0.02      0.37      0.04        70\n","          20       0.03      0.41      0.05       130\n","          21       0.02      0.21      0.04       114\n","          22       0.01      0.15      0.01        39\n","          23       0.02      0.32      0.04       130\n","          24       0.01      0.16      0.03        68\n","          25       0.06      0.43      0.11       138\n","          26       0.03      0.30      0.05       146\n","          27       0.02      0.15      0.04       148\n","          28       0.02      0.14      0.03       119\n","          29       0.02      0.20      0.04       143\n","          30       0.02      0.32      0.03        60\n","          31       0.03      0.41      0.05        85\n","          32       0.03      0.26      0.05       114\n","          33       0.02      0.22      0.04       141\n","          34       0.02      0.28      0.04        75\n","          35       0.02      0.19      0.03       130\n","          36       0.02      0.10      0.03       136\n","          37       0.01      0.13      0.01        63\n","          38       0.02      0.50      0.04        46\n","          39       0.00      0.16      0.01        31\n","          40       0.03      0.23      0.05       142\n","          41       0.01      0.10      0.02       105\n","          42       0.01      0.04      0.01        84\n","          43       0.02      0.21      0.03       141\n","          44       0.03      0.28      0.05       148\n","          45       0.00      0.17      0.00        18\n","          46       0.00      0.00      0.00        37\n","          47       0.02      0.11      0.03       162\n","          48       0.01      0.08      0.02       166\n","          49       0.02      0.35      0.05        79\n","          50       0.00      0.15      0.01        27\n","          51       0.00      0.29      0.01        17\n","          52       0.02      0.48      0.04        46\n","          53       0.02      0.18      0.03       104\n","          54       0.02      0.27      0.03       119\n","          55       0.01      0.57      0.02        30\n","          56       0.02      0.43      0.03        53\n","          57       0.02      0.54      0.04        52\n","          58       0.06      0.63      0.11       167\n","          59       0.01      0.09      0.02       140\n","          60       0.05      0.48      0.09       138\n","          61       0.01      0.25      0.02        48\n","          62       0.02      0.30      0.04       140\n","          63       0.01      0.04      0.02       157\n","          64       0.01      0.10      0.01        68\n","          65       0.03      0.36      0.05       144\n","          66       0.01      0.47      0.02        34\n","          67       0.03      0.38      0.05       103\n","          68       0.05      0.64      0.10       134\n","          69       0.03      0.45      0.06       159\n","          70       0.03      0.18      0.04       126\n","          71       0.03      0.26      0.06       148\n","          72       0.03      0.36      0.06       110\n","          73       0.00      0.15      0.01        26\n","          74       0.01      0.20      0.01        20\n","          75       0.02      0.21      0.04       148\n","          76       0.01      0.24      0.02        66\n","          77       0.03      0.37      0.05       107\n","\n","   micro avg       0.02      0.26      0.04      7641\n","   macro avg       0.02      0.26      0.04      7641\n","weighted avg       0.02      0.26      0.04      7641\n"," samples avg       0.01      0.30      0.02      7641\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"7ytQvgyzHt8v","executionInfo":{"status":"aborted","timestamp":1610530928912,"user_tz":-210,"elapsed":55383,"user":{"displayName":"zeinab taghavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheNU3ejtJBEFfDC_0W_JdLEuwkW7SKnnDuH80MRA=s64","userId":"07214630829740929140"}}},"source":[""],"execution_count":null,"outputs":[]}]}